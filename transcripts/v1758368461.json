{"text": "da bin ich leute heute ist es soweit kubornitis kubornitis stream oder oder auch kubanitis oder kubanitis oder kubanitis oder was auch immer wir sind cool wir sprechen ja auch ubuntu nicht ubuntu aus sondern jubantu also sagen wir nat\u00fcrlich auch eindeutig auch kubanitis kubanitis das wird \u00fcbrigens heute als es \u00fcbrigens nicht sponsort bei hetzen oder so wir machen heute die sachen noch lokal und es wird erstmal auch nur horizontal skaliert heute ich wei\u00df ich wei\u00df das ist eigentlich nicht das performer mindset wie hei\u00dft das doch performer mindset oder wie hei\u00dft das shit high performer halt das nicht das high performer mindset es wird heute auch nicht der einzige stream sein dazu weil da gibt es einiges das ist eine fass ohne boden und es ist jetzt auch nicht so als bin ich zum aktuellen zeitpunkt der krasseste kubanitis experte den es auf diesem planeten gibt ich habe damit schon was gemacht ich habe die letzten tage auch so ein bisschen noch um probiert und ich habe schon mal in die deployment jammel gemacht und dann war es zum laufen gebracht und also ganz komplett ahnungslos bin ich nicht aber ich bin jetzt nicht der der der krasseste profi ever jetzt jetzt aber was machen wir ein bisschen ahnung haben verstehe nicht was ihr meint mit skalieren das wirst du dann im laufe des streams sehen was mit skalieren gemeint ist da wird hyperscaled wird da da kann man dann richtig sch\u00f6ne tolle cloud buzzwords draus machen die hyperscalenden cloud native container infrastructure es keine ahnung ja fast jeden tag wenn ich nicht stream abends mit den mit den gleichen leuten wieder n\u00e4her ich bin auch am spiel ich habe gesehen du bist nicht \u00fcber level 11 rausgekommen nicht so ging nicht so weit gatschi roll w\u00e4hrend der stream anl\u00e4uft leute ich mache mal meine vm an wobei wir heute beim cool aussprechen ich mache meine vm an und ich mache mal ein update bevor wir jetzt hier gro\u00dfen gro\u00dfen kubanete stellt man ja also leute wir gucken uns das wir gucken uns das alles an also das wird jetzt nicht also wenn ihr jetzt erwartet wir machen jetzt hier kubanetes tutorial wie sagt man so sch\u00f6n from zero to hero wenn man cool sein will dann nennt man das sowas das machen wir nicht also ich erkl\u00e4re euch ein bisschen was und ich zeige euch auch ein bisschen was und wir probieren ein paar sachen aus aber die sinne dieser streams ist ja auch einmal dass ich euch was zeigen kann und auch dass ich selbst besser kann das ist ja immer so 11 ich meine ich mache meistens nicht sachen wo ich komplett planlos bin aber wenn man die dann noch mal erkl\u00e4ren muss dann ist nat\u00fcrlich immer ein bisschen schwieriger versteht selbst besser war ein paar tr\u00e4g horizontal skalieren ist doch mehrere maschinen hochfahren k\u00f6nnen zum skalieren oder vertikal ist mehrere leitungen freigeben ich bin mir ehrlich gesagt gar nicht so sicher ich glaube horizontal skalieren ist mehrere mehrere potts starten und vertikal skalieren ist mehrere notes hinzuf\u00fcgen keine ahnung bin ich bin ich so im hyper hyper scale high performance mindset ach du hast recht genau horizontal jaja ne ne stimmt doch mehr leistung mehr leistung ist ja im prinzip mehrere notes hinzuf\u00fcgen wobei du auch du musst gleichzeitig du musst gleichzeitig ein bisschen vertikal skalieren dass du besser horizontal skalieren kannst oder glaub da m\u00fcssen wir mal eben business coach fragen wir uns das erkl\u00e4rt wie das wie das richtig funktioniert wir skalieren prinzipiell nur diagonal korrekt das ist richtig das haben wir schon vor eine weile festgestellt dass f\u00fcr uns diagonal skalieren das beste ist dann musst du dich nicht mit mehr mit so komischen noobkack abgeben wie ob das jetzt vertikal oder horizontal oder y-achse x-achse z-achse wir wir machen naja wie es skalieren diagonal dreidimensional also in alle drei dimensionen wird skaliert du brauchst mindestens f\u00fcnf kuppernets cluster daheim dass du dass du dem kerl auf youtube konkurrenz machen kannst wir skalieren sogar runter das ist richtig wir skalieren tats\u00e4chlich heute auch runter wir skalieren mal container von 8 auf 4 zur\u00fcck und so was ich war mal kurz bevor es richtig los geht aber kurz den ganzen schitt hier zu das haben wir hier noch swelt zeug alles weg ich war mal kurz ein update und dann starte ich mal die vm neu dass es auch was wird hier dass wir top aktuelles kubernetes machen k\u00f6nnen autokonale skalierung was kommt man was kommt dann als als n\u00e4chstes parallelogramm skalierung relativistische skalierung h\u00e4tte ich noch zu bieten dann skalieren wir nicht nur in alle raumdimensionen sondern auch zeitlich was was haltet denn davon dass man kann das beck was wird moment was so warum kompiliert er gerade in den schitt was macht er da 7 zipp ich habe was viel ich zeige euch mal kurz was bevor wir kubernetes anfangen ich habe was was ein cooles projekt auf github gefunden wenn man keinen bock auf 7 7 zipp ged\u00fcrnt hat unter linux nenn sich pzip das ist das macht alle m\u00f6glichen formate unter anderem auch 7 zipp und und es hat ein gui dabei einfach 8 zipp nehmen ja das ist schon ganz sch\u00f6n 6 hat ich habe ich ok leute ich ich muss mal kurz was zum ich muss mal kurz 7 zipp entfernen brauche ich erinnert so und dann zeige ich euch mal was wir heute vor haben beziehungsweise was ansteht das ist vielleicht nicht das alte 7 zipp n\u00e4her so alt ist es nicht ist es last week geabdeut updated oder doch was ist die letzte version 9.1 ist doch gut alles sogar f\u00fcr die ganzen cute cutie chats chatter also ich wollte noch ein update machen gut lassen wir den erst mal updaten also um was geht es heute ich versuche jetzt mal so ganz grob mal auszuholen ja erinnert sich noch eine an zipp drives die hat mit zipp an sicher nichts zu tun das war eher so halt das zipp so wei\u00df gar nicht warum die so hei\u00dfen vom ger\u00e4usche oder so zipp drive ja iomega zipp drive da wurde nichts gezippt immer eine 100 mb diskette war damals schon richtig krasser shit wenn die normalen disketten hatten wie viel 1,44 mb oder 1,2 ich glaube es gab drei disketten 800k 1,2 und 1,44 oder so was g\u00e4r war es gab war das nicht so ne 3,4 gab es nicht das ja gut vielleicht sp\u00e4ter mal aber ich kenne also die 1,44 waren glaube ich die verbreiteten wie ist ja dass ich mich da dass ich mir damals wieder krasse heckermann vorgekommen bin ich habe alte disketten vom amiga genommen das waren ja auch so floppy floppy disk floppy disk und dann habe ich die in mein pc laufwerk reingesteckt und dort unter windows rechts drauf geklickt und gesagt formatieren und dann hatte ich auf einmal ganz viele leere disketten weil ich die amiga dinger recycelt habe und da habe ich boah da habe ich mich richtig geil gesagt mann bin ich bin ich geiler hier heckermann ich habe quasi disketten aus dem nichts erschaffen dabei waren das ja ganz normale floppy disk halt f\u00fcr den amiga das habe ich nicht habe ich damals nicht gecheckt also ja amiga gescaled genau so update ist durch update ist durch ich glaube es war ein kernel update dabei deswegen reboote ich mal 5x dankesch\u00f6n f\u00fcr den sub ich habe oben \u00fcbrigens glaube ich noch ein \u00fcbersehen stripes dann wieder subscrib poggers danke f\u00fcr die subs leute so dann starten wir mal das verdammte spiel und jetzt kann ich sag guck mal ne das war ja kein bock auf krasse giga chat aussprache jetzt kann man damit anfangen das ketten war das stimmt das ketten war schon cool dass sie der klick zu dieses klick dieses hatte schon was nutzt du zsh ja mache ich und wenn ja welches theme ist das das ist gar kein theme das ist also du meinst jetzt mein termin oder wirklich hier den prompt von der shell ich vermute mal weil du zell das was meinst du den prompt also der prompt ist starship allerdings ein custom meister ein custom meister ein custom meister starship prompt wenn du den prompt sehen willst gehst du hierhin auf wuppe loss dot files config starship und da siehst du das ich habe den base prompt genommen von starship du musst du noch sagen ob du wirklich shell oder terminale meinst weil terminale ist anders so und ich habe dieses preset genommen von starship pastel powerline was wiederum selbst auf m365 princess basiert das habe ich genommen und habe ein paar customized sachen angepasst ja zum beispiel habe ich die schrift gemacht wie mein hintergrund ich habe hinzugef\u00fcgt dass wenn man in ein dotnet projekt geht die versionen da stehen und so was ich habe es auch zweizeilig gemacht weil ich zweizeilige prompts pers\u00f6nlich viel besser finde als einseilige prompts ja aber es ist etwas ist das hier aber angepasst und du kannst findest das hier ach du kacke das ist wild ja das ist das ist aber wahrscheinlich noch neuer oder fast so alt wie unsere cobalt geschichten die bei uns laufen wo ich mich auch immer frage wo finden die immer noch leute die den kram k\u00f6nnen wir zeit wird zeit f\u00fcr linux upgrade und dann das ganze einfach in free doors laufen lassen oder oder oder noch besser wine im docker container im container wei\u00df gar ob das gescheit funktioniert ja warum eigentlich nicht ja aber das will man ja wahrscheinlich nicht weil man so altes zeug hat dass es stabil l\u00e4uft ob das mit wine dann also was machen wir heute wo fange ich denn jetzt am besten an leute wo fangen wir am besten an serverschrank 24 wieso klingt auch ganz normal anbieter h\u00f6rt sich halt bisschen oldschool an aber warum warum ich klingt auch klingt auch ganz normal berat uns doch erstmal ob kubernetes native stellt oder mini cube zum ausprobieren reicht das lokal voll und ganz ach ja ich wei\u00df wo ich anfangen ich wei\u00df wo ich anfangen und zwar dass wir es heute nicht der einzige kubernetes stream und zwar wir machen da mehrere ich wei\u00df nicht inwieweit da bin ich wieder unschuldig wei\u00df ich inwieweit das alles so aufeinander aufbauen wird wahrscheinlich kein raspberry pi cluster wahrscheinlich schon in gewisser weise wir machen aber nicht immer so exakt die gleichen beispiel und zwar also wir brauchen mehrere wir brauchen mehrere streams jena es h\u00e4ngt auch ein bisschen davon ab wie weit wir aber es wird auf jeden fall auch aus praktischen gr\u00fcnden mehrere streams geben das erste ist wir setzen uns damit lokal mal ein bisschen auseinander wir m\u00fcssen nicht gleich in die cloud in der cloud irgendwas bauen das n\u00e4chste ist wir gucken uns google cloud kubernetes an warum gucken uns google cloud kubernetes an weil dort wo ich am 1. april anfange zu arbeiten die alle google cloud verwenden ich wei\u00df nicht warum das sind die einzigen die ich kenne die google cloud benutzen aktuell wo ich bin ist es ist es quasi fast nur ausschlie\u00dflich azure ich habe schon ein bisschen was mit aws ich fasse nicht schon ein bisschen was privat gemacht mit google und mit aws w\u00fcrde ich sagen aber abstand am wenigsten aber ich kenne au\u00dferdem niemand der im professionellen umfeld google cloud verwendet gibt es allerdings genug nur weil ich keine kenne hei\u00dft das nix weil ganz einfach ich kenne nicht so viele l\u00e4den von innen die paar handvoll da die ich gesehen habe die haben keine google cloud verwendet gut es hei\u00dft ja nichts erfolgreich ist es trotzdem und ich habe mir ein paar videos angeguckt und angeblich ist dass dieses autopilot kubernetes management feature von der google cloud wohl \u00fcber diesen anwendungszweck am besten dann betreibst du n\u00e4mlich gar kein kubernetes cluster mehr selbst in der google cloud sondern kannst direkt pots deployen und die scaling und managen das f\u00fcr dich aber nat\u00fcrlich machen wir damit nicht fangen wir damit nicht gleich an weil das ist ja langweilig muss erst mal ein bisschen so die grundlagen uns angucken dass man wei\u00df wie das funktioniert weil nur was man macht von hand zumindest ich so ein bisschen selbst angeguckt hat blickt man dann auch durch was man besser sein lassen sollte ja deswegen gibt es mehrere streams wir machen wir wir gucken uns jetzt erst mal wir gucken uns jetzt erst mal das ganze lokal an da machen wir mal so ein setup aus irgendwie so eine 5 5 pots als application server mit einem load balancer und einem ssl zertifikat oder so mit ingress controller davor wir holen uns auch echte let's encrypt zertifikate \u00fcber let's encrypt also echte echte ssl zertifikate \u00fcber let's encrypt mit zert manager und was machen wir sonst noch wir machen den lokalen registry f\u00fcr container images und konfigurieren halt das deployment und den service gucken uns das alles mal ein bisschen an ist der moogame dankesch\u00f6n f\u00fcr den snap was h\u00e4ltst du von IT zertifikat kommt ein bisschen auf die zertifikate an aber allgemein gesagt nicht ganz so viel ja ich w\u00fcrde sagen wir fangen mal relativ easy an ah ja und \u00fcbrigens ich habe schon ein bisschen was vorbereitet wo ich abgucken kann also das ist jetzt nicht fix und fertig aber ich habe mir so ein paar kleine snippets raus kopiert falls ich jetzt im stream nicht direkt drauf komme weil alles auf den kopf wei\u00df ich da auch nicht was h\u00e4ltst du von lpix also das was ich bisher gesehen habe sah sehr nach man pages aus wenn ich lerne aus aber kann auch kann mich auch t\u00e4uschen wir verwenden kein windows server doch klar ich mache immer windows server windows server ist eindeutig die \u00fcberlegene technologie deswegen l\u00e4uft ja auch 90 prozent des internets auf linux guck hier ja was sonst verstehe ich auch die frage gar nicht was denn sonst gibt es noch was anderes windows server und powershell und es auf final auf was aber finals ist gerade irgendwas ich habe \u00fcberhaupt nichts mitgekriegt cs go ist gerade glaube ich also wenn die windows server skalieren bist du nicht arm wenn die oracle datenbacken skalieren dann bist du arm wobei ne ne das stimmt nicht wir k\u00f6nnen es anders machen wenn die windows server skalieren bist du arm wenn die oracle datenbanken skalieren da kannst du gleich insolvenz weil ich wei\u00df was ich machen muss um das problem zu fixen jetzt ist nur die frage m\u00f6chtest du auch wissen wie man das problem fixt was muss man denn machen also die grundvoraussetzung ist schon mal das ist schon mal essentiell und dann zeige ich dir auch dass du gtkmm oder wie das hei\u00dft installieren musst das da musst du installieren und dann funktioniert probier mal aus das musst du installieren und dann startet die kiste am besten mal neu oder oder den system d service neu starten den hast ja bestimmt gestartet und so was aber das ding musste installieren diese library ist pflicht sonst funktioniert copy und paste nicht warum das keine dependency ist und warum das nirgendwo steht ich habe keine ahnung aber du brauchst diese library sonst geht copy und paste nicht gtk mm3 warum wei\u00df nur vmw so also wir uns jetzt mal hier das kubernetes kubernetes vielleicht was sagen die denn was sagen die denn selbst ja das das trifft es eigentlich ganz gut ich wollte sagen kubernetes ist ein tool zur container orchestrierung aber das sagen sie ja selbst also f\u00fcr alle was genau wird heute mit kubernetes gemacht also erst mal erkl\u00e4re ich kurz was kubernetes ist kubernetes ist quasi so was wie docker nur geclustert und mit wie wir kl\u00e4ren das am besten es ist es ist so was wie docker nur verteilt \u00fcber mehrere server ich glaube das kann man eigentlich am besten glaube das umschreibt eigentlich am ehesten ne du kannst auch einen server nehmen du kannst auch einen server nehmen aber f\u00fcr einen server macht es halt wenig sinn hochverf\u00fcgbares docker ja ich glaube ich glaube dass das trifft eigentlich ganz gut sie sagen ja selbst sie sind ein ein orchestrierungstool f\u00fcr container infrastruktur also du kannst damit deine container in deine container verwalten du kannst damit zum beispiel einstellen dass du f\u00fcr einen ich nenne es mal service auch wenn die kubernetes service was anderes ist das wird eine pains champ reihe nur so so schlimm so schlimm ist es auch das einzige problem an kubernetes ist so zumindest wie ich das sehe also was vom lernen her dass es ein fass ohne boden ist es gibt 30 milliarden tools f\u00fcnf tools f\u00fcr die gleichen sachen jede woche spawn irgendein neues tool die einfachsten sachen werden teilweise super krass verkompliziert aber insgesamt ist es an sich an sich ist das doch eine coole sache ja also was du das meinst ja also was du damit zum beispiel machen kannst mal ganz simpler use case ja du hast drei server und m\u00f6chtest also du willst ein webservice anbieten willst webservice anbieten und du willst dass das ganze m\u00f6glichst verf\u00fcgbar ist und du willst dass das ding im laufenden betrieb geupdatet werden kann und du willst dass das auch je nachdem wie viel user gerade drauf sind ordentlich performed oder eben auch skaliert wie man so sch\u00f6n nennt dann ist kubernetes genau das richtige weil du kannst kubernetes nehmen stellst dir verschiedene notes hin hei\u00dft das bei kubernetes also verschiedene server notes sind bei kubernetes in der regel die unterschiedlichen server in einem cluster also cluster member sind quasi notes bei kubernetes das muss ich zwangsl\u00e4ufig eine hardware kiste sein das k\u00f6nnen auch vor ems sein der neueste schrei ist sowas wie kubernetes auf firecracker vor ems wenn man das jetzt mal h\u00f6rt und jemand wei\u00df nicht was es ist er denkt jetzt erst mal was zum teufel will er von uns das machen wir heute nicht ich wei\u00df nicht ob wir es \u00fcberhaupt im stream machen ist auch was ganz ganz ab abgedrehtes das sind minimal vms firecracker das ist glaube ich von glaube das ist von amazon ich bin mir nicht ganz sicher ich glaube von amazon so eine minimal vm kubernetes selbst ist f\u00fcr container workload und der kubernetes cluster besteht aus unterschiedlichen notes in der regel ist es halt ein server jeder note das kann nat\u00fcrlich auch ein server sein worauf dann mehrere vm sind kann es kann es quasi auch sein kubernetes cluster in mehreren vmst auf einem eskserver laufen betreiben was auch immer oder theoretisch kannst das ding auch \u00fcber mehrere cloud anbieter spannen wie dem auch sei da bist du sind ja deine kreativit\u00e4t keine grenzen gesetzt wie du das ganze aufbauen willst zumindest hast du dann den vorteil dass du so sch\u00f6ne sachen machen kannst wie es gibt ein das machen wir \u00fcbrigens heute wenn wir so weit kommen du hast eine neue anwendungsversion und m\u00f6chtest jetzt was deine anwendung l\u00e4uft zur zeit auf version 1 und du willst jetzt auf version 2 und dann kannst du den config file editieren du kannst dar\u00fcber kommandos teile machen aber in der regel macht man \u00fcber config jammels dann ich will jetzt die version 2 von diesem moment ist an der t\u00fcr hat mich abgelenkt also wenn du keine ahnung davon hast und wenn euer online shop so funktioniert dass nur ein service ist da brauchst wahrscheinlich kein kubernetes cluster wie gesagt man muss es man muss es auch nicht \u00fcbertreiben manchmal tut es ohne probleme 123 vms und reverse proxy f\u00fcr ssl davor also es muss nicht sein da ist schon da ist schon ihr werdet wahrscheinlich so im lauf des streams ein bisschen an an klingen sehen da ist schon auch wenn es erstmal easy aussieht eine geh\u00f6rige portion komplexit\u00e4t drinne gar nicht so sehr in den reinen basis funktionen von kubernetes sondern das gesamte \u00f6kosystem drum herum was es gibt ja wo bin ich jetzt stehengeblieben genau also du willst du hast ein service laufen du willst den service jetzt abqueren wessen von version 1 auf version 2 normalerweise wird es dann auf dich und vm einloggen neu pullen vm den service wieder starten was auch immer mit komponiert das geht es relativ einfach du gehst in den jammel feil machen wir heute auch alles jammel feil tr\u00e4gt die neue container image version ein das image bauste vorher irgendwo entweder lokal mit docker oder \u00fcber jenkins oder github action so wie irgendwelche continues pipeline geschiss bauste dann image und pusht das in registry rein und dann tr\u00e4gst du dann im jammel feil ein ich will jetzt auf version 2 dann applies du diese konfig machen wir jetzt gleich alles dann applies diese konfig auf deinen kubernetes cluster und es skaliert dann automatisch also der reihe nach deine einzelnen container hoch also nicht skaliert der upgrade an der reihe nach deine einzelnen container ja richtig stripes genau so dass der service nicht ausf\u00e4llt was ist der unterschied zwischen k8s und k3s ich glaube das eine ist irgendwie super leicht gewichtiges komponiert gibt es ein paar sachen es gibt ein paar sachen wenn man das ist zum beispiel was was du eher auf dem respiratory pie laufen lassen kann kannst das ist aber glaube ich soweit ich wei\u00df 100 prozent api kompatibel zum gro\u00dfen kubernetes es gibt verschiedenste sachen f\u00fcr minimal kubernetes es gibt es gibt das originale kubernetes oftmals mit k8s abgek\u00fcrzt gibt es auch null ks es gibt kind es gibt mini cube es gibt alle m\u00f6glichen dinger total total viel sachen mini schiff feier cue war es auch immer alles wir nehmen heute mini cube wir nehmen heute mini cube weil es am einfachsten ist f\u00fcr mich mit mini cube lokal ein bisschen rum zu basteln so mini cube ist nahezu 100 prozent api kompatibel mit es kann halt einige sachen nicht ich kann euch aber nicht aus dem kopf sagen was es aber zum gr\u00f6\u00dften teil api kompatibel mit kubernetes und du kannst auch die ganz normalen kubernetes tools nehmen f\u00fcr f\u00fcr mini cube um damit lokal was auszuprobieren ist halt haupts\u00e4chlich gedacht f\u00fcr auf einer kiste lernen und \u00fcben mini cube wir machen jetzt mal ein eigenes wir machen jetzt mal ein eigenes verzeichnis daf\u00fcr repose mk dir nennen wir es einfach mal sogar du brauchst noch cube ctl ne brauche ich nicht weil guck mal da ist es das ist mittlerweile bei mini cube dabei also was hei\u00dft dabei der l\u00e4dt es im hintergrund runter aber trotzdem ich installier es trotzdem weil das braucht man normalerweise hast du schon recht plan der mission lernen ja also wie gesagt meine streams sind ja immer so so ein ding also ich mache meistens sachen wo ich selbst ein bisschen schon von ahnung habe aber nicht so krass den durchblick und dadurch dass ich das euch zeige und ein bisschen was erkl\u00e4ren muss lernt man selbst besser das ist immer ganz gute kombination so sieht es aus jetzt k\u00f6nnen wir anfangen es ist relativ easy zu starten mit mini cube und zwar ihr braucht keine vm ihr braucht kein kubernetes installation und nix ihr sagt einfach mini cube start und jetzt macht ihr eine sache falls ihr das auch ausprobieren wollt macht ihr eine sache man muss mal kurz nachgucken insecure oder mir ist das insecure registry machte jetzt mini cube start ins insecure registry werdet gleich sehen warum jetzt leider kobe ne das geht uns runter und direkt best practice ist lokal es ist lokal \u00fcbungs practice chat ihr werdet gedisst gerade im chat also quasi aber du bist auch teil des chats kommt eigentlich ein video zu dem mobil funk anbieter suchen was ich euch da erz\u00e4hlen soll so ach ja es gibt verschiedene nummer so der vollst\u00e4ndigkeit vollst\u00e4ndigkeit halber erw\u00e4hnt es gibt verschiedene m\u00f6glichkeiten mini cube zu betreiben die standard variante ist \u00fcber irgendeinen virtualisierungs provider also meistens kvm also wenn ihr mini cube startet dann legt es normalerweise unter der haupe eine neue vm an wenn man keine vm anlegen m\u00f6chte kann man das ganze auch selbst als container in docker laufen lassen ist ja kein ding ich mein kubernetes ist ja letztendlich tool zur container verwaltung warum sollte man das nicht auch selbst in dem container laufen lassen k\u00f6nnen also ihr seht ja hier ich habe keine vm installiert also ich habe hier keine virtualisierungsm\u00f6glichkeiten in der vm drin das hei\u00dft er l\u00e4sst jetzt das back end einfach in einem eigenen docker container laufen also es ist jetzt quasi kubernetes in docker womit man eine weitere container verwalten gab es ist egal letztendlich muss man sich damit nicht besch\u00e4ftigen das ist ja zum \u00fcben und die api und wie man das anspricht und so ist \u00fcberhaupt kein unterschied also diese lokale lokale so entwicklungsumgebung mit mini cube k\u00f6nnen die genauso benutzen wie also die du brauchst keine extra s\u00fcndung so ist quasi genau das gleiche wie f\u00fcrs gro\u00dfe kuppernet ist nur dass es lokal bei einem auf dem rechner l\u00e4uft sagen mal logien halbist du irgendwie besoffen oder so ich kann dir nicht folgen was du f\u00fcr komisches teuch im chat schreibst die ganze zeit was was soll was soll uns das sagen lokal gesch\u00fctzt von meter alles klar written in rust container runtime nie kenne ich nicht ich kann nicht jede rust software kennen ich schreibe \u00fcber mastadon schreibst du das einfach nur so oder schreibst du mit jemand ich glaube ich glaube du machst aktuell die taktik was ich auch manchmal mache alles was einem in den kopf f\u00e4llt in irgendein text einkommen in irgendein text eingabe f\u00e4llt tippen und enter dr\u00fccken aber mach nur ich beschwer mich nicht alles interactions auf twitch wobei das war youtube wo das so wichtig ist ach ja und wir brauchen tats\u00e4chlich anscheinend doch nachher noch potman und so gucken wir uns an also wir haben mini cube gestartet wir k\u00f6nnen jetzt mal sowas machen wie cube ctl get ach nee ha sekunde leute lasst lasst euch nicht jebaden von irgendwelchen youtube tutorials oder sowas ich zeige euch jetzt mal wie das die ganzen youtube tutorial dudes machen und dann zeige ich euch wie man es richtig macht ok also die ganzen youtube nicht alle es gibt ein gut es gibt gute tutorials das beste deutschsprachige tutorial auf youtube was ich kenne dazu kubernetes ist also f\u00fcr leute die ein bisschen ahnung von linux und sich nicht vor kommando zeilen scheuen und relativ relativ ja schon ein bisschen bisschen background was container angeht haben das ist das beste also wenn wenn sich einmal wirklich in zwei stunden in zwei stunden gut strukturiert das kubernetes tutorial angucken will finde ich das pers\u00f6nlich auf deutsch finde ich das pers\u00f6nlich am besten ich k\u00f6nnte nat\u00fcrlich auch einfach dabei bleiben weil ich wir werden ja wahrscheinlich nicht alles machen was er im video macht aber in gro\u00dfteil und ihr k\u00f6nnt ja live auch noch was dazu beitragen und wir machen auch ein paar sachen die er so glaube ich nicht im video zeigt ich glaube es ist selbstwertig f\u00fcr karte \u00fcber letzten krippen macht er im video gar nicht aber wenn mal einer sich in ruhe und tutorial angucken will ich finde das ist das beste so ja genau also ich zeige euch jetzt mal wie das die ganzen tutorial duz auf youtube und auch teilweise auf jude me und so machen wenn die kubanets kommandos oder generell wenn die kommando zeilen tools verwenden dann machen die so was hier guck jupe ctl get pots oder da machen sie so get events und solche dinger du siehst die typen in tutorial videos zum kurz ich dr\u00fccke schon ins den ganzen krempel immer f\u00fcnf mal tippen so nicht so nicht leute das ist das ist mist auch wenn man das in allen m\u00f6glichen tutorials sieht das erste was man braucht ist und jeder linux admin wird wissen das erste was man braucht ist shell completion leute jetzt nicht unbedingt f\u00fcr minicube f\u00fcr minicube ist das relativ egal dann m\u00fcssen wir nicht so viel machen aber f\u00fcr kubectl brauchen wir best completion mir h\u00f6rt das ist aktuell noch nicht der fall das geht noch nicht und wir wollen nicht nur einfach tab completion haben sondern wir wollen auch quasi ja kontext sensitive tab completion haben also wenn ich zum beispiel sage hier irgendwie describe potts oder ne pot genau describe nee nee quatsch nee das klappe kann ich so die ressource hinten dran schreiben blabla blub so und dann will ich dass das auto completed so und dazu gibt es folgendes kubectl jetzt muss ich mal \u00fcberlegen wie das war completion zsh genau so kubectl bringt n\u00e4mlich eine eigene completion definition f\u00fcr bash zsh und was noch best und zsh wahrscheinlich mit richtig nice so und das kann man dann in seine basher z eintragen die dann folgenderma\u00dfen aus muss blo\u00df mal kurz seine zhsh wie auch immer shell konfig ich muss mal kurz gucken wo habe ich denn meine ach da da ist es doch muss doch unten irgendwie sein source und wie machen das jetzt so geht das einfach so bin ich gerade ein bisschen ich glaube ich glaube wir m\u00fcssen es redirecten ich bin mir nicht ganz sicher ob das so funktioniert nein aber ich habe ich habe dings brauche ich nicht genau so ist gut ja genau und jetzt habe ich kubectl und guck mal tab tab jetzt habe ich autocomplete ja und ihr seht autocomplete hier endpoints autocomplete und der autocomplete jetzt quasi auch kontext sensitive infos so an der stelle gibt es halt einen endpoint der kubernetes hei\u00dft und den kann ich da eintragen jetzt kann ich zum beispiel describe irgendwie sowas wie hier da gibt es halt pots gibt es halt noch kein man kann auch sagen all namespaces dann zeigt er dann alle pots an die es gibt mal mal get pots dann erz\u00e4hle ich mal ein bisschen so was zur zur benahmung wie kubernetes denkt dass man dinger nennen soll also das hier sind jetzt die laufenden container die wir zur zeit haben in unserem kubernetes cluster wobei das nicht stimmt wobei das nicht ganz richtig ist ich muss das ich muss das anders erkl\u00e4ren also kubernetes ist zwar ein orchestrierung stuhl f\u00fcr container aber kubernetes arbeitet gar nicht auf container basis das klingt jetzt erst mal verwirrend aber wenn ich euch das ein bisschen genauer erkl\u00e4ren wird es hoffentlich klarer und zwar das was somit kubernetes verwaltet sind zwar unter der haube container aber container werden in kubernetes zusammengefasst zu pots ein pot ist quasi so eine logische einheit wieviel du meinst wie viele notes mein cluster hat einen lokale vm gerade so also was wollte ich jetzt sagen genau kubernetes das kleinste mit dem kubernetes hantiert sind pots pots ist so die kleinste logische einheit die man die man mit kubernetes verwalten kann in der regel k\u00f6nnte man sagen dass ein pot so was wie ein container ist wobei das nicht stimmt ein pot ist quasi also ist quasi so der container f\u00fcr die container oder so dass so ist so die so die der rahmen oder so die kiste um die container klingt jetzt bescheuert kann man kann man echt schlecht beschreiben also ich erkl\u00e4re ich mal zum beispiel warum man das braucht ich erkl\u00e4re ich mal warum man das braucht und zwar mal angenommen du hast jetzt eine anwendung in die net anwendung die verpackst du in container image und die l\u00e4sst du dann als pot laufen dann ist der pot genau ein container drin aber du k\u00f6nntest jetzt ja in diesem pot zum beispiel noch einen container f\u00fcr metriken haben oder noch ein container f\u00fcr secrets oder noch ein container f\u00fcr reverse proxy oder so was also k\u00f6nntest ja du k\u00f6nntest ja quasi damit die anwendung von als einheit funktioniert also ein docker compose file ist ein pot ja das kommt ungef\u00e4hr hin wobei man sagen muss in so einem docker compose file definierst du ja auch mehrere logische services die du miteinander verbindest du kannst jetzt mit docker compose file auch eine datenbank reinschreiben ein application server und die dann miteinander verbinden das wird schon in einzelne pots machen das du w\u00fcrdest quasi nur sachen die logisch zusammengeh\u00f6ren in ein pot packen ja also zum beispiel ein container f\u00fcr connection zu deinem secret store oder so was in der richtung also werd mal angenommen deine deine anwendung braucht api keys und sonstige geschichten die hast du nicht in einem kubernetes cluster sondern irgendwo extern liegen und du willst es auch nicht in kubernetes cluster integrieren dann kannst du dir noch einen zweiten container rein passen rein packen der nur daf\u00fcr da ist diese secrets zeig mal wie ist denn dieses bild ja ja genau also das ganze nennt sich \u00fcbrigens seit k wenn man noch mehrere container rein packt in so einen pot genau so was hier das ist das ist ein sch\u00f6nes beispiel das ist das ist ein sch\u00f6nes sch\u00f6nes beispiel ja aber aber chat gpt erkl\u00e4rt er nicht warum das ist ein sch\u00f6nes beispiel ja du hast zum beispiel deine anwendung die schreibt logs und jetzt willst du diese log files allerdings zu einem zentralen logserver schicken und die anwendung selbst kann kein syslog und hat auch keinen bock sich irgendwie mit logs am einsammeln dingern zu besch\u00e4ftigen dann w\u00fcrdest du einen pot erstellen der enth\u00e4lt die anwendung und noch einen zweiten container der daf\u00fcr da ist die logs einzusammeln und an irgendein zentrales log management system zu schicken das ist ein gutes beispiel ja du k\u00f6nntest zum beispiel noch einen dritten container dabei machen der metriken bereitstellt f\u00fcr prometheus monitoring oder so was hast du quasi eine logische einheit also das hier w\u00e4re dann quasi dein application server pot allerdings enth\u00e4lt er eben alle container in diesem pot die die anwendung braucht um ebenso als logische einheit gestartet werden zu k\u00f6nnen und das zu machen was sie machen soll eine datenbank w\u00e4re wiederum ein zweiter pot das ist eigentlich gar nicht so verkehrt was was ein bisschen doof ist an kuba die haben halt f\u00fcr alles irgendwelche gew\u00f6hnungsbed\u00fcrftige bezeichnung ja container sind pots was schon irgendwie klar ist weil ein pot mehrere container enthalten kann du hast deployments da versteht jeder eigentlich was anderes darunter als irgendwelche komischen yaml files die pots definieren aber gut da muss man sich ein bisschen dran gew\u00f6hnen was phippe goes to the zoo cloud native computing foundation hat es kuba neta story moment jetzt lernen wir was ok pots hier hier werden pots erkl\u00e4rt ich verstehe den zusammenhang kann ich so genau ich verstehe den zusammenhang zwischen diesem bild und und potz nicht wie dem auch sei wir machen jetzt wir machen jetzt mal weiter man muss sich zumindest ein bisschen dran gew\u00f6hnen dass sie alles irgendwie irgendwie anders nennen und zwar ist es zum beispiel bei kuba neta ist auch so die nennen dort wo der traffic reingeht also quasi das was jeder normale mensch irgendwie reverse proxy nennt das nennen die ingress zum beispiel und das gegenteil von ingress also da wo der traffic quasi reingeht ist nicht etwa outgress sondern egress weil ist halt so und da gibt wie gesagt gibt gibt gibt noch mehr sachen wo ganz komisch benannt sind wo man sich einfach dran gew\u00f6hnen muss was genau ist minikub minikub ist ein tool wo du lokal bei dir kuba netas laufen lassen kannst zum \u00fcben also wir haben das ganze noch nicht gestartet doch also nachdem minikub installiert ist es ganz einfach \u00fcber package manager oder als single binary runterladen es ist glaube ich auch in go programmiert als single binary runterladen starten dann startet ihr minikub am besten dass ihr noch hinten dran wenn ihr lokale container registry haben wollt und gut ist dann habt ihr minikub gestartet euer kuba netas l\u00e4uft irgendwann mal l\u00e4uft es dann und dann k\u00f6nnt ihr auch schon kommandos an euren kuba netas schicken und das ganze machen mit kubectl kubectl kubectl get pods zum beispiel so und man sieht hier schon folgendes das ist das wo ich gerade am wo ich gerade am erz\u00e4hlen gewesen bin ein port ist die kleinste logische einheit die kuba netas verwaltet kuba netas selbst oder in dem fall minikub selbst wenn du startest kommt schon mit ein paar vorinstallierten pods um die musst du dich nicht k\u00fcmmern das ist es ja auch hier das ist im name space kube system und dort sind interne sind interne pods die man die das braucht dass es funktioniert was zum beispiel hier sowas wie core dns das ist das ist wichtig daf\u00fcr dass die pods sich untereinander beziehungsweise auch die services aufgel\u00f6st werden k\u00f6nnen und im cluster selbst die dienste per namen angesprochen werden k\u00f6nnen und so was da muss man sich jetzt nicht so viel gedanken dr\u00fcber machen aber damit kommt das halt also sprich man sieht hier laufen schon ein paar container was man hier auch schon sieht das brauchen wir sp\u00e4ter noch hier sieht man dass manche pods ready sind es ist so in kuba netas dass ein also in docker ist es so du startest das ding so und entweder es krascht oder es l\u00e4uft aber so wirklich gro\u00dfartig wissen wann das jetzt ready ist und wann nicht tust du nicht ja du kannst den docker zwar einstellen dass er restarten soll wenn es krascht aber wann die webanwendung da drin gestartet ist und ich wei\u00df nicht so und nachdem es in kuba netas ja unter anderem darum geht dass das ganze m\u00f6glichst ausfallfrei l\u00e4uft haben container oder pods muss man sagen sind da sind da keine container haben container verschiedene m\u00f6glichkeiten zu \u00fcberpr\u00fcfen ob sie bereit sind da gibt es n\u00e4mlich sowas was der andreas gerade schreibt es gibt so was wie readiness und leifnis da kann man dann wirklich health checks auf die container machen auf die pods machen und erst wenn die wirklich ready sind funktionieren also in dem fall dass ein web response web request durchgeht bei einer anwendung erst dann kommen die quasi in den ready state und der interne load balancer in kuba netas w\u00fcrde die dann ansteuern genau kompost kann auch so was in der richtung ja ich finde hier ist das jetzt das sch\u00f6ner gemacht so also mit was ich \u00fcberlege gerade mit was wir anfangen wir machen uns erst mal eine leere datei nennen wir das einfach mal hier touch keine ahnung cute punkt jammel und machen wir wishell studio code auf ja wir trasten cute wir trasten nicht cute punkt jammel nope so jetzt k\u00f6nnen wir als erstes mal die kuba netas extension installieren ja ja reload mal die kuba netas extension ist nicht so dolle also ist ehrlich gesagt das einzige was ich von der kuba netas extension benutze ist das hier die janta jammel das jammel synthax autocomplete f\u00fcr kuba netas files so das n\u00e4chste ist ein config file f\u00fcr kuba netas also so ein jammelfile hier das nennt sich wenn ich nicht ganz falsch liege manifest warum weil config datei zu dumm ist weil die das immer gerne ein bisschen fancy nennen was kuba netas angeht also ein jammel konfig datei f\u00fcr kuba netas nennt sich manifest und in so einem manifest kann man fast beliebig viele dinge reinschreiben die man konfigurieren will f\u00fcr seinen kuba netas cluster man kann wie gesagt man kann mehr als eins reinschreiben allerdings muss man sich immer \u00fcberlegen man kann zwar jetzt auch gleich man kann diese datei immer nur im gesamten einlesen lassen von kuba netas also wenn ich da jetzt 20 sachen drinne definiere und ich will dass kuba netas den kram \u00fcbernimmt dann immer nur alles auf eimer wenn alles in einer datei drin steht also es macht durchaus sinn sachen die zusammengeh\u00f6ren zusammen zu definieren also man k\u00f6nnte jetzt zum beispiel wobei da schreiten sich ja die geister dar\u00fcber was man wie aufsplitten sollte in config files und ob man wie man es nennt und ob man f\u00fcr jeden dings in unterordner macht und dann ein eigenes jammelfile f\u00fcr service deployment was auch immer wir schreiben das jetzt erst einmal sinnvoll zum zum \u00fcben schreiben wir das jetzt erstmal alles in ein jammelfile also wir brauchen erstmal eine testanwendung w\u00fcrde ich sagen oder wir haben hier keine testanwendung wir machen man wir machen mal eine minimal dotnet testanwendung wobei da m\u00fcssen wir einen container bauen und sowas hello world wir nehmen den hello world container da seht ihr auch schon mal gleich dass wie das aussieht wenn ein container crashed wir machen hello world hello world container der beendet sich n\u00e4mlich da werdet ihr gleich sehen als kuba netas auch sagt das ding ist das ding l\u00e4uft nicht mehr also deployment so sieht ein kuba netas deployment aus ich mache mal kurz die schriftgr\u00f6\u00dfen ticken in ticken gr\u00f6\u00dfer 18 aber wir waren sogar 20 kommen dass man gescheit was sieht wunderbar also so sieht in deployment in kuba netas aus das ist das das ist das template was hier die studio add-on dabei hat also in deployment ist in kuba netas eine config die kuba netas sagt welche container es starten soll ganz platt gesagt leute ich glaube euch im chat dass ihr die super krassen kuba netas checker seid wahrscheinlich mehr als ich aber es hat keinen sinn wenn wir jetzt alle 800.000 kuba netas tools die es gibt immer wild durcheinander im chat schmei\u00dfen wie gesagt das ist ein un\u00fcberschaubare landscape an tools und sachen man k\u00f6nnte es auch man k\u00f6nnte man k\u00f6nnte auch gar kein manifest schreiben und man kann das manifest in python generieren das gibt es n\u00e4mlich auch also wo wir gerade dabei sind hier man kann auch so was verwenden guck wenn man keinen wenn man keinen bock auf jammel hat kann man auch das komplett erstellen mit jeder x beliebigen programmiersprache was was haben die denn ich glaube python java und noch was aber das ist einfach zu viel auf einmal deswegen war jetzt erstmal jammel files und gucken uns das ganze an so also was gibt es denn so in einem deployment zu sehen erstmal in unseren namen ausdenken cute app super toll ich mache noch mal deployment cute app dann haben wir hier so sachen wie metadata das ist erstmal nicht so wild metadata kann man alles m\u00f6glich reinschreiben es gibt \u00fcbrigens auch noch annotations schwachsinn es gibt labels es gibt labels und es gibt annotations ich habe noch keine ahnung beziehungsweise ich habe ich habe noch nicht gecheckt wozu man annotations \u00fcberhaupt braucht ehrlich gesagt au\u00dfer dass manche ja nennen wir es mal third party tools gerne annotations benutzen normalerweise kommt man eigentlich mit labels mehr oder weniger aus kannst du den programm kurz kurz erkl\u00e4ren es gibt noch gar kein programm code das ist das template was visual studio macht wenn ich deployment wenn ich deployment hinzuf\u00fcge meiner kombiniertes config mehr gibt es da noch nicht also metadata das nennen wir mal cute app also quasi wie diese ressource ressource nennt sich das \u00fcbrigens was angelegt wird also diese ganze datei nennt sich manifest und was hier angelegt wird ist glaube ich eine ressource und der typ der ressource ist in dem fall deployment damit eure jammel files auch garantiert immer funktionieren gibt es hier oben eine api version also das hei\u00dft mal angenommen die bringen version 2 irgendwann raus was viel viel mehr sachen unterst\u00fctzt und hier oben immer noch api version 1 angegeben habt solange die version 1 weiter unterst\u00fctzen funktioniert euer manifest auch weiter api version gibt es \u00fcbrigens f\u00fcr f\u00fcr nahezu alle verschiedenen ressource typen so wir machen jetzt mal wir machen jetzt mal ein bisschen zackig da kannst du noch selektor und labels und so anlegen das ist wird gleich das wird gleich relevanter wenn wir da noch notbalancer und so was und so was da vorh\u00e4ngt so template metadata labels k\u00f6nnen wir auch so lassen cute app manifest v2 und v3 kann das war das in chrome ja sondern hier schreiben wir unser docker image rein beziehungsweise unser container image also immer einfach mal hello world dass du diese hello docker container dann kann man noch limits eintragen wie viel ram und cpu das verwenden darf ich kann euch ehrlich gesagt nicht genau sagen in welcher einheit die cpu ist das ist irgendwas irgendwas super super spezielles das ist nicht einfach nur anteile von einem cpu kern das sind irgendwelche megahertz sind nicht h\u00f6chstwahrscheinlich nicht irgendwie irgendwie irgendwie cpu zeit oder sowas was auch immer das habe ich nicht so genau gecheckt was das jetzt f\u00fcr eine einheit ist zumindest ram ist relativ klar und man sollte auch limits angeben das ist sinnvoll weil wenn ihr meine anwendung habt mit memory leak dann frisst ihr euch im zweifelsfall euren kompletten note an ram voll wenn ihr da keine limits angegeben habt das hei\u00dft man sollte immer man kann durchaus gro\u00dfz\u00fcgige limits angeben muss ja gar nicht sein aber ganz ohne limits ist meistens doof weil man kann das auch so einstellen beziehungsweise macht es automatisch man angenommen ihr habt memory leak und die anwendung wird immer gr\u00f6\u00dfer immer gr\u00f6\u00dfer immer gr\u00f6\u00dfer und irgendwann nach einem gigabyte ram ger\u00e4t es ans limit und kackt ab dann startet kuba netis die neu und dann l\u00e4uft das ding wieder eine woche oder so was auch immer bis es wieder abkackt wegen memory leaks aber ganz ohne limits ist doof so container port tja geben wir einfach mal container port 80 ein wir haben kein container port weil dieses image kein port aufmacht so jetzt sagen wir mal kubectl deploy apply nicht deploy apply minus f cute wir machen hier unten mal was auf watch kubectl get pots ihr seht es gibt noch keine pots aber jetzt gibt es einen pot gibt es n\u00e4mlich einen cute app pot und ihr seht das ding ist gleich wieder abgekackt weil es ist kein pot also das ist der standard docker hello world pot den ich hier gestartet habe der macht nichts anderes wie hello world ausgeben kuck docker run hello world der macht nichts anderes wie hello world ausgeben sich wieder zu beenden deswegen da hier guck hier deswegen funktioniert das nat\u00fcrlich nicht kuba neta sieht aha anwendung startet anwendung krascht kuba netas denkt die anwendung krascht weil die l\u00e4uft ja nicht mehr und jetzt versucht er die neu zu starten und das krascht immer wieder doch doch der hat gepullt das geht das geht nur relativ relativ zackig das ist ja auch nichts nix dabei sind das docker image wof\u00fcr muss man docker auch installiert sein nein du brauchst docker nicht installiert haben ich habe einfach bei mir lokal docker installiert und das sind streng genommen es sind docker images ja aber streng genommen sind docker images ja gar keine docker images mehr sondern sind oci images oder wie dieser kram jetzt hei\u00dft open container images also das ist was hei\u00dft ein oci open container image open container initiative image alles klar w\u00e4re auch zu einfach gewesen also das ist eine spezifikation von container images die auch docker verwendet aber mittlerweile auch ganz viele andere container tools aus dem container \u00f6kosystem also es muss kein docker image in dem sinn sein aber bietet sich jetzt in dem fall an so jetzt haben wir ein hello world container der niemals ready wird weil er immer krascht wie gesagt logisch weil naja macht ja macht ja auch nix der beendet sich auch einfach wieder so bevor wir ein netzwerk container starten k\u00f6nnen wir ja machen gut im eich traffic container traffic kurmei kommen wir mal starten wir bauen gleich noch eine kleine eigene dotnet anwendung das dauert zwei minuten doch jetzt sehen wir mal hier das zum beispiel traffic kurmei das ist was mit einem webserver wir applyen die datei wieder ihr erkennt ein gewisses muster wenn ihr schon mal beispielsweise euch den terraform stream angeguckt habt von mir das war ja oder generell wenn die terraform oder solche infrastructure management tools verwendet habt ihr erkennt ein gewisses muster es gibt eine config datei dessen state dann applied wird wobei man sagen muss terraform ist da etwas als die reinen kopernet des manifestes weil da werden teilweise nicht die states getrackt also wenn ich das hier jetzt umbenenne da kann es durchaus sein dass der pot weiter l\u00e4uft so mal gucken zumindest jetzt starten wir mal anderen container so und wir sehen schon der alte container wurde gestartet und dann der neue container wurde gestartet der alte container wurde gestoppt ich nenne mal container es ist eigentlich ein pot steht man auch hier oben aber irgendwie kommt mir container einfacher von den lippen wurde gestartet und der scheint jetzt auch zu funktionieren denn da ist running der ist ab des abend running und das passt ja eigentlich so weit ganz gut jetzt muss man sich nat\u00fcrlich \u00fcberlegen ok ich bin \u00fcbrigens gar nicht sicher ob das ding standardm\u00e4\u00dfig auf pot 80 l\u00e4uft das ist eine gute frage doch kann man probieren mal aus wei\u00df ich gar nicht was macht das standardm\u00e4\u00dfig pot 80 ok alles klar dann ups na geil jetzt ha eine sache wo ich nicht wei\u00df wie das in meinem neuen terminal geht wie switche ich die terminals jetzt lull einfach gar nicht aber irgendwie geht das ich war noch nie in der dass ich musste das noch nie ausprobieren so jetzt ist nat\u00fcrlich die sache gut wir haben wir haben jetzt ein pot laufen mit einem service drin auf pot 80 wir kommen da allerdings nicht dran also selbstverst\u00e4ndlich mal karl lokal host oder so da geht nichts ich kann mir mal zum beispiel die mini cube ip angucken und dann kann ich sagen karl ip auf mini cube geht aber auch nett also der l\u00e4uft zwar und wir wissen auch dass da was auf pot das wird funktionieren ja wir k\u00f6nnen wir k\u00f6nnen auch mini cube tunnel und so ged\u00f6ns machen kann man alles machen aber man sieht der container ist nicht so wirklich erreichbar damit ein container erreichbar ist m\u00fcssen wir erst einen service davor bauen einen service kann man sich so als art ja nie kann man nicht wirklich laut balancer kann man nicht so als eingang kann man nicht so als eingang zu diesem zu diesem zu diesen pots vorstellen so und jetzt kommt das was ich am anfang gesagt habe man kann in so einen kubernetes manifest nahezu beliebig viele ressourcen definieren und wir machen das jetzt zu \u00fcbungszwecken so dass wir den service f\u00fcr dieses deployment also im prinzip f\u00fcr die anwendung hier unten drunter definieren ich finde das ich finde das an der stelle nicht verkehrt das geh\u00f6rt hier auch direkt zusammen das kann man kann man finde ich schon machen an der stelle da streiten sich tats\u00e4chlich da jetzt die geister dran wie man das macht und auch wie man das benennt ja ich kenne leute die machen ordner also gut ordner ist prinzipiell f\u00fcr einzelne anwendung nicht verkehrt machen ordner machen da drunter dann immer so was wie deployment extra service sonst wie manche machen auch eine datei mit dem namen von dem jeweiligen jeweilige anwendung dieser drin laufen lassen wollen und schreiben dann deployments und service in eine datei allerdings ingress in der anderen ist ein bisschen ist die musik zu laut wirklich passt doch eigentlich oder bin ich zu leise ich glaube die ist an sich einfach nur kratten bisschen sehr intensiv ok komm wir machen dass alle happy sind so jetzt ist es minimal leiser aber immer noch gut so also damit diese ports verf\u00fcgbar sind oder wahrscheinlich m\u00fcsste man eher sagen dass die pots aus diesem deployment verf\u00fcgbar sind also nicht nur innerhalb des clusters sondern auch von au\u00dfen muss man da ein service davon und service ist quasi so der eingang zu einem deployment denkt das kann man ganz gut sagen es gibt verschiedene m\u00f6glichkeiten wie man services definieren kann also verschiedene typen von von services der in dem fall jetzt ganz praktische service w\u00e4re so was wie den load balancer zum beispiel weil wir machen auch gleich noch mehrere mehrere pots hier aktuell ist es ja nur ein potter l\u00e4uft so also sagen wir jetzt hier mal service jetzt muss ich mal kurz \u00fcberlegen meta meta metadata kann das gleiche sein das ist \u00fcbrigens auch da streiten sich auch die geister dran ob das hier nicht irgendwie app hei\u00dfen soll und das irgendwie service da kann man sich da kann man sich auch dar\u00fcber streiten wie man das machen will so port port muss rein wo ich m\u00f6chte dass dieser service erreichbar ist von au\u00dfen in dem fall sagen wir auch mal pot 80 und taget port ist auch 80 kann man im kubernetes manifest auch ja klar warum nicht das ist ja ganz normal textart ganz normales jammel so das hei\u00dft wir legen jetzt ein service an auf port 80 der auf diesen service ich nenne es mal balance balance an der stelle oder nicht so und hier ist das entscheidende hier im selektor entscheidet man jetzt wohin dieser service seine requests weiter leitet also nur weil das in der gleichen datei drinne steht bedeutet es nicht automatisch dass das zusammen geh\u00f6rt also hier wo ich den selektor angebe selektor ab cute ab und ihr seht hier das hier ist ich vermute mal das hier ist das relevante zumindest hier drauf match der also der guckt danach was es f\u00fcr deployments gibt was es f\u00fcr pots gibt mit diesem passenden tag dazu und dann balance der da drauf also sprich ist es egal in welchen dateien die stehen also wir sagen wir wollen alle cute so und jetzt damit das ganze hier testweise funktioniert m\u00fcssen wir noch ein typ angeben man sieht es gibt mehrere typ es gibt cluster ip ich glaube cluster ip ist der default wenn ich nicht ganz falsch liege so mal gucken ob ich das aus dem kopf halbwegs hinkriege also external name wei\u00df ich jetzt gar nicht was hei\u00dft cluster ip hat hei\u00dft dieser service kriegt eine ip aus dem cluster zugewiesen also irgendeine ip aus der aus der cluster range not port bedeutet auf dem jeweiligen cluster knoten wo der container gerade l\u00e4uft kriegt er kriegt er was zugewiesen und also random port zwischen 30.000 und 35.000 oder so und in dem fall will ich load balance haben und jetzt applyen wir das ganze mal und dann hoffe ich dass ich nichts vergessen habe und der kram funktioniert so jetzt k\u00f6nnen wir zum beispiel hier unten mal sagen kubectl get service und man sieht da aha gucke mal da wir haben ein load balancer angelegt mit dieser cluster ip per external ip pending kann ruhig pendeln soviel es will und im port mapping das hei\u00dft wenn ich jetzt einen curl mache mini cube ip k\u00f6nnen wir uns merken wenn ich jetzt einen curl drauf mache da h\u00e4tte ich jetzt eigentlich gedacht dass es funktioniert keck wait das geht mit mini cube ich habe das selbst so mit mini cube schon gemacht da hat ja auch eine klasse ne ne die klasse die klasse ip da m\u00fcsstest du dich tunnen ne ne ne ne ne das funktioniert so glaube ich oder das ist immer jetzt jetzt jetzt lernen wir was okay wir probieren mal was aus braucht der lb nicht noch eine externe ip ne daten garten port mapping das sollte eigentlich ich meine wir k\u00f6nnten jetzt weiter gehen und noch noch den ingress davor machen aber sollte das nicht einfach so gehen jetzt ok ok also \u00fcbrigens ich zeige mal dass hier das hier funktioniert an der stelle nicht ja die du kannst die dinger nicht erreichen du kannst aber zum beispiel sagen mini cube ssh und dann m\u00fcsste es da drinne wahrscheinlich jetzt auch nicht ne ne ne ich brauche kein ingress wenn ich das hier ok mal gucken wir das mit cluster ip ist ich h\u00e4tte jetzt eigentlich wecken k\u00f6nnen das funktioniert so egal probieren wir mal aus apply ja guck mal mal hier unten was er macht cluster ip das das wird das wird so nat\u00fcrlich auch nicht funktionieren ich bin mir das muss das muss mit laut balancer noch funktionieren ja not port k\u00f6nnen wir auch ausprobieren dass in dem fall ja immer immer das gleiche in dem fall macht es ja keinen unterschied ist ja immer der gleiche note mal irgendwas habe ich irgendwas habe ich verkehrt gemacht irgendwas ergibt gerade keinen sinn ich habe wir k\u00f6nnen weitermachen und den und den reverse proxy davor pappen ach ich wei\u00df was ich mache was ich falsch mache ich benutze es doch auch falsch ich benutze es ich habe alles richtig gemacht ich habe alles richtig gemacht ich bin bescheuert also apply ist es stimmt alles passt mal auf es stimmt alles ich habe das nur falsch gemacht so nat\u00fcrlich die cluster ip ist ja nicht erreichbar intern deswegen muss ich jetzt folgendes machen mini cube service cute app und siehe da das service funktioniert da h\u00e4tte ich drauf gemusst ich h\u00e4tte ja auf die gem\u00e4bte auf den gem\u00e4bten port gehen m\u00fcssen guck auf den port h\u00e4tte ich gehen m\u00fcssen der map der port 30.000 430 466 auf port 80 von dem von dem container so so ist richtig jetzt funktioniert so das ist das ist mein service der hier l\u00e4uft das ist dieser hallo welt container von von traffic das ist der mini cube ip mit dem porter weitergeleitet wird ne load balance hast recht wahrscheinlich br\u00e4uchte ich das br\u00e4uchte das nicht wir k\u00f6nnen zwar wir k\u00f6nnen es mal weglassen und gucken ob es dann funktioniert nope siehst du funktioniert nicht notport w\u00fcrde funktionieren notport w\u00fcrde in dem fall wahrscheinlich auch funktionieren hallo mein gottes auto komplett wieder buggy ups ja das funktioniert auch also laut balancer braucht man an der stelle nicht wirklich und der service type notport auch externer ja das m\u00fcssen wie gesagt ich will es ja da gar so zumindest man sieht das service funktioniert damit was man jetzt ja schon sieht was ein bisschen doof ist ich meine wer kann hier wer hantiert damit den dinger wer kann damit rum hantieren mit den ports und und so also ich meine woher will jetzt zum beispiel jemand der auf mein service zugreift woher will der wissen dass er das \u00fcber port 3 2 2 5 9 machen muss das kann er nicht wissen so was man jetzt machen k\u00f6nnte ist man stellt das hier auf typ load balancer und mit einer wirklich externen erreichbaren ip dann m\u00fcsstest du aber die jedes mal wenn du das \u00fcber den cloud anbieter machst da m\u00fcsste es m\u00fcsste sich quasi \u00fcber jedes mal m\u00fcsstest du dann eine externe ip ressource anlegen in deiner cloud und hast dann am ende ganz viele externe viel sinnvoller ist es an der stelle so was wie ein reverse proxy davor zu pappen was wir auch schon gemacht haben mit docker compose mit docker mit zeug haben wir im stream schon \u00f6fters gemacht und zwar ein reverse proxy der dann auch am besten mit einem g\u00fcltigen let's encrypt zertifikat ausgestattet ist und der \u00fcber port 80 und 443 erreichbar ist das hei\u00dft neben dieser definition von den pots und dem service brauchen wir jetzt noch was f\u00fcr eingehenden traffic und das nennt sich bei kubernetes ingress das gegenteil von ingress ist egress nicht outgress so da gibt es verschiedene programme die man verwenden kann man kann traffic verwenden man kann engine x verwenden oder proxy oder sowas wir verwenden jetzt bei engine x aus dem einzigen grund weil bei mini cube schon in engine x addon f\u00fcr ingress dabei ist das macht man folgenderma\u00dfen also wenn ihr eine richtig fette kubernetes installation habt die bei jedem cloud anbieter l\u00e4uft oder so da k\u00f6nnt ihr nat\u00fcrlich daneben was ihr wollt aber mini cube bringt verschiedene addons mit und da ist zum beispiel dabei sowas wie ein ingress und das ist in dem fall engine x das hat noch mehrere praktische addons dabei zum beispiel registry die benutzen wir auch gleich noch wir machen n\u00e4mlich eine eigene dotnet anwendung die wir dann deployen hier \u00fcber zu der lokalen lokalen container registry ich muss noch mal kurz \u00fcberlegen in welcher reihenfolge wir das machen also ich w\u00fcrde sagen wir machen jetzt als erstes mal wir machen das in zwei stufen wir machen als erstes den ingress dann machen wir die dotnet an das drei zeilen dort netter es dauert nicht lang die dotnet anwendung und eine lokale registry und dann machen wir g\u00fcltige let's encrypt zertifikate f\u00fcr den ingress ich glaube das ist eine gute eine gute reinfolge das so zu machen ok das hei\u00dft als erstes enable wir mal unseren engine x addons enable ingress so und jetzt muss das muss ich ein bisschen abgucken leute das kriege ich n\u00e4mlich aus dem kopf nicht gebacken so also wir haben unsere pots bei der gelegenheit ihr seht hier vielleicht kommt mal mal im watch ihr seht hier wir haben aktuell nur einen pot laufen also quasi einen pot mit einem container drin das ist nat\u00fcrlich jetzt nicht sonderlich ausfallsicher man kann sagen das ist nat\u00fcrlich eh nicht ausfallsicher weil das alles auf meiner vm l\u00e4uft aber geben wir mal von aus es w\u00e4re nicht so ich habe im anfang gesagt eigentlich das kombiniert ist daf\u00fcr da um mehrere instanzen so von containern f\u00fcr uns zu verwalten deswegen kann man auch sagen man m\u00f6chte mehrere davon laufen haben da geht man hier hinten den moment wo muss man da hin templates ach fuck ich kann mir das immer nicht merken in spex spex muss man das irgendwo muss man replica reinschreiben man alter wo muss denn das wo muss denn das hin das muss doch unter spex oder ja das muss doch unter spex bin im deployment oder nicht ach da in die spex meine g\u00fcte mal ja genau ja ich bin gro\u00dfer jammel fan was das angeht also da kann man jetzt eingeben wie viele pots standardm\u00e4\u00dfig laufen sollen sagen wir mal zum beispiel f\u00fcnf ja komm vier wenn ich das jetzt noch mal apply werdet ihr feststellen auf einmal hoch guck mal da damit ja vier container das hei\u00dft unsere anwendung l\u00e4uft jetzt vier mal und wenn ich jetzt hier minikube service aufmach und funktioniert das auch kein ding aber es gibt noch mehr m\u00f6glichkeiten der chrome kecht ja scheint so wir k\u00f6nnen das ganze auch noch mal stimmt aber machen wir das guckt ihr seht jetzt auch die ip \u00e4ndert sich immer seht ihr das guck 12 6 12 10 11 also ihr seht die ip \u00e4ndert sich immer ich komme immer auf unterschiedlichen pots raus guck mal 69 pots machen wir jetzt mal jetzt wird hyperscaled hier apply ich hoffe das geht \u00fcberhaupt in meiner fremden gescheit b\u00e4mm jetzt haben wir die anwendung 69 mal laufen wenn ich jetzt in k\u00f6ln mache wir m\u00fcssen kurz warten bis alles erstellt wurde das ist leute ist das high performer mindset jetzt habe ich so guckt ich lande jetzt jedes mal auf einem anderen oder fast jedes mal auf einem anderen pot jetzt hier an der ip oben so und jetzt guckt mal was wo man jetzt schon mal sieht dass es sehr praktisch ist so ein container orchestrierungstool zu verwenden es gibt nat\u00fcrlich auch m\u00f6glichkeiten das automatisch zu skalieren anhand der last der ram auslastung und sonst was das machen wir heute nicht das n\u00e4chste mal geben wir auf was realistisches runter ich sag jetzt mal vier jetzt sage ich apply und passt auf es wird automatisch runter skaliert auf nur vier laufende pots warum ich linux verwende weil man gerade so netzwerk und technische sachen viel besser unter linux machen kann minikube unter windows geht auch dann macht er unter der haube glaube wie vm in der dann aber h\u00f6chstwahrscheinlich linux l\u00e4uft weil letztendlich sind es ja alles linux container und ich finde so sachen unter linux zu machen tausendmal angenehmer als unter windows allein schon dass man ein gescheites terminal hat ja das neue terminal unter windows ist halbwegs ertr\u00e4glich aber nix gegen ordentlich ordentliche linux umgebung so jetzt habt ihr gesehen man kann den kram hier auch ordentlich rauf und runter und runter skalieren wie man will am besten ist nat\u00fcrlich sp\u00e4ter man macht das automatisch aber das gucken wir uns mal in einem anderen stream an da habe ich auch gerade gesagt vor zwei minuten so soll das mal \u00fcberlegen was wollte ich als n\u00e4chstes machen genau also wir machen jetzt wir machen jetzt den ingress controller also es ist immer noch das problem dass ich hier in k\u00f6ln machen muss mit so einem komischen port normalerweise will man ja dass jemand so drauf zugreifen am besten noch mit dem dns namen ja darauf zugreifen kann also gar nicht gar nicht mal mit einem dns vor allem aber auf jeden fall \u00fcber port 80 und dann auf dem richtigen service rauskommt das hei\u00dft wir brauchen jetzt noch irgendein service der das ob dessen ingress ingress template hat ja wir brauchen das noch irgendein service der eben am eingang sitzt das entgegen nimmt und dann weiter leitet an den service wiederum an die pots das ganze nennt sich mal kubernetes nennt sich das ingress also nennen wir das hier mal cute app ingress wei\u00df ja nicht ob es minus in ingress wir k\u00f6nnen es nennen es aber wieder cute so wobei das tats\u00e4chlich sinnvoll ist das vielleicht ja anderweitig zu bedenken weil es k\u00f6nnte ja mehrere eing\u00e4nge geben aber wir machen es jetzt einfach mal so so host da k\u00f6nnen wir jetzt in dns name eintragen was auch sinnvoll ist ja dann nehmen wir jetzt nehmen wir jetzt zum beispiel mal kackel punkt keck w punkt services prefix alles service bei service kommt jetzt der name rein das service ist was wir hier oben definiert haben das ist cute app und bei port port 80 so und da muss man jetzt und da muss man jetzt ein bisschen gucken hier geht es drum um also um den target service nicht nicht hier irgendwie vom vom ingress controller dann brauchen wir noch ein paar andere sachen damit das ganze funktioniert das ist so magic kubernetes zeug und das muss ich jetzt abgucken weil das kriege ich aus dem kopf nicht hin wir verwenden als eingangs proxy engine x das hei\u00dft ich muss das wie gesagt das gucke ich ab das war ich aus dem kopf nicht ingress class name muss ich eintragen n\u00e4mlich engine x warum weil es nicht funktioniert jammel brauchste ultra wide screen monitore so und wenn ich jetzt alles richtig gemacht habe was ich mal hoffe dann sollten wir jetzt den ingress bekommen der sich f\u00fcr diese domain zust\u00e4ndig f\u00fchlt und dahin weiter leitet apply ok cube ctl get ingress what the fuck ingress ja haben wir host kackel punkt keck w services port 80 damit das damit das ganze funktioniert lokal m\u00fcssen wir folgendes machen und zwar in die host datei dieses ding hier mal eintragen also kacke punkt services aus dem kopf ip ist zuerst ja ip ist zuerst genau ip ist zuerst einer ip mit mehreren amt punkt 49 punkt 2 ist glaube ich die mini cube ip ok guck mal mal pink pink ok karl moment der wahrheit es funktioniert chrome was nein chrome und siehe da kackel kacke punkt services ist verf\u00fcgbar unter der domain und wir kommen auf unseren verschiedenen verschiedenen pots raus also der der traffic fluss ist jetzt so ich muss den namen aufl\u00f6sen in dem fall hier kackel punkt keck w punkt services da kommt die ip von meinem kubanet ist klasse da raus 1 2 1 6 8 4 9 punkt 2 dann geht das hier in den engine x ingress controller rein der leitet es weiter auf dieses backer der leitet wohlgemerkt alles weiter man kann auch sagen ich m\u00f6chte blo\u00df ab hier weiter leiten und sowas er leitet alles weiter an dieses backer mit dem namen cute app auf port 80 also auf den service cute app port 80 den service haben wir hier definiert target port port das ist das worauf der ingress controller das ganze weiter leitet und das service selbst verteilt das dann wieder auf alle m\u00f6glichen pots so hei\u00dft im endeffekt es funktioniert auch immer noch wenn ich hier reinschreibe 69 pots machen wir es mal apply damit skaliert das ganze hoch diagonales 5 hat hyperskalierungs cloud native high IQ zeug wie jetzt hoch skaliert und wenn ich jetzt wieder ein k\u00f6ln mache auf keck w punkt services sehen wir kriegen fast immer eine andere ip und wir landen jetzt auf einem von 96 69 verschiedenen pots kann euch das sogar zeigen dass das relativ viel viel ab kann was jetzt kein wunder ist wir k\u00f6nnen mal einen htp benchmark starten k\u00f6nnen man hatte tp benchmark starten sowas wie hey f\u00fcr eine minuten warum geht altpunkt nicht was sind das z mal zehn sekunden zehn sekunden volle pulle und ihr seht guck mal meine cpu last hier oben ist jetzt einfach mal ist jetzt einfach mal 100% oh ja habe ich hatte tp vergessen ihr seht auch meine laut geht \u00fcbelst hoch ja und wir haben 195.000 request geschickt alle hatten hatte tp status 200 hat wohl funktioniert ich bin mir jetzt gerade aus dem kopf nicht sicher wie man sich kommando zeilen m\u00e4\u00dfig die auslastung anzeigen lassen kann gibt es kubectl top oder so dinge also man kann zusatztool sowas wie lenz und sowas verwenden aber mal gucken ob kann man kubectl top gibt sowas hot node node minikube matrix api not verf\u00fcgbar ok minikube add-ons enable matrix server ist das so aber ok ok wo ich das restarten oder so das k\u00f6nnte jetzt in dem minikube geschichte sein dass das nicht will matrix server ist enabled der ram ist voll quatsch glaube ich nicht ach so ja doch das k\u00f6nnte tats\u00e4chlich sein ich war mal kurz vier aber ich glaube nicht dass da voll ist weil so dieser container braucht ja nix ich glaube da hat keine schmerzen so und wir sehen hier ist autocomplete das ok naja ich war mir k\u00f6nnen einfach mal minikube k\u00f6nnen das ding mal neu starten moment moment insecure konti registries ich glaube dann einfach probleme mit der matrix api wir starten wir starten das ding mal neu vielleicht geht es dann muss man manchmal machen ich hatte schon \u00f6fters das minikube lokal manche sachen ich wollte da muss man das neu starten oder funktioniert dass wir das mal neu starten dann gucken so weil als n\u00e4chstes gucken wir uns mal an wie man das mit lokaler image registry macht weil aktuell verwenden wir hier \u00f6ffentliche public verf\u00fcgbare docker images das ist ja meistens nicht der fall also wenn man eigene anwendung laufen lassen will da will man in der regel nicht diese anwendung public in einem docker container beziehungsweise in container image ins internet stellen zumindest viele wollen das nicht jetzt geht's in nebel ok minikube hat keinen bock minikube not found ok kann ich sowas machen wie get node also minikube wie dem auch sei pot ich kann mir anzeigen lassen kann man das so watchen lassen oder muss man das von hand machen wir f\u00fchren jetzt noch mal den kram hier aus also moment ne wobei es doch eigentlich ok keine ahnung na gut die selbst k\u00f6nnen ja gar nicht so viel sachen hier abkommen dass ich das sehe bin ich ein bisschen doof dass es nicht geht aber ach jetzt funktioniert not minikube auf einmal ok alles klar nice gut zu wissen hat wahrscheinlich einfach ein bisschen gedauert bis es gestartet ist und sagt jetzt w\u00fcrde ich eigentlich erwarten dass der ein bisschen mehr ausgelastet ist ist er nicht hatte aber kein bock drauf denkt sich nope was was interessieren mich 128.000 naja gut machen wir mal machen wir einfach mal mehr potts ist ja apply kubectl get potts haben wieder jede menge potts am start das ding ist hat einfach nur eine krasse krasse verzirkt ne wobei das ist jetzt jetzt pot erstellen so und jetzt nochmal attp benchmark mein ganzer rechner ist voll man es k\u00f6nnte auch sein dass das irgendwie richtig funktioniert weil das im container l\u00e4uft keine ahnung wei\u00df eh nicht wie man sich das gescheit auf kommando zeile anzeigen l\u00e4sst anscheinend so aber wirklich tut es ja nicht wirklich ja cpu 15 prozent na gut durchaus m\u00f6glich dass es nicht mehr sein kann als 15 prozent weil ich ja nur in zwei kerne docker container habe f\u00fcr mein minikube ach guck jetzt geht es ein bisschen hoch also wirklich wirklich wirklich toll zur \u00fcbersicht ist das ist gut aber jetzt war mal wieder mit dem was gescheites jetzt machen wir eine lokale container registry normalerweise ist es ja so wenn ihr anwendungen programmiert habt oder wenn ihr in einem unternehmen seid das der anwendung entwickelt hat und das auf kubanetis deployed dann ist das ja keine open source anwendung die irgendwo im internet verf\u00fcgbar ist ja das kenne ich das kenne ich habe ich aber noch nicht benutzt ich kenne das ich habe ich habe erst gestern mir ein paar kubanetis frontends angeguckt so und dementsprechend willst du ja auch keine container images bauen mit deiner close source super secret versicherungs banken anwendung und hier auf docker hab \u00f6ffentlich ins internet pushen so was du nat\u00fcrlich machen kannst ist wenn die firma in der cloud ist und in einem cloud anbieter vertraut kann man eine container registry verwenden also ich sag mal ihr kennt docker hab wahrscheinlich docker hab ist so die \u00e4lteste container registry die es gibt docker hab ist nicht die einzige github hat beispielsweise auch eine eigene docker hab kennen die meisten aber so docker hab ist halt public oder man meldet sich an und kriegt da auch ein private account aber es ist immer noch in der cloud das ist im internet also vielleicht m\u00f6chte man nicht seine anwendung oder die das unternehmen die anwendung ins internet stellen entweder benutzt man dann irgendeine registry wie docker hab allerdings blo\u00df angemeldet und private oder github private registry oder man hostet sich eine container registry selber ich pers\u00f6nlich w\u00fcrde eher dazu tendieren die container registry nicht selber zu hosten sondern die von jeweiligen cloud anbieter zu nehmen also wenn man zum beispiel bei amazon ist bei azure oder bei google oder geben wir mal bei google wenn ihr gcp google cloud kunde seid dann k\u00f6nnt ihr auch da eine container registry verwenden f\u00fcr unseren fall werden wir jetzt eine lokale container registry mit mit einem add-on was im minikube eingebaut ist verwenden das ist ganz praktisch dann muss ich mich hier nirgendswo anmelden docker hab technisch sondern kann das hier alles lokal machen dann machen wir eine kleine dotnet anwendung und die packen wir dann anstelle hier von who am i traffic container packen wir in einen docker container bzw. bauen container image und pushen das zu unserer lokalen container registry und bei der gelegenheit gucken wir uns dann auch noch ein paar einheiten an damit man so was updaten kann ohne dass die anwendung dabei ausf\u00e4llt so wir machen jetzt erstmal hier in den ordner nennen wir mal app so dotnet habe ich \u00fcberhaupt habe ich \u00fcberhaupt asp dotnet runtime installiert asp habe ich gar nicht null dachte mir noch irgendwie ich gehe mal kurz durch den chat durch was was da so gekommen ist ich habe wieder die h\u00e4lfte nicht gelesen wenn firmenleute mit kubernetes erfahrung suchen wo ist dann in dem job genau die schwierigkeit dass die firmen wahrscheinlich gar nicht wissen was sie wollen ich glaube viele denken sie wollen kubernetes weil man das kennt und damit ja automatisch alles agil wird und skaliert automatisch und alles super verf\u00fcgbar ist das ist aber ein trugschluss also man ist wahrscheinlich mit bekannter technologie mit leuten die sich damit auskennen besser als wenn man jetzt \u00fcbers kniepricht kubernetes f\u00fcr irgendeinen anwendungszweck wo es vielleicht noch gar nicht mal gro\u00dfartig sinnvoll ist ja sie juzu dankesch\u00f6n f\u00fcr den sub also ich glaube die gr\u00f6\u00dfte schwierigkeit ist dass die firmen wahrscheinlich nicht so genau wissen was sie wollen und ansonsten das f\u00fcr mich komplizierteste was kubernetes angeht ist eindeutig in dem \u00f6kosystem den \u00fcberblick zu behalten wie gesagt ich habe es am anfang gesagt ich bin auch nicht die pot sind running ich bin auch nicht das super krasse kubernetes checker ja und das \u00f6kosystem ist wirklich unglaublich komplex es gibt f\u00fcr es gibt f\u00fcr alles acht verschiedene l\u00f6sungen allein schon allein schon wie speichert man apk und passw\u00f6rter in kubernetes da gibt es f\u00fcnf sechs verschiedene varianten von einfach bis super von ein bisschen einfacher und super kompliziert ne habe ich nicht da als secrets stores die besser nicht weil als secrets steht das im klartext in dein jammel files drinnen das kann man machen aber sobald du den jammel files ins git einchecks stehen deine secrets im klartext drinnen so da gibt es verschiedene m\u00f6glichkeiten es gibt so sachen wie siehe secrets dann speichert man das ganze verschl\u00fcsselt verschl\u00fcsselt im git und entsperrt das quasi per key beim kubernetes cluster start dann gibt es irgendwas komisches von mozilla was quasi irgendwie so ein makro f\u00fcr jammel files ist dann gibt es sowas wie wollt und so da wird es aber schon langsam richtig kompliziert also gibt es unglaublich viel geschiss drumherum also gibt es ganz viel also da die ich glaube es gibt keinen den \u00fcberblick beh\u00e4lt ja da gibt es so sachen wie helm wo dran steht es ist ein paket ein package manager f\u00fcr kubernetes und du guckst dir mal helm chart definitionen an und musst erst mal kotzen wenn du die dateien siehst also da gibt es dann ja auch es ist auch nicht das einzige was es gibt gibt es auch wieder drei verschiedene sachen dann kannst du im zweifelsfall auch einfach sagen auch ich schei\u00df auf jammel und kannst es direkt \u00fcber terraform kubernetes provider und das ist also dadurch zu blicken ist tats\u00e4chlich f\u00fcr mich f\u00fcr mich das schwierigste ich blicke da auch nicht durch soll ich ganz ehrlich also es gibt es gibt so viele sachen wo ich keine ahnung von hab an sich tools was es f\u00fcr m\u00f6glichkeiten gibt und und sp\u00e4ter wenn es dann richtung service mesh und solche geschichten geht habe ich mich auch nur am rande mit besch\u00e4ftigt da wird sein ganz abgedreht also muss sagen so die basic funktion von kubernetes sowas wie das was wir gemacht haben das ist tats\u00e4chlich gar nicht so schwer wenn man sich ein bisschen an die namen gew\u00f6hnt hat wie die den kram nennen aber das was damit dann noch zus\u00e4tzlich kommt macht es kompliziert eine sache werden wir vielleicht gleich noch sehen mit manager und und zertifikate anlegen damit dann schon wieder ein bisschen komplizierter so wir bauen jetzt mal eine eigene net anwendung die wir benutzen als application application wie gesagt leute kein dotnet programmier stream das wird was ganz kleines was im prinzip nur aus drei zeilen oder so besteht also dort net new web so eine neue net anwendung und verwendest du das wim plugin wischl zu kontinue wenn ich wimm aufmache ist es der reale wimm noch nicht mal neowimm tats\u00e4chlich da stinkt normale wimm das ok ja ich traste traste alles hier voller trust dollen anwendung am start ja das habe ich da habe ich da habe ich jetzt gar nicht dran gedacht leute ja die ganzen cluster und storage m\u00f6glichkeiten gibt es ja auch noch du musst ja das m\u00fcssen wir uns ja auch noch angucken hier nicht heute im stream aber an der n\u00e4chsten stein guckt mal meine anwendungen hier sind ja komplett stateless aktuell es sollen sie im besten fall auch sein in kuba jedes ja aber irgendwo m\u00fcssen die daten ja hin der einfachste fall ist es gibt irgendwo den datenbank service und du connectest dich dahin liest und schreibst ein gutes aber manchmal brauchst du auch persistente dateien oder config files oder config eintr\u00e4ge mit environment variable und sowas also aber sp\u00e4testens bei dateien f\u00e4ngt es dann so an wo wie bekommst du wie bekommst du falls in den container da gibt es auch 1000 varianten von clusterfile system bis s3 du kannst dar\u00fcber s3 machen gibt es 1000 varianten und gerade diese vielfalt und dann die auswahl aus dieser vielfalt was lohnt sich denn jetzt f\u00fcr den jeweiligen anwendungsfall anwendungsfall ja das ist das ist moment ist glaube ich nicht die landkarte die ich kenne doch das ist die landkarte ich kenne ja die ist wirklich nice ja und das ist beim weitem nicht alles was da drin steht wenn ich reingucke gibt es viele sagen es ist vieles weil ich noch nie davon geh\u00f6rt habe und auch einiges was hier noch fehlt also da den \u00fcberblick zu behalten was es alles gibt und dann das passende auszuw\u00e4hlen f\u00fcr den anwendungsfall den man hat das ist fast das schwierigste an der ganzen kubanides geschichte und was man ehrlicherweise auch sagen muss was auch schwieriger ist als mit vms ist so ich meine ich habe jetzt hier drei pots laufen alles sch\u00f6n und gut aber wenn dann mal irgendwas nicht l\u00e4uft in so einem kubanides cluster rauszufinden warum da jetzt also mal so eine anwendung zu debuggen in so einem kubanides cluster ist gar nicht mal so einfach im prinzip k\u00f6nnt ihr eigentlich nur folgendes machen logs f\u00fcr den container k\u00f6nnt ihr euch angucken so viel mehr dbug m\u00f6glichkeiten habt ihr nicht es gibt noch die m\u00f6glichkeit sich rein zu connecten in den container das k\u00f6nnen wir uns k\u00f6nnen wir uns auch gleich angucken also man kann ja mit docker was ist docker exek minus it und bin w\u00e4sche also wir machen das mal docker beispiel ja ich krieg das immer nicht aus dem kopf hin was habe ich verkehrt gemacht docker exek und wenn er schon l\u00e4uft docker exek wenn er schon l\u00e4uft docker exek kann man sich ja in lokale docker container rein verbinden das gleiche gibt es auch gibt es auch cube ctl exek oder kann man sich versuchen in wei\u00df ich aber aus dem kopf \u00fcberhaupt nicht wird es funktionieren kann man sich versuchen rein zu connecten in die container wenn es da zum beispiel eine shell gibt und sowas es minus minus kommandok die haben aber die haben aber aktuell keine shell aber nicht dass du trotz ist es trotzdem schwierig so was so was zu dbuggen also da muss ich sagen das ist immer noch wenn du so eine vm hast wurde dich rein connecten kannst und da dann rum wurschteln kannst drinne angenehmer so wirklich rauszufinden wo ist der fehler zu gucken ok warum funktioniert jetzt die namensaufl\u00f6sung nicht und so was also so ein container deployment zu dbuggen finde ich pers\u00f6nlich eine ganze ecke anspruchsvoller ich habe das leider wieder zugemacht anspruchsvoller als eine vm wo was drinne l\u00e4uft hei\u00dft die frage wo kommen deine locks raus deine locks sind ja wahrscheinlich da nicht lokal in dem container sondern auf irgendeiner lock aggregierungs loki oder irgendwas oder lock sammel plattform oder elastix und was gibt es denn da alles mir f\u00e4llt es muss loki ein von dem was ich verwenden w\u00fcrde kibana gibt es da noch und und zeugs jenkins nicht jenkins ist kein lock lock sammel plattform das ist das falsche was ich hier auf mache das ist nicht gecheckt dass ich hier moment ich mache das writer ist wieder zu pepega ok hat es hat es wieder nicht gecheckt hier max nepos minikube ja trust ok wir machen anwendung wo ich noch ein paar Sachen zeigen kann hatten wir hier noch nie rider offen auf der neuen vm ich glaube nicht ich glaube das ist neu ok sekunde da m\u00fcssen wir erst mal was einstellen file settings short cuts ja fast fast ein action braucht wieder den standard short cuts ok ja ja remove und editor font ja mal wegen jet planes mono soll mir recht sein ok so jetzt jetzt sieht man ja hier was ach ja und sie pass mal auf leute flashbang ich war noch ein bisschen gr\u00f6\u00dfer so also wir machen jetzt wirklich eine ganz simple minimale dotnet anwendung also hello world wollen wir nicht wir wollen return cute chatter so und dann m\u00fcssen wir noch noch einstellen hier war name gleich wir machen das auch gleich mit environment variablen dass man auch was weiteres lernen kann environment variablen get so wunderbar reicht gut das ist unser unsere minimal dotnet web anwendung das reicht auch mehr brauchen wir zum testen an der stelle auch nicht reicht voll und ganz aus wir machen jetzt noch zwei sachen oder besser gesagt eine sache die sehr n\u00fctzlich ist und zwar sagen wir noch ich glaube bilder bilder ad health checks nee service ad health checks und dann bei ad ad nee ach fuck wie war das map map health checks und zwar das ganze unter slash helft wisst ihr warum helft und weil aus irgendwelchen gr\u00fcnden die kubanete leute sich gedacht haben es ist eine tolle idee das mit z hinten dran zu schreiben wahrscheinlich weil das andere schon zu oft belegt war oder so keine ahnung worum das herkommt aber die finden es cooler dass das mit slash helft vielleicht wei\u00df das deck overflow auch warum das so ist historisch die it comes from google internal best practices ok leute ihr habt es geh\u00f6rt helft ist google best practice excellent danke google so das ist unsere dotnet anwendung das war es auch schon mehr brauchen wir nicht was wir jetzt machen m\u00fcssen ist ein image bauen f\u00fcr diese dotnet anwendung und das mache ich ganz einfach ich klaue mir den image weil habe ich n\u00e4mlich schon vorbereitet kioko dankesch\u00f6n f\u00fcr den sub und sorry falls ich irgendjemanden \u00fcbersehen habe mit subs subscriptions ad docker support linux docker file das kommt alles weg weil das rider template kacke ist und jetzt b\u00e4rm soll ich das noch mal kurz erkl\u00e4ren oder wollen wir das docker feil einfach zur kenntnis nehmen und nicht mehr reingucken kann ich den stream als arbeits- oder weiterbildungszeit absetzen ich w\u00fcrde sagen schon wir machen hier schon wichtige wichtige dinge so weil mit diesem docker feil kann ich jetzt aus meiner dotnet anwendung ein container bauen und diesen image bauen und dieses image kann ich in eine container registry pushen und auf diese container registry kann ich dann an der stelle hier von kubernetes drauf zugreifen und meine eigene anwendung aus meiner eigenen container registry ziehen und nicht mehr aus einer \u00f6ffentlichen container registry im internet ja so sieht es aus wir k\u00f6nnen mal kurz schauen ob mein bild funktioniert docker bild punkt ich spreche mal kurz ab docker bild minus t cute app schauen wir mal irgendwann ist das dann auch fertig gebaut so docker run it's minus minus m cute app jawoll da sind wir port 80 karl localhost achso ich habe keinen ports weitergeleitet lull pk 80 80 cute chatter gibt es nicht guck mal keiner unsere anwendung hatten back wir brauchen noch wenn wenn die environment variable nicht gesetzt ist brauchen wir noch default value excellent ok cute app noch mal bilden keine cute chatters bekannt ok run cute app also wunderbar ok das ist unsere absolut high IQs maximum IQ also bessere web anwendung geht nicht mehr und wenn ich jetzt slash health mache dann kommt healthy zur\u00fcck und wenn ich weglasse kommt nichts weil bei google sagt da muss ein z hinten dran gut das ist unsere web anwendung fertig aus das war es mit c sharp f\u00fcr heute demn\u00e4chst wieder mehr hier test machen das n\u00e4chste mal wobei man keine tests braucht wenn man immer fehlerfreien code schreibt dann braucht man keine doku keine tests und keine kommentare weil es geht ja eh also sagen wir unsere unsere unsere unsere anwendung und jetzt m\u00fcssen wir diese anwendung pushen in unsere lokale container registry ich habe hier schon mal in wei\u00dfer voraussicht mein kubanete ist gestartet mit insecure registries weil naja von meiner container registries kein g\u00fcltiges SSL zertifikat als erstes mal enable registry und irgendwann ist es dann durch ich kann euch auch mal zeichnen cube ctl get ports for all namespaces und da seht ihr hier kommt mal da cube system code s etcd bla bla bla und hier ist die container registry also die addons sind unter der haube auch nur container warum nicht pots tja muss man mal google fragen wie das sein kann gut und jetzt m\u00fcssen wir in diese lokale container registry unsere app pushen das machen wir nicht mit docker ich kann euch auch sagen warum wir das nicht mit docker machen weil docker es nicht mag wenn die registry kein g\u00fcltiges SSL zertifikat hat das muss man das kann man irgendwie konfigurieren bei docker fragt mich nicht wo also von der idee her w\u00fcrde man w\u00fcrde man das bilden und dann w\u00fcrde man sagen docker push und dann die registry ja man muss in irgendeine konfig feier kann man es reinschreiben aber wir nehmen einfach potman weil potman hat eine konfig option daf\u00fcr potman ist quasi ein docker klon von rett hat mit ein paar sachen die docker nicht kann und umgedreht so weil jetzt kann man sagen potman bild und jetzt kommt es darauf an wie man das image nennt damit das ganze funktioniert warum nicht pot woman fragt mal twitter heute ist heute ist equal payday wei\u00dft du das eigentlich leute equal payday ist heute jetzt da sollte man schon mal potman in pot woman wenigstens mal zumindest f\u00fcr heute umbenennen so also damit das ganze funktioniert damit man sein sein image in die lokale container registry pushen kann muss man das taggen und zwar richtig taggen und zwar mit der ip und mit der ip beziehungsweise dem namen von der container registry und mit dem image wie das wie das ganze wie das ganze hei\u00dfen also ich muss mal sagen wie potman bild wie gesagt mit docker geht das auch wenn es g\u00fcltiges zertifikat vor der registry hat also potman bild 192 wobei wir machen es anders wir machen das wir machen wir machen den script kommen wir sind wir sind mal super ordentlich wir machen bild.sh bin bash registry gleich hoffe ich schreibe das ist jetzt richtig registry gleich mini cube ip auf port 5000 l\u00e4uft die registry das wei\u00df ich weil ich nachgeguckt habe und wir k\u00f6nnen noch so was wie einen tag hinten angeben vielleicht vielleicht gar nicht so bl\u00f6d wenn wir tag hinten angeben so und jetzt kommt das was wir an potman kommandos ausf\u00fchren ist n\u00e4mlich potman bild minus t registry also in dem fall die ip und der port und der port name von der app und jetzt k\u00f6nnen wir noch einen tag hinten dran machen so und wenn das fertig ist k\u00f6nnen wir sagen potman push und das muss ich mir jetzt kopieren weil ich das aus dem kopf nicht wei\u00df minus minus tls verify gleich falls und dann funktioniert das auch alles \u00fcber htp beziehungsweise auch mit ung\u00fcltigen zertifikaten und jetzt hier den gleichen krempel ich habe \u00fcbrigens auch einen punkt vergessen so wenn ich das script jetzt richtig gebaut habe dann sollte der uns jetzt einen container image bilden und in die registry pushen ok was habe ich denn verkehrt gemacht potman bild minus t registry ah ich habe den tag vergessen anzugeben ok ja also 1.0.0 hrq app ok jetzt ok er baut und ihr seht es sieht fast aus wie bei docker nur dass jetzt potman ist wieso der code war richtig funktioniert ohne jegliche \u00e4nderung so image ist gepusht nice und was ich jetzt machen kann ist folgendes ich trage jetzt an der stelle einfach ein localhost ich kann an der stelle jetzt localhost verwenden weil die registry innerhalb des kubanier des clusters l\u00e4uft es funktioniert an der stelle localhost 5000 slash cute app doppelpunkt versionsnummer 1.0.0 und jetzt werdet ihr feststellen ich zeige jetzt mal watch mein karl keckel ihr seht hier das ist der aktuelle service der da l\u00e4uft das ist noch der hallo welt service das ist noch nicht noch nicht unsere web anwendung was ist k\u00f6ln minus s glaube ich f\u00fcr weniger output genau so das ist noch nicht unser unser web anwendung ich habe jetzt in dem fall hier in unserer in unserem manifest unserem kubanier das manifest das image ausgetauscht was ich als anwendung verwenden will vorher hatte ich hier diese traffic hello world ab und jetzt habe ich unsere eigene anwendung da rein gepackt die wir die wir vorher als container image verpackt haben und in diese registry gepusht haben und wenn jetzt alles funktioniert ich hoffe es mal wir machen hier wir machen hier parallel noch mal ein get pods dass wir n\u00e4mlich sehen wenn es fehler gibt studio muss mal kurz weg und jetzt sage ich apply und vorher immer raus apply b\u00e4r so alte container werden neue container werden gestartet alte container werden gestattet cute chat eier zack ich habe jetzt quasi neue anwendung deployed ohne das request kaputt gegangen sind zwischendurch wie euch vielleicht aufgefallen ist das nat\u00fcrlich schon nice gelb machen wir das mal weg also wir wissen jetzt dass mit dem image funktioniert so jetzt haben wir hier unsere eigene anwendung drauf laufen jetzt brauchen wir noch ein paar andere geschichten dass das ordentlich funktioniert weil aktuell ist es so und um euch das mal zu demonstrieren beziehungsweise dass man ich zeige es auch mal dass man sich besser vorstellen kann machen wir mal sexo machen wir mal 69 container ihr werdet feststellen gleich gibt es wieder jede menge container w\u00e4hrend das aufbauen wir machen mal eine ein ein dirty hack in unserem build script den sehe ich hier nehme ich gerade dr\u00fcben dass ich das auch aus anderen verzeichnissen ausf\u00fchren kann so abbild und jetzt machen wir mal eine neue versionsnummer von der app ja also zum beispiel okay ich \u00e4ndere ich \u00e4ndere der form halber auch noch was ja es gibt ja west version 1.0.1 von dieser app und zwar ist es cute chatter ja ich will ich will hier wobei irgendwas ist es jetzt okay schreibt man das \u00fcberhaupt so ist es jetzt noch noch mehr alles und das will ich jetzt einchecken also ich baue neues image draus aus der anwendung mit diesem mit diesem fix image version 1.0.1 pusht das wieder in die registry so und jetzt apply ich das und was wird jetzt passieren leute es wird nix passieren es wird nix passieren weil es hat sich ja nix ge\u00e4ndert es hat sich ja nix ge\u00e4ndert warum sind meine pots eigentlich alle pending da unten ich glaube ich habe zu viele kann es sein dass ich mit den ressourcen am limit bin dass das meine dass ich zu viele pots habe okay moment wir skalieren es mal runter auf was sinnvolles auf 10 oder so also wenn ich das jetzt applye und wieder curl mache werdet ihr vielleicht feststellen es ist immer noch die alte message warum na ja weil die versionsnummer die hier drin ist immer noch 1.0 ist ich will ja 1.0.1 haben so und jetzt wird euch gleich ein problem auffallen wenn ich das jetzt applye dann wird er updaten dann wird er updaten von dieser von dem tag 1.0.0 auf 1.0.1 und wenn ihr mal genau guckt was er hier unten macht wird euch vielleicht was auffallen der startet neue container und l\u00e4sst dann die alten container l\u00f6st dann die alten container aber euch wird auffallen das zeug ist relativ schnell ready das zeug ist relativ schnell ready das komisch und das ist aber auch doof weil wenn jetzt im falschen momenten ein request rankommt k\u00f6nnte es ja theoretisch sogar sein dass der request auf dem container landet der noch gar nicht richtig gestartet ist das hei\u00dft wir m\u00fcssen jetzt nicht nur den container starten sondern wir m\u00fcssen auch checken ob der container bereit ist ob der container bereit ist machen wir mit den health checks hier drinnen wenn du eine fehlerhafte version deploys der ist ja quasi ach so jaja jaja ach das ist eine gute idee das kann ich auch zeigen das kann ich auch zeigen guckt mal ich skaliere das mal ein bisschen runter auf sechs st\u00fcck erst mal wir werfen keine exception ich trage einfach eine tag ein den es nicht gibt also ich gehe mal kurz runter auf sechs das sieht man also mal angenommen ich sage jetzt hier nehm mal image version 1.0 2 die es noch nicht gibt und ich applye das jetzt dann werdet ihr feststellen der versucht es zu applyen aber er image pull oder macht nicht weiter und die alten laufen weiter das hei\u00dft ich habe obwohl ich eine fehlerhafte konfig haben das ist wirklich auch eine coole obwohl ich eine fehlerhafte konfig habe ein weiterhin funktionierenden service zwar die alte version aber egal ich meine besser als kaputt kaputt w\u00e4re glaube ich das bl\u00f6deste was was an der stelle passieren k\u00f6nnte so wenn ich das als wieder r\u00fcckg\u00e4ngig mache mal wieder version 1 rein sache play dann checkt er das ganze auch und er geht wieder auf sechs replikas hoch also ich habe angeheben ich m\u00f6chte sechs davon laufen lassen und das ist wirklich cool und das im gro\u00dfen stil ist eigentlich der grund schlechthin also dieses solche features in der richtung ist nicht explizit dieses ist der grund schlechthin warum kuba netis sich \u00fcberhaupt so durchgesetzt hat du kannst halt wunderbar deinen ganzen workload \u00fcber mehrere notes skalieren und du kannst relativ gut ausfallfrei deine software managen damit das ist eigentlich der haupts\u00e4chliche grund warum warum kuba netis so beliebt ist oder sich auch so durchgesetzt hat gab ja noch ein paar andere andere dinge es gab dann zwischenzeitlich mal docker swarm was ja immer noch gibt kaum jemand benutzt da gibt es noch ein paar andere geschichten wo mir grad nichts ein gibt es noch nomad gibt es noch das ist aber glaube ich auch noch mehr mit vms dabei f\u00e4llt jemand nehmen eine gute kuba netis alternative ein hat der canonical nicht auch mal den versuch mit lxd gestartet irgendwie sowas zu bauen was keiner haben wollte mesos mesos gibt es noch richtig apache war das apache openshift wobei ist openshift noch deutlich mehr nicht noch deutlich mehr dabei dass du da auch ne moment das ist nicht openshift das ist openshift also openshift ist die telekom cloud das wei\u00df ich die telekom cloud ist openshift aber das hei\u00dft ja nicht dass es das hei\u00dft ja nicht dass es schlecht sein muss wie hei\u00dft das andere wo noch vms und so dabei sind open stack meine ich ich meine open stack und die telekom cloud ist open stack darum war das nicht openshift open stack meine ich genau ja richtig richtig so sieht es aus so was wollte ich was wollte ich denn jetzt noch machen ja also ihr habt jetzt ja gesehen es sind jetzt ganz viele services neu gestartet worden aber die dinge waren sofort ready das will man ja nicht das hei\u00dft wir haben ja hier einen health check definiert service fabric okay mir f\u00e4llt schon au\u00dfer kuba netis gar nicht gar nicht mehr so viel ein so man kann jetzt an der stelle sagen hier wo man wo man replikas eintr\u00e4gt kann ich auch noch sagen strategy type rolling ich glaube rolling update glaube ich ist sogar der default da bin ich mir aber nicht ganz sicher rolling update und dann kann man das braucht man eigentlich gar nicht ja da kann man dann noch so was sagen wie max unavailable 1 und max max search 1 oder man kann auch 2 in dem fall so kann man jetzt einstellen wie er updates machen soll also rolling update ist glaube ich relativ selbsterkl\u00e4rend und wobei das ja noch nicht alles ist was wir machen m\u00fcssen und hier kann man jetzt ausw\u00e4hlen wie er vorgehen soll beim rolling upgrade also von diesen sechs replikas wie viele d\u00fcrfen nicht erreichbar sein w\u00e4hrend dem update und wie viele d\u00fcrfen mehr da sein als target also mit diesen settings w\u00fcrde folgendes passieren er k\u00f6nnte zum beispiel einen neuen container erzeugen schon im vorfeld bevor er den alten stoppt und dann k\u00f6nnen alten stoppen so dass kurzzeitig f\u00fcnf nur laufen und das zwischenzeitlich kurz auch mal sieben laufen k\u00f6nnen was ja wichtig ist weil bei einem update m\u00fcssen ja neue container gestartet werden alte container gestoppt werden ist halt die frage wie man das macht man kann nat\u00fcrlich auch hier mehr eintragen wenn man damit dacor ist dass auch mal von diesen replikas weniger laufen hiermit kann man das quasi ein bisschen feintunen wie man dieses update wie man dieses update durchf\u00fchren will wenn ich jetzt wenn ich jetzt sag wie ging das jetzt die deployment cube cube rollout cube ctl rollout restart wobei ich brauche ich gar nicht ich kann es einfach machen doch anscheinend ja ok cube ctl rollout restart deployment das sieht man jetzt macht er das halt entsprechend dieser policy das maximum einer eben hier immer abgecreated wird aber wenn ich jetzt hier zum version runter runter gehe auf version eins sach apply dann seht ihr ok der erzeugt zwei neue und er stoppt immer kann auch immer ein zwei austauschen immer ein mehr und ein weniger kann man das ein bisschen feintunen was er was er machen darf so was aber viel wichtiger ist wir brauchen noch health checks weil ihr seht jedes mal wenn der container gestartet ist er sofort ready das stimmt ja gar nicht der container ist gar nicht sofort ready der muss ja auch erst mal starten die web anwendung muss starten ich mein das geht schnell ja mein potman run cute cute app ach so das hie\u00df ja irgendwie anders was wei\u00df ich so ihr seht so ein container startet schnell ist mal exemplarisch lokal bei mir mit docker der startet schnell aber es ist trotzdem eine halbe sekunde wo der container gestartet ist aber noch nicht bereit ist requests anzunehmen und es k\u00f6nnte ja auch durchaus l\u00e4nger dauern ok keine frage so und daf\u00fcr gibt es solche sachen wie ready probe und lifeboard oder life nes probe und readiness probe das muss man das muss man abgucken wo das hin muss unter unter ports muss unterhalb von ports also dahin muss das an der stelle als wer cringe das kann ich dir nicht erz\u00e4hlen habe ich keine gro\u00dfe erfahrung drinne ich w\u00fcrde mal sagen mit datenbank migrations aber du willst die postgres version updaten oder was also willst du nicht deine datenbank intern die struktur updaten du willst die datenbank version updaten oder wie das eigentlich erwarten dass deine anwendung sowohl mit der alten als auch mit der neuen version funktioniert weil ansonsten ansonsten w\u00e4re das easy du updatest zuerst die datenbank wenn du die datenbank version updaten willst du updatest zuerst die datenbank dann die anwendung und dann ist gut das schema ok du willst so was wie eine datenbank migration oder so haben das ist eine gute frage habe ich keine erfahrung mit wie man das gescheit synchronisiert jetzt bestimmt irgendwelche super tollen best practice sachen f\u00fcr aber ich kann da nichts sinnvolles zu sagen ich kann jetzt h\u00f6chstens was zusammen reiben was mir da so auf dem stegreif einf\u00e4llt aber ich wenn du das schema updatest dann erweiterst du das schema ja in der regel oder machst du das schema incompatibel also geht die alte anwendung noch mit der neuen schema version oder nicht wahrscheinlich nicht wenn das so wenn das so voneinander abh\u00e4ngt m\u00fcsste ich mir gedanken zu machen kann ich dir so vielleicht an der chat ahnung wir haben hier viele 5 hat cloud native kubernetes 6 herz vielleicht f\u00e4llt da jemand was zu ein wahrscheinlich kommen die leute jetzt wahrscheinlich kommen die leute jetzt an und sagen ha ha null gar keine sql datenbanken mehr verwenden sondern einfach nur noch cloud native data basis wobei das halt nicht realistisch ist das ist dann wenn du die falschen leute was sie kommen dann immer mit so was so ja warum verwendest du auch nicht klaut technologie in k\u00fcrze bescheuern wer definiert was klaut technologie sachen ist und was nicht also das ist das ist was du hast zwerggr\u00fcnde ist ein sehr praktischer und realer anwendungsfall und darauf gibt es bestimmt auch eine gute l\u00f6sung habe ich aber keine erfahrung mit kann ich dir gerade nicht sagen zu also das naheliegendste ist nat\u00fcrlich du musst sicherstellen dass die alte anwendung auch mit dem neuen schema funktioniert also dass du kannst das schema zwar erweitern aber dass es nicht incompatibel zur alten anwendung machen dann funktioniert es dann ist easy dann kannst du zuerst die datenbank updaten und dann die anwendung und wenn das nicht so ist f\u00e4llt jetzt so spontan nicht so viel ein weil da m\u00fcsstest du ja sicherstellen dass du irgendwie so einen rolling update machst so die datenbank updaten dann wobei es halt je nach datenbank auch so ist dass du gar keine unterschiedlichen datenbank anspricht sondern alle datenbanken in dem cluster \u00fcber halt ein connection string wenn du so willst dass du gar nicht explizit sagen kannst ich will jetzt nur auf diese tja da gibt es bestimmt coole m\u00f6glichkeiten zu bis jetzt hatte ich das muss ich mich mit diesem problem noch nicht besch\u00e4ftigen wenn es soweit ist sag ich dir bescheid ich w\u00fcrde einfach sagen ist job der dba es k\u00fcmmert dich nicht drum dass sie das machen sag einfach nicht mein problem easy ja aber kommen wir mal zur container geschichte zur\u00fcck damit ich mitbekomme wann meine anwendung wirklich gestartet ist gibt es readiness probes und es gibt dieses autocomplete alter ist es schlecht man leifnis probe gibt es das eine ist wenn der container ready also wenn der container gestartet ist zum ersten mal und das zweite ist ob der container noch weiterhin funktioniert und ich copy paste mir das jetzt hieraus aus dem beispiel weil ich keinen bock habe das nochmal alles zu tippen so sieht das ganze dann aus aber gibt sogar eine hilfe dazu der guckt ob der ob der container ready ist und der guckt ob der container noch am leben ist und wenn diese das ist der unterschied wenn diese leifnis probe fehlschl\u00e4gt dann restartet kubernetes den container kann auch sehr praktisch sein falls die anwendung nicht komplett krascht sondern intern blo\u00df in so einem zustand ger\u00e4t dass sie nicht mehr richtig funktioniert also leifnis probe startet kubernetes im zweifelsfall den container neu und hier kann unter anderem dieser ready check entscheiden wann das denn in den load balancer aufgenommen wird es gibt auch startup probe ja aber das braucht man in der regel nicht genau so und wenn ich das jetzt apply dann werdet ihr vielleicht gleich was feststellen guckt mal das geht jetzt deutlich langsamer seht ihr das das das geht jetzt deutlich langsamer alles und warum weil er halt immer f\u00fcnf sekunden wartet bis das ding auch wirklich am leben ist also der wartet f\u00fcnf sekunden macht dann diesen health check ob es am leben seht es dauert deutlich l\u00e4nger aber jetzt haben wir den gro\u00dfen vorteil jetzt gehen keine requests mehr keine neuen requests mehr verloren also wenn ihr einen ganz ung\u00fcnstigen moment erwischt dass ihr gerade weiter geleitet worden seid auf einen server und der wird gerade weg gestartet oder so dann kann es sein dass man immer noch einen fehler kriegt aber es verschwindet gering und das kann ich euch jetzt auch zeigen ich mache mal einen ich mache mal den benchmark ok ich mache ich mache eine neue version wir gehen wieder zur\u00fcck zu cute cute chatter ich mache ich mache eine neue version 1.0.2 von unserem container pusht das in die registry update die container version und jetzt passt mal auf ich mache einen benchmark hey haben wir waren 30 sekunden dann war mal l\u00e4nger eine minute aber eine minute eine minute holle pulle requests auf diesen service ich fange jetzt an und jetzt mache ich das update apply wir k\u00f6nnen wir k\u00f6nnen nebenbei nebenbei zugucken cute pots ja ihr seht die pots werden neu gestartet und werden hinzugenommen und alles latest funktioniert nicht apply l\u00e4uft immer nur dann wenn sich das manifest ge\u00e4ndert hat latest funktioniert nicht es gibt tricks dass es mit latest funktioniert aber das stammt mich funktioniert das mit latest nicht so unser rolling update ist fertig und jetzt bin ich mal gespannt ob requests verloren gegangen sind das l\u00e4uft noch ok irgendwie das zum irgendwie spackt es rum wir warten mal kurz die minute ab es sp\u00e4mt es sp\u00e4mt immer noch heftig so requests und guck mal wir haben eine millionen requests gemacht und nur und jetzt halt euch mal von einer millionen requests sind nur w\u00e4hrend diesem rolling update 39 oder 40 requests mal als nicht erreichbar zur\u00fcckgekommen von einer millionen request 40 und wir haben unsere anwendung geupdatet im hintergrund ich wei\u00df wir hatten mal so einen krassen manager dude der gesagt h\u00e4tte jetzt 40 requests dann sollten wir uns jetzt mal auf die konzentrieren die nicht funktioniert haben haben wir vielleicht logs wo wir die bangen k\u00f6nnen warum diese requests fehlgeschlagen sind aber ich glaube das ist eine ganz gute ausbeute von einer millionen requests 40 requests fehlgeschlagen und das w\u00e4hrend wir unsere anwendung geupdatet haben also das ist schwierig hinzukriegen anderweitig h\u00e4ttest du 100 prozent wenn du kein crawling ab du meinst in rolling update machen w\u00fcrdest die rolling update ist das beste was man machen kann du hast als du hast als auswahl sollst du recreate ich bin den haus gebrauche ist das nix es gibt so verr\u00fcckte leute auf youtube die haben drei kubernetes cluster daheim ist unsinnig und was man an der stelle auch noch mal dazu sagen muss wir haben jetzt ja ihren kubernetes cluster selbst erstellt und verwalten den kubernetes cluster selbst normalerweise musst du dich mit so grundlegenden sachen wie skalierung und so gar nicht gro\u00dfartig auseinandersetzen wenn du ein managed cluster also nicht einfach nur ein managed cluster sondern so wenn du den autopilot cluster von google beispielsweise wenn es da brauchst du nur noch deployments machen damit m\u00fcssen wir uns auch noch besch\u00e4ftigen die n\u00e4chsten tage oder versions updates versions updates von kubernetes und solche geschichte da musste dich nicht mit besch\u00e4ftigen wenn du managed cluster verwenden also vieles von diesen basic sachen musst du gar nicht selbst machen zumal ich meine wenn du den benchmark jetzt ausf\u00fchrst sind dann alle 100 prozent kann schauen wir mal ich lasse noch mal laufen kann durchaus sein dass die meine cpu ist ja auch voll am anschlag dass die ein oder anderen requests einfach nicht nicht richtig durchgehen also managed cluster k\u00fcmmern sich um versions updates von kubernetes und so was schon da hat man viel arbeit nicht mehr was aber eigentlich auch ganz gut ist weil ganz ehrlich da h\u00e4ngt halt sehr viel so lowlevel kram dran wo es schwierig ist das alles das alles richtig zu machen und was auch ein gewisses tiefgreifendes know-how erfordert alle staub ja das waren die control set ich habe kein control set gemacht ich habe terminal auf und zu gemacht ja jetzt habe ich eine millionen responses und zwar alles 200 ja also wir hatten tats\u00e4chlich 40 requests verlust in unserem upgrade gibt schlimmeres oder von eine millionen requests ok chat wie hat es euch bis jetzt gefallen kubernetes stells wie gesagt ich bin ich ich bin selbst nicht ich bin ja selbst nicht das super super oberkombiniertes checker aber ich glaube so mit lokaler registry und so was hier das war schon ganz cool dann lasst uns doch jetzt noch mal kurz eine viertelstunde \u00fcberlegen was wir als n\u00e4chstes machen also als n\u00e4chstes eine sache die mir eingefallen ist ja environment variablen genau n so volume secrets cert manager das will ich unbedingt zeichen mit let's encrypt lokal dns challenge na service mesh das ist jetzt schon ein bisschen zu etwas dann vielleicht irgendwann noch mal ja und ends backends traffic gibt naja bisschen ja ok geht aber ich finde den namen so dass es da hat man eigentlich was recht logisches also was naheliegendes so nach dem motto man checkt seine config in geht ein wieder super cool benannt dazu kommt da nat\u00fcrlich aber auch noch so dinger wie automatische tests und also man k\u00f6nnte man zum beispiel sogar solche dinger machen wie man ganz abgedreht weiterspinnen man k\u00f6nnte ja zum beispiel sagen ok man hat noch eine test suite f\u00fcr diese test suite f\u00fcrs jetzt super krasse dort net anwendung und man l\u00e4sst es dann irgendwie automatisch noch bilden und l\u00e4sst die tests durchlaufen und l\u00e4sst sich dinger machen dann automatisch noch auf irgendeinem ce server das image bauen dass man das nicht lokal machen muss so was in der richtung gibt ja gibt ja viele viele dinger man kann ganz abgedrehte sachen machen ja also irgendwann was wir uns auf jeden fall noch angucken ist das finde ich pers\u00f6nlich eine coole sache weil du damit weil du hiermit kannst du solche manifest erstellen in pyson zum beispiel finde ich pers\u00f6nlich eine super geschichte ist eigentlich overkill und komplett unsinnig f\u00fcr vieles aber ich pers\u00f6nlich finde es sehr nice weil man halt zum beispiel so sachen definieren kann guck hier hier hier kann man zum beispiel sagen hier cube service load balancer cube deployment also das was wir eben im manifest definiert haben kann man hier drin der ja ich wei\u00df man kann auch hellen und so was verwenden ich pers\u00f6nlich w\u00fcrde dann aber fast sagen mir pers\u00f6nlich ist das hier finde ich den ansatz hier cooler weil man dann vollst\u00e4ndige programmier sprache und ich mache halt lieber das im source code als irgendwelche jamme files und description files und so bauen also das ist cool ja hellen m\u00fcssen m\u00fcssen wir uns aber der vollst\u00e4ndigkeit halber auf jeden fall angucken weil fast jedes zweite tutorial hei\u00dft ja in hellen repo enden und ausf\u00fchren zum beispiel secret manager secret manager sagt das auch customize habe ich keine ahnung von kein plan vielleicht k\u00f6nnen wir uns irgendwann auch mal terraform noch in kombination mit dem kubernetes provider angucken habe ich jetzt auch noch nicht gemacht dann gucken wir uns nat\u00fcrlich noch google cloud managed kubernetes und auto autopilot an das ist der eigentliche grund warum ich mich in den letzten tagen wieder ein bisschen mehr damit besch\u00e4ftigt habe weil ich wei\u00df dass bei meinem neuen arbeitgeber google cloud mit wahrscheinlich managed kubernetes oder autopilot cluster vielleicht darf sogar ich mir aussuchen was ich haben will einsetzen wer das hei\u00dft da muss ich ein bisschen gucken metal lb ich habe keine ahnung was das ist ich kenne kong kong ist auch so ein lautballon kein lautballon so ein api gateway ist das harambe ich habe keine ahnung was es mir sagen soll ehrlich gesagt netwerk lautballon was ja selbstverst\u00e4ndlich da habe ich ja ganz vergessen das m\u00fcssen wir noch mal hier matrix ja matrix auch nat\u00fcrlich da m\u00fcssen wir uns noch sidecar container angucken f\u00fcr so monitoring und logs also ihr seht schon die einfache die bild in kubernetes sachen die sind an sich recht easy ja kommt man recht schnell mit aber das ist ein loch ohne boden was da alles dran h\u00e4ngt dann ja volumes volumes ja haben wir ja schon und das was da dran ist wirklich ein fast ohne boden so bild in sachen den kubernetes jammel eintragen apply ja kein ding aber dieses ganze zeug was hier noch dabei kommt das ist halt extrem viel was es da gibt ja autodiscovery ja das sollten wir uns auch mal angucken ja ja genau ne traffic prominent engine ich bin ich bin team engine ich habe traffic selbst auch schon verwendet ich muss sagen ganz im ernst ich bleibe bei engine ich benutze kadi wenn super simpel sein muss traffic von hand konfigurieren ist eh abfakt und traffic macht halt in so einem umfeld hier schon irgendwie sinn weil es auch viele service discovery funktionen ganz ehrlich aber es ist so easy in engine x ingress zu machen warum soll ich ja traffic verwenden glaubt glaube ich einen bogen erst mal drum machen ja schreiben wir nochmal dass wir uns das dann auch grafisch angucken k\u00f6nnen dahinter wenn euch noch was einf\u00e4llt k\u00f6nnt ihr ruhig auch im n\u00e4chsten stream schreiben da k\u00f6nnen wir uns das noch angucken also wir machen jetzt auf jeden fall erst mal weiter mit zert manager und environments und volumes und so was im n\u00e4chsten stream ist ach du schei\u00dfe das ist nicht nice so nice jetzt f\u00e4ngt dieses wortling schei\u00dfe schon in deutschland an oder was hat er seine adresse geleakt oder oder was war da mega f\u00fcr ein arsch man ok leute ich wann machst du tiktok sein ja ich macht ich werd krasser kubernetes cloud native influencer auf auf tiktok leute es ist fast 22 ich muss jetzt eh mal ins bett au\u00dferdem muss ich mal ganz dringend kacken also das wird eine serie ja nicht unbedingt am st\u00fcck nicht unbedingt am st\u00fcck jetzt kubernetes jeden stream bis wir mit allem durch den immer mal wieder machen wir machen wir ja ok leute wir sehen uns dann bis denn zu", "segments": [{"id": 1, "start": 30.0, "end": 58.56, "text": "da bin ich leute heute ist es soweit kubornitis kubornitis stream"}, {"id": 2, "start": 58.56, "end": 69.48, "text": "oder oder auch kubanitis oder kubanitis oder kubanitis oder was auch immer"}, {"id": 3, "start": 75.48, "end": 82.44, "text": "wir sind cool wir sprechen ja auch ubuntu nicht ubuntu aus sondern jubantu also sagen wir nat\u00fcrlich"}, {"id": 4, "start": 82.44, "end": 98.28, "text": "auch eindeutig auch kubanitis kubanitis das wird \u00fcbrigens heute als es \u00fcbrigens nicht"}, {"id": 5, "start": 98.28, "end": 105.72, "text": "sponsort bei hetzen oder so wir machen heute die sachen noch lokal und es wird erstmal auch nur"}, {"id": 6, "start": 105.72, "end": 111.76, "text": "horizontal skaliert heute ich wei\u00df ich wei\u00df das ist eigentlich nicht das performer mindset wie"}, {"id": 7, "start": 111.76, "end": 118.16, "text": "hei\u00dft das doch performer mindset oder wie hei\u00dft das shit high performer halt das nicht das high"}, {"id": 8, "start": 118.16, "end": 130.96, "text": "performer mindset es wird heute auch nicht der einzige stream sein dazu weil da gibt es einiges"}, {"id": 9, "start": 130.96, "end": 137.4, "text": "das ist eine fass ohne boden und es ist jetzt auch nicht so als bin ich zum aktuellen zeitpunkt"}, {"id": 10, "start": 137.88, "end": 145.0, "text": "der krasseste kubanitis experte den es auf diesem planeten gibt ich habe damit schon was gemacht ich"}, {"id": 11, "start": 145.0, "end": 150.12, "text": "habe die letzten tage auch so ein bisschen noch um probiert und ich habe schon mal in die deployment"}, {"id": 12, "start": 150.12, "end": 157.6, "text": "jammel gemacht und dann war es zum laufen gebracht und also ganz komplett ahnungslos bin ich nicht"}, {"id": 13, "start": 157.6, "end": 167.56, "text": "aber ich bin jetzt nicht der der der krasseste profi ever jetzt jetzt aber was machen wir"}, {"id": 14, "start": 167.56, "end": 177.92, "text": "ein bisschen ahnung haben verstehe nicht was ihr meint mit skalieren das wirst du dann im laufe des"}, {"id": 15, "start": 177.92, "end": 185.16, "text": "streams sehen was mit skalieren gemeint ist da wird hyperscaled wird da da kann man dann richtig"}, {"id": 16, "start": 185.16, "end": 201.16, "text": "sch\u00f6ne tolle cloud buzzwords draus machen die hyperscalenden cloud native container infrastructure"}, {"id": 17, "start": 201.16, "end": 211.64, "text": "es keine ahnung ja fast jeden tag wenn ich nicht stream abends"}, {"id": 18, "start": 211.64, "end": 219.28, "text": "mit den mit den gleichen leuten wieder n\u00e4her ich bin auch am spiel ich habe gesehen du bist"}, {"id": 19, "start": 219.28, "end": 225.44, "text": "nicht \u00fcber level 11 rausgekommen nicht so ging nicht so weit gatschi roll"}, {"id": 20, "start": 225.44, "end": 233.92, "text": "w\u00e4hrend der stream anl\u00e4uft leute ich mache mal meine vm an wobei wir heute beim cool aussprechen"}, {"id": 21, "start": 233.92, "end": 241.2, "text": "ich mache meine vm an und ich mache mal ein update bevor wir jetzt hier gro\u00dfen gro\u00dfen kubanete"}, {"id": 22, "start": 241.2, "end": 246.2, "text": "stellt man ja also leute wir gucken uns das wir gucken uns das alles an also das wird jetzt nicht"}, {"id": 23, "start": 246.2, "end": 250.92, "text": "also wenn ihr jetzt erwartet wir machen jetzt hier kubanetes tutorial wie sagt man so sch\u00f6n"}, {"id": 24, "start": 250.92, "end": 259.6, "text": "from zero to hero wenn man cool sein will dann nennt man das sowas das machen wir nicht also ich"}, {"id": 25, "start": 259.6, "end": 262.84, "text": "erkl\u00e4re euch ein bisschen was und ich zeige euch auch ein bisschen was und wir probieren ein paar"}, {"id": 26, "start": 262.84, "end": 269.16, "text": "sachen aus aber die sinne dieser streams ist ja auch einmal dass ich euch was zeigen kann und"}, {"id": 27, "start": 269.16, "end": 274.8, "text": "auch dass ich selbst besser kann das ist ja immer so 11 ich meine ich mache meistens nicht sachen wo"}, {"id": 28, "start": 274.8, "end": 279.76, "text": "ich komplett planlos bin aber wenn man die dann noch mal erkl\u00e4ren muss dann ist nat\u00fcrlich immer"}, {"id": 29, "start": 279.76, "end": 284.36, "text": "ein bisschen schwieriger versteht selbst besser war ein paar tr\u00e4g horizontal skalieren ist doch"}, {"id": 30, "start": 284.36, "end": 288.16, "text": "mehrere maschinen hochfahren k\u00f6nnen zum skalieren oder vertikal ist mehrere leitungen freigeben"}, {"id": 31, "start": 288.16, "end": 295.92, "text": "ich bin mir ehrlich gesagt gar nicht so sicher ich glaube horizontal skalieren ist mehrere"}, {"id": 32, "start": 295.92, "end": 302.12, "text": "mehrere potts starten und vertikal skalieren ist mehrere notes hinzuf\u00fcgen"}, {"id": 33, "start": 302.12, "end": 312.28, "text": "keine ahnung bin ich bin ich so im hyper hyper scale high performance mindset"}, {"id": 34, "start": 318.76, "end": 328.68, "text": "ach du hast recht genau horizontal jaja ne ne stimmt doch mehr leistung mehr leistung ist ja"}, {"id": 35, "start": 328.68, "end": 334.24, "text": "im prinzip mehrere notes hinzuf\u00fcgen wobei du auch du musst gleichzeitig du musst gleichzeitig"}, {"id": 36, "start": 334.24, "end": 341.88, "text": "ein bisschen vertikal skalieren dass du besser horizontal skalieren kannst oder glaub da m\u00fcssen"}, {"id": 37, "start": 341.88, "end": 345.52, "text": "wir mal eben business coach fragen wir uns das erkl\u00e4rt wie das wie das richtig funktioniert"}, {"id": 38, "start": 349.16, "end": 358.04, "text": "wir skalieren prinzipiell nur diagonal korrekt das ist richtig das haben wir schon vor eine"}, {"id": 39, "start": 358.04, "end": 363.16, "text": "weile festgestellt dass f\u00fcr uns diagonal skalieren das beste ist dann musst du dich nicht mit mehr"}, {"id": 40, "start": 363.16, "end": 373.44, "text": "mit so komischen noobkack abgeben wie ob das jetzt vertikal oder horizontal oder y-achse x-achse z-achse"}, {"id": 41, "start": 373.44, "end": 381.36, "text": "wir wir machen naja wie es skalieren diagonal dreidimensional also in alle drei dimensionen"}, {"id": 42, "start": 381.36, "end": 397.48, "text": "wird skaliert du brauchst mindestens f\u00fcnf kuppernets cluster daheim dass du dass du"}, {"id": 43, "start": 397.48, "end": 405.56, "text": "dem kerl auf youtube konkurrenz machen kannst wir skalieren sogar runter das ist richtig wir"}, {"id": 44, "start": 405.56, "end": 409.72, "text": "skalieren tats\u00e4chlich heute auch runter wir skalieren mal container von 8 auf 4 zur\u00fcck"}, {"id": 45, "start": 409.72, "end": 417.92, "text": "und so was ich war mal kurz bevor es richtig los geht aber kurz den ganzen schitt hier zu"}, {"id": 46, "start": 417.92, "end": 424.8, "text": "das haben wir hier noch swelt zeug alles weg ich war mal kurz ein update und dann starte ich mal"}, {"id": 47, "start": 424.8, "end": 432.24, "text": "die vm neu dass es auch was wird hier dass wir top aktuelles kubernetes machen k\u00f6nnen"}, {"id": 48, "start": 432.24, "end": 444.64, "text": "autokonale skalierung was kommt man was kommt dann als als n\u00e4chstes parallelogramm skalierung"}, {"id": 49, "start": 444.64, "end": 455.44, "text": "relativistische skalierung h\u00e4tte ich noch zu bieten dann skalieren wir nicht nur in"}, {"id": 50, "start": 455.44, "end": 463.56, "text": "alle raumdimensionen sondern auch zeitlich was was haltet denn davon dass man kann das"}, {"id": 51, "start": 463.56, "end": 470.4, "text": "beck was wird moment was so warum kompiliert er gerade in den schitt was macht er da 7 zipp"}, {"id": 52, "start": 470.4, "end": 479.0, "text": "ich habe was viel ich zeige euch mal kurz was bevor wir kubernetes anfangen ich habe"}, {"id": 53, "start": 479.0, "end": 484.36, "text": "was was ein cooles projekt auf github gefunden wenn man keinen bock auf 7 7 zipp ged\u00fcrnt"}, {"id": 54, "start": 484.36, "end": 497.2, "text": "hat unter linux nenn sich pzip das ist das macht alle m\u00f6glichen formate unter anderem auch 7 zipp"}, {"id": 55, "start": 497.2, "end": 506.56, "text": "und und es hat ein gui dabei einfach 8 zipp nehmen ja das ist schon ganz sch\u00f6n 6 hat ich"}, {"id": 56, "start": 506.56, "end": 517.4, "text": "habe ich ok leute ich ich muss mal kurz was zum ich muss mal kurz 7 zipp entfernen brauche ich"}, {"id": 57, "start": 517.4, "end": 524.72, "text": "erinnert so und dann zeige ich euch mal was wir heute vor haben beziehungsweise was ansteht das"}, {"id": 58, "start": 524.72, "end": 533.24, "text": "ist vielleicht nicht das alte 7 zipp n\u00e4her so alt ist es nicht ist es last week geabdeut"}, {"id": 59, "start": 533.24, "end": 544.2, "text": "updated oder doch was ist die letzte version 9.1 ist doch gut alles sogar f\u00fcr die ganzen cute"}, {"id": 60, "start": 544.2, "end": 557.24, "text": "cutie chats chatter also ich wollte noch ein update machen gut lassen wir den erst mal"}, {"id": 61, "start": 557.24, "end": 568.56, "text": "updaten also um was geht es heute ich versuche jetzt mal so ganz grob mal auszuholen ja erinnert"}, {"id": 62, "start": 568.56, "end": 575.8, "text": "sich noch eine an zipp drives die hat mit zipp an sicher nichts zu tun das war eher so halt das"}, {"id": 63, "start": 575.8, "end": 583.68, "text": "zipp so wei\u00df gar nicht warum die so hei\u00dfen vom ger\u00e4usche oder so zipp drive ja iomega zipp"}, {"id": 64, "start": 583.68, "end": 591.88, "text": "drive da wurde nichts gezippt immer eine 100 mb diskette war damals schon richtig krasser shit"}, {"id": 65, "start": 591.88, "end": 599.2, "text": "wenn die normalen disketten hatten wie viel 1,44 mb oder 1,2 ich glaube es gab drei disketten 800k"}, {"id": 66, "start": 599.2, "end": 608.36, "text": "1,2 und 1,44 oder so was g\u00e4r war es gab war das nicht so ne 3,4 gab es nicht das ja gut vielleicht"}, {"id": 67, "start": 608.36, "end": 615.84, "text": "sp\u00e4ter mal aber ich kenne also die 1,44 waren glaube ich die verbreiteten wie ist ja dass ich"}, {"id": 68, "start": 615.84, "end": 620.36, "text": "mich da dass ich mir damals wieder krasse heckermann vorgekommen bin ich habe alte disketten vom"}, {"id": 69, "start": 620.36, "end": 627.8, "text": "amiga genommen das waren ja auch so floppy floppy disk floppy disk und dann habe ich die in mein pc"}, {"id": 70, "start": 627.8, "end": 633.32, "text": "laufwerk reingesteckt und dort unter windows rechts drauf geklickt und gesagt formatieren und dann"}, {"id": 71, "start": 633.32, "end": 638.4, "text": "hatte ich auf einmal ganz viele leere disketten weil ich die amiga dinger recycelt habe und da"}, {"id": 72, "start": 638.4, "end": 643.68, "text": "habe ich boah da habe ich mich richtig geil gesagt mann bin ich bin ich geiler hier heckermann ich"}, {"id": 73, "start": 643.68, "end": 651.24, "text": "habe quasi disketten aus dem nichts erschaffen dabei waren das ja ganz normale floppy disk"}, {"id": 74, "start": 651.24, "end": 657.24, "text": "halt f\u00fcr den amiga das habe ich nicht habe ich damals nicht gecheckt also ja amiga gescaled"}, {"id": 75, "start": 657.24, "end": 672.0, "text": "genau so update ist durch update ist durch ich glaube es war ein kernel update dabei"}, {"id": 76, "start": 672.0, "end": 680.84, "text": "deswegen reboote ich mal 5x dankesch\u00f6n f\u00fcr den sub ich habe oben \u00fcbrigens glaube ich noch"}, {"id": 77, "start": 680.84, "end": 691.24, "text": "ein \u00fcbersehen stripes dann wieder subscrib poggers danke f\u00fcr die subs leute so dann starten wir mal"}, {"id": 78, "start": 691.24, "end": 714.52, "text": "das verdammte spiel und jetzt kann ich sag guck mal ne das war ja kein bock auf krasse giga chat"}, {"id": 79, "start": 714.52, "end": 725.64, "text": "aussprache jetzt kann man damit anfangen das ketten war das stimmt das ketten war schon cool"}, {"id": 80, "start": 725.64, "end": 737.48, "text": "dass sie der klick zu dieses klick dieses hatte schon was nutzt du zsh ja mache ich und wenn ja"}, {"id": 81, "start": 737.48, "end": 749.12, "text": "welches theme ist das das ist gar kein theme das ist also du meinst jetzt mein termin oder"}, {"id": 82, "start": 749.12, "end": 755.56, "text": "wirklich hier den prompt von der shell ich vermute mal weil du zell das was meinst du den prompt"}, {"id": 83, "start": 755.56, "end": 763.96, "text": "also der prompt ist starship allerdings ein custom meister ein custom meister ein"}, {"id": 84, "start": 763.96, "end": 769.6, "text": "custom meister starship prompt wenn du den prompt sehen willst gehst du hierhin auf"}, {"id": 85, "start": 769.6, "end": 779.28, "text": "wuppe loss dot files config starship und da siehst du das ich habe den base prompt genommen von"}, {"id": 86, "start": 779.28, "end": 786.36, "text": "starship du musst du noch sagen ob du wirklich shell oder terminale meinst weil terminale ist"}, {"id": 87, "start": 786.36, "end": 797.92, "text": "anders so und ich habe dieses preset genommen von starship pastel powerline was wiederum selbst"}, {"id": 88, "start": 797.92, "end": 805.28, "text": "auf m365 princess basiert das habe ich genommen und habe ein paar customized sachen angepasst ja"}, {"id": 89, "start": 805.28, "end": 810.72, "text": "zum beispiel habe ich die schrift gemacht wie mein hintergrund ich habe hinzugef\u00fcgt dass wenn"}, {"id": 90, "start": 810.72, "end": 817.56, "text": "man in ein dotnet projekt geht die versionen da stehen und so was ich habe es auch zweizeilig"}, {"id": 91, "start": 817.56, "end": 823.68, "text": "gemacht weil ich zweizeilige prompts pers\u00f6nlich viel besser finde als einseilige prompts ja aber"}, {"id": 92, "start": 823.68, "end": 834.6, "text": "es ist etwas ist das hier aber angepasst und du kannst findest das hier ach du kacke das ist wild"}, {"id": 93, "start": 834.6, "end": 842.12, "text": "ja das ist das ist aber wahrscheinlich noch neuer oder fast so alt wie unsere cobalt"}, {"id": 94, "start": 842.12, "end": 849.84, "text": "geschichten die bei uns laufen wo ich mich auch immer frage wo finden die immer noch"}, {"id": 95, "start": 849.84, "end": 868.2, "text": "leute die den kram k\u00f6nnen wir zeit wird zeit f\u00fcr linux upgrade und dann das ganze einfach in"}, {"id": 96, "start": 868.2, "end": 880.48, "text": "free doors laufen lassen oder oder oder noch besser wine im docker container im container"}, {"id": 97, "start": 880.48, "end": 886.72, "text": "wei\u00df gar ob das gescheit funktioniert ja warum eigentlich nicht ja aber das will man ja"}, {"id": 98, "start": 886.72, "end": 890.88, "text": "wahrscheinlich nicht weil man so altes zeug hat dass es stabil l\u00e4uft ob das mit wine dann"}, {"id": 99, "start": 891.48, "end": 896.8, "text": "also was machen wir heute"}, {"id": 100, "start": 896.8, "end": 907.8, "text": "wo fange ich denn jetzt am besten an leute wo fangen wir am besten an"}, {"id": 101, "start": 907.8, "end": 917.44, "text": "serverschrank 24 wieso klingt auch ganz normal anbieter h\u00f6rt sich halt bisschen"}, {"id": 102, "start": 917.44, "end": 924.36, "text": "oldschool an aber warum warum ich klingt auch klingt auch ganz normal"}, {"id": 103, "start": 937.8, "end": 953.24, "text": "berat uns doch erstmal ob kubernetes native stellt oder mini cube zum ausprobieren"}, {"id": 104, "start": 953.24, "end": 958.84, "text": "reicht das lokal voll und ganz ach ja ich wei\u00df wo ich anfangen ich wei\u00df wo ich anfangen und zwar"}, {"id": 105, "start": 958.84, "end": 967.68, "text": "dass wir es heute nicht der einzige kubernetes stream und zwar wir machen da mehrere ich wei\u00df"}, {"id": 106, "start": 967.68, "end": 970.88, "text": "nicht inwieweit da bin ich wieder unschuldig wei\u00df ich inwieweit das alles so aufeinander"}, {"id": 107, "start": 970.88, "end": 977.68, "text": "aufbauen wird wahrscheinlich kein raspberry pi cluster wahrscheinlich schon in gewisser"}, {"id": 108, "start": 977.68, "end": 982.48, "text": "weise wir machen aber nicht immer so exakt die gleichen beispiel und zwar also wir brauchen"}, {"id": 109, "start": 982.48, "end": 986.28, "text": "mehrere wir brauchen mehrere streams jena es h\u00e4ngt auch ein bisschen davon ab wie weit wir"}, {"id": 110, "start": 987.28, "end": 996.0, "text": "aber es wird auf jeden fall auch aus praktischen gr\u00fcnden mehrere streams geben das erste ist"}, {"id": 111, "start": 996.0, "end": 1000.52, "text": "wir setzen uns damit lokal mal ein bisschen auseinander wir m\u00fcssen nicht gleich in die"}, {"id": 112, "start": 1000.52, "end": 1009.92, "text": "cloud in der cloud irgendwas bauen das n\u00e4chste ist wir gucken uns google cloud kubernetes an"}, {"id": 113, "start": 1009.92, "end": 1015.48, "text": "warum gucken uns google cloud kubernetes an weil dort wo ich am 1. april anfange zu arbeiten die"}, {"id": 114, "start": 1015.48, "end": 1019.56, "text": "alle google cloud verwenden ich wei\u00df nicht warum das sind die einzigen die ich kenne die google"}, {"id": 115, "start": 1019.56, "end": 1027.52, "text": "cloud benutzen aktuell wo ich bin ist es ist es quasi fast nur ausschlie\u00dflich azure ich"}, {"id": 116, "start": 1027.52, "end": 1032.92, "text": "habe schon ein bisschen was mit aws ich fasse nicht schon ein bisschen was privat gemacht mit"}, {"id": 117, "start": 1032.92, "end": 1038.72, "text": "google und mit aws w\u00fcrde ich sagen aber abstand am wenigsten aber ich kenne au\u00dferdem niemand der"}, {"id": 118, "start": 1038.72, "end": 1042.84, "text": "im professionellen umfeld google cloud verwendet gibt es allerdings genug nur weil ich keine"}, {"id": 119, "start": 1042.84, "end": 1048.88, "text": "kenne hei\u00dft das nix weil ganz einfach ich kenne nicht so viele l\u00e4den von innen die"}, {"id": 120, "start": 1048.88, "end": 1052.24, "text": "paar handvoll da die ich gesehen habe die haben keine google cloud verwendet gut es hei\u00dft ja"}, {"id": 121, "start": 1052.24, "end": 1057.32, "text": "nichts erfolgreich ist es trotzdem und ich habe mir ein paar videos angeguckt und angeblich ist"}, {"id": 122, "start": 1057.32, "end": 1065.64, "text": "dass dieses autopilot kubernetes management feature von der google cloud wohl \u00fcber diesen"}, {"id": 123, "start": 1065.64, "end": 1069.84, "text": "anwendungszweck am besten dann betreibst du n\u00e4mlich gar kein kubernetes cluster mehr selbst in der"}, {"id": 124, "start": 1069.84, "end": 1075.44, "text": "google cloud sondern kannst direkt pots deployen und die scaling und managen das f\u00fcr dich aber"}, {"id": 125, "start": 1075.44, "end": 1079.16, "text": "nat\u00fcrlich machen wir damit nicht fangen wir damit nicht gleich an weil das ist ja langweilig"}, {"id": 126, "start": 1079.16, "end": 1085.16, "text": "muss erst mal ein bisschen so die grundlagen uns angucken dass man wei\u00df wie das funktioniert"}, {"id": 127, "start": 1085.16, "end": 1089.28, "text": "weil nur was man macht von hand zumindest ich so ein bisschen selbst angeguckt hat blickt man"}, {"id": 128, "start": 1089.28, "end": 1091.72, "text": "dann auch durch was man besser sein lassen sollte"}, {"id": 129, "start": 1100.84, "end": 1107.36, "text": "ja deswegen gibt es mehrere streams wir machen wir wir gucken uns jetzt erst mal wir gucken uns jetzt"}, {"id": 130, "start": 1107.36, "end": 1118.2, "text": "erst mal das ganze lokal an da machen wir mal so ein setup aus irgendwie so eine 5 5 pots als"}, {"id": 131, "start": 1118.2, "end": 1125.84, "text": "application server mit einem load balancer und einem ssl zertifikat oder so mit ingress controller"}, {"id": 132, "start": 1125.84, "end": 1136.88, "text": "davor wir holen uns auch echte let's encrypt zertifikate \u00fcber let's encrypt also echte"}, {"id": 133, "start": 1136.88, "end": 1155.52, "text": "echte ssl zertifikate \u00fcber let's encrypt mit zert manager und was machen wir sonst noch wir"}, {"id": 134, "start": 1155.52, "end": 1162.92, "text": "machen den lokalen registry f\u00fcr container images und konfigurieren halt das deployment und den"}, {"id": 135, "start": 1162.92, "end": 1167.96, "text": "service gucken uns das alles mal ein bisschen an ist der moogame dankesch\u00f6n f\u00fcr den snap"}, {"id": 136, "start": 1167.96, "end": 1178.2, "text": "was h\u00e4ltst du von IT zertifikat kommt ein bisschen auf die zertifikate an aber allgemein gesagt nicht"}, {"id": 137, "start": 1178.2, "end": 1187.52, "text": "ganz so viel ja ich w\u00fcrde sagen wir fangen mal relativ easy an ah ja und \u00fcbrigens ich habe"}, {"id": 138, "start": 1187.52, "end": 1194.16, "text": "schon ein bisschen was vorbereitet wo ich abgucken kann also das ist jetzt nicht fix und fertig aber"}, {"id": 139, "start": 1194.16, "end": 1197.56, "text": "ich habe mir so ein paar kleine snippets raus kopiert falls ich jetzt im stream nicht direkt"}, {"id": 140, "start": 1197.56, "end": 1202.28, "text": "drauf komme weil alles auf den kopf wei\u00df ich da auch nicht was h\u00e4ltst du von lpix also das"}, {"id": 141, "start": 1202.28, "end": 1208.4, "text": "was ich bisher gesehen habe sah sehr nach man pages aus wenn ich lerne aus aber kann auch kann"}, {"id": 142, "start": 1208.4, "end": 1219.52, "text": "mich auch t\u00e4uschen wir verwenden kein windows server doch klar ich mache immer windows server"}, {"id": 143, "start": 1219.52, "end": 1225.88, "text": "windows server ist eindeutig die \u00fcberlegene technologie deswegen l\u00e4uft ja auch 90 prozent"}, {"id": 144, "start": 1225.88, "end": 1233.64, "text": "des internets auf linux guck hier ja was sonst verstehe ich auch die frage gar nicht was denn"}, {"id": 145, "start": 1233.64, "end": 1253.68, "text": "sonst gibt es noch was anderes windows server und powershell und es auf final auf was aber"}, {"id": 146, "start": 1253.68, "end": 1260.44, "text": "finals ist gerade irgendwas ich habe \u00fcberhaupt nichts mitgekriegt cs go ist gerade glaube ich"}, {"id": 147, "start": 1260.44, "end": 1274.48, "text": "also wenn die windows server skalieren bist du nicht arm wenn die oracle datenbacken skalieren"}, {"id": 148, "start": 1274.48, "end": 1281.36, "text": "dann bist du arm wobei ne ne das stimmt nicht wir k\u00f6nnen es anders machen wenn die windows"}, {"id": 149, "start": 1281.36, "end": 1285.96, "text": "server skalieren bist du arm wenn die oracle datenbanken skalieren da kannst du gleich insolvenz"}, {"id": 150, "start": 1311.36, "end": 1324.56, "text": "weil ich wei\u00df was ich machen muss um das problem zu fixen jetzt ist nur die frage m\u00f6chtest du auch"}, {"id": 151, "start": 1324.56, "end": 1334.36, "text": "wissen wie man das problem fixt was muss man denn machen also die grundvoraussetzung ist schon mal"}, {"id": 152, "start": 1335.36, "end": 1343.8, "text": "das ist schon mal essentiell und dann zeige ich dir auch dass du gtkmm oder wie das hei\u00dft installieren"}, {"id": 153, "start": 1343.8, "end": 1358.6, "text": "musst das da musst du installieren und dann funktioniert probier mal aus das musst du"}, {"id": 154, "start": 1358.6, "end": 1367.4, "text": "installieren und dann startet die kiste am besten mal neu oder oder den system d service neu starten"}, {"id": 155, "start": 1367.4, "end": 1371.88, "text": "den hast ja bestimmt gestartet und so was aber das ding musste installieren diese library ist"}, {"id": 156, "start": 1371.88, "end": 1376.0, "text": "pflicht sonst funktioniert copy und paste nicht warum das keine dependency ist und warum das"}, {"id": 157, "start": 1376.0, "end": 1379.88, "text": "nirgendwo steht ich habe keine ahnung aber du brauchst diese library sonst geht copy und paste nicht"}, {"id": 158, "start": 1380.88, "end": 1394.28, "text": "gtk mm3 warum wei\u00df nur vmw so also wir uns jetzt mal hier das kubernetes kubernetes vielleicht"}, {"id": 159, "start": 1394.28, "end": 1400.6, "text": "was sagen die denn was sagen die denn selbst ja das das trifft es eigentlich ganz gut ich"}, {"id": 160, "start": 1400.6, "end": 1406.32, "text": "wollte sagen kubernetes ist ein tool zur container orchestrierung aber das sagen sie ja selbst also"}, {"id": 161, "start": 1406.32, "end": 1414.0, "text": "f\u00fcr alle was genau wird heute mit kubernetes gemacht also erst mal erkl\u00e4re ich kurz was"}, {"id": 162, "start": 1414.0, "end": 1425.68, "text": "kubernetes ist kubernetes ist quasi so was wie docker nur geclustert und mit wie wir kl\u00e4ren"}, {"id": 163, "start": 1425.68, "end": 1435.32, "text": "das am besten es ist es ist so was wie docker nur verteilt \u00fcber mehrere server ich glaube"}, {"id": 164, "start": 1435.4, "end": 1442.04, "text": "das kann man eigentlich am besten glaube das umschreibt eigentlich am ehesten ne du kannst"}, {"id": 165, "start": 1442.04, "end": 1446.12, "text": "auch einen server nehmen du kannst auch einen server nehmen aber f\u00fcr einen server macht es"}, {"id": 166, "start": 1446.12, "end": 1453.32, "text": "halt wenig sinn hochverf\u00fcgbares docker ja ich glaube ich glaube dass das trifft eigentlich"}, {"id": 167, "start": 1453.32, "end": 1459.64, "text": "ganz gut sie sagen ja selbst sie sind ein ein orchestrierungstool f\u00fcr container infrastruktur"}, {"id": 168, "start": 1459.64, "end": 1465.96, "text": "also du kannst damit deine container in deine container verwalten du kannst damit zum beispiel"}, {"id": 169, "start": 1465.96, "end": 1472.36, "text": "einstellen dass du f\u00fcr einen ich nenne es mal service auch wenn die kubernetes service was"}, {"id": 170, "start": 1472.36, "end": 1479.28, "text": "anderes ist das wird eine pains champ reihe nur so so schlimm so schlimm ist es auch das einzige"}, {"id": 171, "start": 1479.28, "end": 1485.76, "text": "problem an kubernetes ist so zumindest wie ich das sehe also was vom lernen her dass es ein fass"}, {"id": 172, "start": 1485.76, "end": 1490.8, "text": "ohne boden ist es gibt 30 milliarden tools f\u00fcnf tools f\u00fcr die gleichen sachen jede woche spawn"}, {"id": 173, "start": 1490.8, "end": 1496.4, "text": "irgendein neues tool die einfachsten sachen werden teilweise super krass verkompliziert"}, {"id": 174, "start": 1496.4, "end": 1506.28, "text": "aber insgesamt ist es an sich an sich ist das doch eine coole sache ja also was du das meinst ja"}, {"id": 175, "start": 1506.28, "end": 1513.04, "text": "also was du damit zum beispiel machen kannst mal ganz simpler use case ja du hast drei server"}, {"id": 176, "start": 1513.04, "end": 1522.32, "text": "und m\u00f6chtest also du willst ein webservice anbieten willst webservice anbieten und du willst dass das"}, {"id": 177, "start": 1522.32, "end": 1529.72, "text": "ganze m\u00f6glichst verf\u00fcgbar ist und du willst dass das ding im laufenden betrieb geupdatet werden"}, {"id": 178, "start": 1529.72, "end": 1537.12, "text": "kann und du willst dass das auch je nachdem wie viel user gerade drauf sind ordentlich performed"}, {"id": 179, "start": 1537.12, "end": 1542.12, "text": "oder eben auch skaliert wie man so sch\u00f6n nennt dann ist kubernetes genau das richtige weil du"}, {"id": 180, "start": 1542.16, "end": 1546.84, "text": "kannst kubernetes nehmen stellst dir verschiedene notes hin hei\u00dft das bei kubernetes also verschiedene"}, {"id": 181, "start": 1546.84, "end": 1553.88, "text": "server notes sind bei kubernetes in der regel die unterschiedlichen server in einem cluster also"}, {"id": 182, "start": 1553.88, "end": 1560.72, "text": "cluster member sind quasi notes bei kubernetes das muss ich zwangsl\u00e4ufig eine hardware kiste"}, {"id": 183, "start": 1560.72, "end": 1567.08, "text": "sein das k\u00f6nnen auch vor ems sein der neueste schrei ist sowas wie kubernetes auf firecracker"}, {"id": 184, "start": 1567.08, "end": 1571.4, "text": "vor ems wenn man das jetzt mal h\u00f6rt und jemand wei\u00df nicht was es ist er denkt jetzt erst mal was"}, {"id": 185, "start": 1571.4, "end": 1577.24, "text": "zum teufel will er von uns das machen wir heute nicht ich wei\u00df nicht ob wir es \u00fcberhaupt im"}, {"id": 186, "start": 1577.24, "end": 1582.92, "text": "stream machen ist auch was ganz ganz ab abgedrehtes das sind minimal vms firecracker das ist glaube"}, {"id": 187, "start": 1582.92, "end": 1589.56, "text": "ich von glaube das ist von amazon ich bin mir nicht ganz sicher ich glaube von amazon so"}, {"id": 188, "start": 1589.56, "end": 1601.16, "text": "eine minimal vm kubernetes selbst ist f\u00fcr container workload und der kubernetes cluster besteht"}, {"id": 189, "start": 1601.16, "end": 1606.2, "text": "aus unterschiedlichen notes in der regel ist es halt ein server jeder note das kann nat\u00fcrlich"}, {"id": 190, "start": 1606.2, "end": 1611.24, "text": "auch ein server sein worauf dann mehrere vm sind kann es kann es quasi auch sein kubernetes cluster"}, {"id": 191, "start": 1611.24, "end": 1617.6, "text": "in mehreren vmst auf einem eskserver laufen betreiben was auch immer oder theoretisch kannst"}, {"id": 192, "start": 1617.6, "end": 1621.92, "text": "das ding auch \u00fcber mehrere cloud anbieter spannen wie dem auch sei da bist du sind ja"}, {"id": 193, "start": 1621.92, "end": 1628.04, "text": "deine kreativit\u00e4t keine grenzen gesetzt wie du das ganze aufbauen willst zumindest hast du dann"}, {"id": 194, "start": 1628.04, "end": 1634.2, "text": "den vorteil dass du so sch\u00f6ne sachen machen kannst wie es gibt ein das machen wir \u00fcbrigens heute wenn"}, {"id": 195, "start": 1634.2, "end": 1640.68, "text": "wir so weit kommen du hast eine neue anwendungsversion und m\u00f6chtest jetzt was deine anwendung l\u00e4uft zur"}, {"id": 196, "start": 1640.68, "end": 1650.12, "text": "zeit auf version 1 und du willst jetzt auf version 2 und dann kannst du den config file editieren du"}, {"id": 197, "start": 1650.12, "end": 1654.52, "text": "kannst dar\u00fcber kommandos teile machen aber in der regel macht man \u00fcber config jammels dann"}, {"id": 198, "start": 1655.12, "end": 1659.48, "text": "ich will jetzt die version 2 von diesem moment ist an der t\u00fcr"}, {"id": 199, "start": 1671.48, "end": 1680.72, "text": "hat mich abgelenkt also wenn du keine ahnung davon hast und wenn euer online shop so funktioniert"}, {"id": 200, "start": 1680.72, "end": 1684.44, "text": "dass nur ein service ist da brauchst wahrscheinlich kein kubernetes cluster wie gesagt man muss es"}, {"id": 201, "start": 1684.76, "end": 1690.76, "text": "man muss es auch nicht \u00fcbertreiben manchmal tut es ohne probleme 123 vms und reverse proxy"}, {"id": 202, "start": 1690.76, "end": 1695.48, "text": "f\u00fcr ssl davor also es muss nicht sein da ist schon da ist schon ihr werdet wahrscheinlich"}, {"id": 203, "start": 1695.48, "end": 1700.04, "text": "so im lauf des streams ein bisschen an an klingen sehen da ist schon auch wenn es erstmal easy"}, {"id": 204, "start": 1700.04, "end": 1706.6, "text": "aussieht eine geh\u00f6rige portion komplexit\u00e4t drinne gar nicht so sehr in den reinen basis funktionen"}, {"id": 205, "start": 1706.6, "end": 1712.76, "text": "von kubernetes sondern das gesamte \u00f6kosystem drum herum was es gibt ja wo bin ich jetzt stehengeblieben"}, {"id": 206, "start": 1712.76, "end": 1716.44, "text": "genau also du willst du hast ein service laufen du willst den service jetzt abqueren wessen von"}, {"id": 207, "start": 1716.44, "end": 1723.56, "text": "version 1 auf version 2 normalerweise wird es dann auf dich und vm einloggen neu pullen vm den service"}, {"id": 208, "start": 1723.56, "end": 1727.24, "text": "wieder starten was auch immer mit komponiert das geht es relativ einfach du gehst in den jammel"}, {"id": 209, "start": 1727.24, "end": 1732.56, "text": "feil machen wir heute auch alles jammel feil tr\u00e4gt die neue container image version ein das image"}, {"id": 210, "start": 1732.56, "end": 1738.28, "text": "bauste vorher irgendwo entweder lokal mit docker oder \u00fcber jenkins oder github action so wie"}, {"id": 211, "start": 1738.28, "end": 1745.12, "text": "irgendwelche continues pipeline geschiss bauste dann image und pusht das in registry rein und"}, {"id": 212, "start": 1745.12, "end": 1749.92, "text": "dann tr\u00e4gst du dann im jammel feil ein ich will jetzt auf version 2 dann applies du diese konfig"}, {"id": 213, "start": 1749.92, "end": 1756.6, "text": "machen wir jetzt gleich alles dann applies diese konfig auf deinen kubernetes cluster und es"}, {"id": 214, "start": 1756.6, "end": 1767.44, "text": "skaliert dann automatisch also der reihe nach deine einzelnen container hoch also nicht skaliert"}, {"id": 215, "start": 1767.64, "end": 1774.04, "text": "der upgrade an der reihe nach deine einzelnen container ja richtig stripes genau so dass der"}, {"id": 216, "start": 1774.04, "end": 1787.04, "text": "service nicht ausf\u00e4llt was ist der unterschied zwischen k8s und k3s ich glaube das eine ist"}, {"id": 217, "start": 1787.04, "end": 1792.32, "text": "irgendwie super leicht gewichtiges komponiert gibt es ein paar sachen es gibt ein paar sachen wenn"}, {"id": 218, "start": 1792.32, "end": 1797.56, "text": "man das ist zum beispiel was was du eher auf dem respiratory pie laufen lassen kann kannst"}, {"id": 219, "start": 1797.56, "end": 1801.92, "text": "das ist aber glaube ich soweit ich wei\u00df 100 prozent api kompatibel zum gro\u00dfen kubernetes"}, {"id": 220, "start": 1801.92, "end": 1810.08, "text": "es gibt verschiedenste sachen f\u00fcr minimal kubernetes es gibt es gibt das originale kubernetes"}, {"id": 221, "start": 1810.08, "end": 1819.04, "text": "oftmals mit k8s abgek\u00fcrzt gibt es auch null ks es gibt kind es gibt mini cube es gibt alle"}, {"id": 222, "start": 1819.04, "end": 1825.28, "text": "m\u00f6glichen dinger total total viel sachen mini schiff feier cue war es auch immer alles wir"}, {"id": 223, "start": 1825.28, "end": 1833.44, "text": "nehmen heute mini cube wir nehmen heute mini cube weil es am einfachsten ist f\u00fcr mich mit"}, {"id": 224, "start": 1833.44, "end": 1844.12, "text": "mini cube lokal ein bisschen rum zu basteln so mini cube ist nahezu 100 prozent api kompatibel mit"}, {"id": 225, "start": 1845.12, "end": 1851.24, "text": "es kann halt einige sachen nicht ich kann euch aber nicht aus dem kopf sagen was es aber zum"}, {"id": 226, "start": 1851.24, "end": 1855.24, "text": "gr\u00f6\u00dften teil api kompatibel mit kubernetes und du kannst auch die ganz normalen kubernetes tools"}, {"id": 227, "start": 1855.24, "end": 1862.44, "text": "nehmen f\u00fcr f\u00fcr mini cube um damit lokal was auszuprobieren ist halt haupts\u00e4chlich gedacht"}, {"id": 228, "start": 1862.44, "end": 1874.32, "text": "f\u00fcr auf einer kiste lernen und \u00fcben mini cube wir machen jetzt mal ein eigenes wir machen jetzt"}, {"id": 229, "start": 1874.32, "end": 1883.6, "text": "mal ein eigenes verzeichnis daf\u00fcr repose mk dir nennen wir es einfach mal sogar du brauchst"}, {"id": 230, "start": 1883.6, "end": 1898.64, "text": "noch cube ctl ne brauche ich nicht weil guck mal da ist es das ist mittlerweile bei mini cube"}, {"id": 231, "start": 1898.64, "end": 1905.32, "text": "dabei also was hei\u00dft dabei der l\u00e4dt es im hintergrund runter aber trotzdem ich installier"}, {"id": 232, "start": 1905.32, "end": 1908.84, "text": "es trotzdem weil das braucht man normalerweise hast du schon recht"}, {"id": 233, "start": 1914.08, "end": 1921.12, "text": "plan der mission lernen ja also wie gesagt meine streams sind ja immer so so ein ding also ich"}, {"id": 234, "start": 1921.12, "end": 1925.32, "text": "mache meistens sachen wo ich selbst ein bisschen schon von ahnung habe aber nicht so krass den"}, {"id": 235, "start": 1925.32, "end": 1930.24, "text": "durchblick und dadurch dass ich das euch zeige und ein bisschen was erkl\u00e4ren muss lernt man selbst"}, {"id": 236, "start": 1930.24, "end": 1943.08, "text": "besser das ist immer ganz gute kombination so sieht es aus jetzt k\u00f6nnen wir anfangen es ist"}, {"id": 237, "start": 1943.08, "end": 1949.2, "text": "relativ easy zu starten mit mini cube und zwar ihr braucht keine vm ihr braucht kein kubernetes"}, {"id": 238, "start": 1949.2, "end": 1955.16, "text": "installation und nix ihr sagt einfach mini cube start und jetzt macht ihr eine sache falls ihr"}, {"id": 239, "start": 1955.16, "end": 1965.24, "text": "das auch ausprobieren wollt macht ihr eine sache man muss mal kurz nachgucken insecure oder mir ist"}, {"id": 240, "start": 1965.24, "end": 1979.2, "text": "das insecure registry machte jetzt mini cube start ins insecure registry werdet gleich sehen warum"}, {"id": 241, "start": 1985.2, "end": 1990.6, "text": "jetzt leider kobe ne das geht uns runter und direkt best practice ist lokal es ist lokal"}, {"id": 242, "start": 1990.6, "end": 2008.28, "text": "\u00fcbungs practice chat ihr werdet gedisst gerade im chat also quasi aber du bist auch teil des"}, {"id": 243, "start": 2008.28, "end": 2014.08, "text": "chats kommt eigentlich ein video zu dem mobil funk anbieter suchen was ich euch da erz\u00e4hlen soll"}, {"id": 244, "start": 2021.16, "end": 2030.52, "text": "so ach ja es gibt verschiedene nummer so der vollst\u00e4ndigkeit vollst\u00e4ndigkeit halber erw\u00e4hnt"}, {"id": 245, "start": 2030.52, "end": 2039.24, "text": "es gibt verschiedene m\u00f6glichkeiten mini cube zu betreiben die standard variante ist \u00fcber"}, {"id": 246, "start": 2039.24, "end": 2046.4, "text": "irgendeinen virtualisierungs provider also meistens kvm also wenn ihr mini cube startet dann legt es"}, {"id": 247, "start": 2046.4, "end": 2054.36, "text": "normalerweise unter der haupe eine neue vm an wenn man keine vm anlegen m\u00f6chte kann man das ganze"}, {"id": 248, "start": 2054.36, "end": 2060.04, "text": "auch selbst als container in docker laufen lassen ist ja kein ding ich mein kubernetes ist ja"}, {"id": 249, "start": 2060.04, "end": 2065.52, "text": "letztendlich tool zur container verwaltung warum sollte man das nicht auch selbst in"}, {"id": 250, "start": 2065.52, "end": 2073.16, "text": "dem container laufen lassen k\u00f6nnen also ihr seht ja hier ich habe keine vm installiert also ich"}, {"id": 251, "start": 2073.16, "end": 2077.92, "text": "habe hier keine virtualisierungsm\u00f6glichkeiten in der vm drin das hei\u00dft er l\u00e4sst jetzt das back"}, {"id": 252, "start": 2077.92, "end": 2083.72, "text": "end einfach in einem eigenen docker container laufen also es ist jetzt quasi kubernetes in"}, {"id": 253, "start": 2083.72, "end": 2088.84, "text": "docker womit man eine weitere container verwalten gab es ist egal letztendlich muss man sich damit"}, {"id": 254, "start": 2088.84, "end": 2093.6, "text": "nicht besch\u00e4ftigen das ist ja zum \u00fcben und die api und wie man das anspricht und so ist \u00fcberhaupt"}, {"id": 255, "start": 2093.6, "end": 2100.2, "text": "kein unterschied also diese lokale lokale so entwicklungsumgebung mit mini cube k\u00f6nnen"}, {"id": 256, "start": 2100.2, "end": 2104.96, "text": "die genauso benutzen wie also die du brauchst keine extra s\u00fcndung so ist quasi genau das"}, {"id": 257, "start": 2104.96, "end": 2112.48, "text": "gleiche wie f\u00fcrs gro\u00dfe kuppernet ist nur dass es lokal bei einem auf dem rechner l\u00e4uft sagen"}, {"id": 258, "start": 2112.48, "end": 2118.36, "text": "mal logien halbist du irgendwie besoffen oder so ich kann dir nicht folgen was du f\u00fcr komisches"}, {"id": 259, "start": 2118.36, "end": 2128.0, "text": "teuch im chat schreibst die ganze zeit was was soll was soll uns das sagen lokal gesch\u00fctzt"}, {"id": 260, "start": 2128.0, "end": 2135.88, "text": "von meter alles klar written in rust container runtime nie kenne ich nicht ich kann nicht jede"}, {"id": 261, "start": 2135.88, "end": 2146.08, "text": "rust software kennen ich schreibe \u00fcber mastadon schreibst du das einfach nur so oder schreibst du"}, {"id": 262, "start": 2146.08, "end": 2153.0, "text": "mit jemand ich glaube ich glaube du machst aktuell die taktik was ich auch manchmal mache alles was"}, {"id": 263, "start": 2153.0, "end": 2158.28, "text": "einem in den kopf f\u00e4llt in irgendein text einkommen in irgendein text eingabe f\u00e4llt tippen und enter"}, {"id": 264, "start": 2158.28, "end": 2167.04, "text": "dr\u00fccken aber mach nur ich beschwer mich nicht alles interactions auf twitch wobei das war youtube"}, {"id": 265, "start": 2167.04, "end": 2173.96, "text": "wo das so wichtig ist ach ja und wir brauchen tats\u00e4chlich anscheinend doch nachher noch potman"}, {"id": 266, "start": 2173.96, "end": 2178.6, "text": "und so gucken wir uns an also wir haben mini cube gestartet wir k\u00f6nnen jetzt mal sowas machen wie"}, {"id": 267, "start": 2178.6, "end": 2186.4, "text": "cube ctl get ach nee ha sekunde leute lasst lasst euch nicht jebaden von irgendwelchen youtube"}, {"id": 268, "start": 2186.4, "end": 2193.0, "text": "tutorials oder sowas ich zeige euch jetzt mal wie das die ganzen youtube tutorial dudes machen und"}, {"id": 269, "start": 2193.0, "end": 2198.48, "text": "dann zeige ich euch wie man es richtig macht ok also die ganzen youtube nicht alle es gibt"}, {"id": 270, "start": 2198.48, "end": 2203.16, "text": "ein gut es gibt gute tutorials das beste deutschsprachige tutorial auf youtube was ich"}, {"id": 271, "start": 2203.16, "end": 2210.44, "text": "kenne dazu kubernetes ist also f\u00fcr leute die ein bisschen ahnung von linux und sich nicht"}, {"id": 272, "start": 2210.44, "end": 2216.84, "text": "vor kommando zeilen scheuen und relativ relativ ja schon ein bisschen bisschen background was"}, {"id": 273, "start": 2216.84, "end": 2224.4, "text": "container angeht haben das ist das beste also wenn wenn sich einmal wirklich in zwei stunden"}, {"id": 274, "start": 2224.4, "end": 2229.56, "text": "in zwei stunden gut strukturiert das kubernetes tutorial angucken will finde ich das pers\u00f6nlich"}, {"id": 275, "start": 2229.72, "end": 2235.72, "text": "auf deutsch finde ich das pers\u00f6nlich am besten ich k\u00f6nnte nat\u00fcrlich auch einfach dabei bleiben"}, {"id": 276, "start": 2235.72, "end": 2241.68, "text": "weil ich wir werden ja wahrscheinlich nicht alles machen was er im video macht aber in gro\u00dfteil und"}, {"id": 277, "start": 2241.68, "end": 2246.36, "text": "ihr k\u00f6nnt ja live auch noch was dazu beitragen und wir machen auch ein paar sachen die er so"}, {"id": 278, "start": 2246.36, "end": 2250.4, "text": "glaube ich nicht im video zeigt ich glaube es ist selbstwertig f\u00fcr karte \u00fcber letzten"}, {"id": 279, "start": 2250.4, "end": 2253.88, "text": "krippen macht er im video gar nicht aber wenn mal einer sich in ruhe und tutorial angucken will"}, {"id": 280, "start": 2253.88, "end": 2267.64, "text": "ich finde das ist das beste so ja genau also ich zeige euch jetzt mal wie das die ganzen"}, {"id": 281, "start": 2267.64, "end": 2274.28, "text": "tutorial duz auf youtube und auch teilweise auf jude me und so machen wenn die kubanets"}, {"id": 282, "start": 2274.28, "end": 2278.44, "text": "kommandos oder generell wenn die kommando zeilen tools verwenden dann machen die so was hier guck"}, {"id": 283, "start": 2278.44, "end": 2291.6, "text": "jupe ctl get pots oder da machen sie so get events und solche dinger du siehst die typen"}, {"id": 284, "start": 2291.6, "end": 2297.64, "text": "in tutorial videos zum kurz ich dr\u00fccke schon ins den ganzen krempel immer f\u00fcnf mal tippen so"}, {"id": 285, "start": 2297.64, "end": 2303.6, "text": "nicht so nicht leute das ist das ist mist auch wenn man das in allen m\u00f6glichen tutorials sieht das"}, {"id": 286, "start": 2303.6, "end": 2308.16, "text": "erste was man braucht ist und jeder linux admin wird wissen das erste was man braucht ist"}, {"id": 287, "start": 2308.16, "end": 2314.16, "text": "shell completion leute jetzt nicht unbedingt f\u00fcr minicube f\u00fcr minicube ist das relativ egal dann"}, {"id": 288, "start": 2314.16, "end": 2321.84, "text": "m\u00fcssen wir nicht so viel machen aber f\u00fcr kubectl brauchen wir best completion mir h\u00f6rt das ist"}, {"id": 289, "start": 2321.84, "end": 2328.12, "text": "aktuell noch nicht der fall das geht noch nicht und wir wollen nicht nur einfach tab completion"}, {"id": 290, "start": 2328.12, "end": 2334.88, "text": "haben sondern wir wollen auch quasi ja kontext sensitive tab completion haben also wenn ich zum"}, {"id": 291, "start": 2334.88, "end": 2342.92, "text": "beispiel sage hier irgendwie describe potts oder ne pot genau describe nee nee quatsch nee das"}, {"id": 292, "start": 2342.92, "end": 2348.92, "text": "klappe kann ich so die ressource hinten dran schreiben blabla blub so und dann will ich dass"}, {"id": 293, "start": 2348.92, "end": 2354.84, "text": "das auto completed so und dazu gibt es folgendes kubectl jetzt muss ich mal \u00fcberlegen wie das war"}, {"id": 294, "start": 2354.84, "end": 2367.76, "text": "completion zsh genau so kubectl bringt n\u00e4mlich eine eigene completion definition f\u00fcr bash zsh"}, {"id": 295, "start": 2367.76, "end": 2371.28, "text": "und was noch"}, {"id": 296, "start": 2371.28, "end": 2382.28, "text": "best und zsh wahrscheinlich mit richtig nice so und das kann man dann in seine"}, {"id": 297, "start": 2382.28, "end": 2387.92, "text": "basher z eintragen die dann folgenderma\u00dfen aus muss blo\u00df mal kurz seine zhsh wie auch immer"}, {"id": 298, "start": 2387.92, "end": 2399.44, "text": "shell konfig ich muss mal kurz gucken wo habe ich denn meine ach da da ist es doch muss doch"}, {"id": 299, "start": 2399.44, "end": 2417.8, "text": "unten irgendwie sein source und wie machen das jetzt so geht das einfach so bin ich gerade"}, {"id": 300, "start": 2417.8, "end": 2426.8, "text": "ein bisschen ich glaube ich glaube wir m\u00fcssen es redirecten ich bin mir nicht ganz sicher ob"}, {"id": 301, "start": 2426.8, "end": 2436.96, "text": "das so funktioniert nein aber ich habe ich habe dings brauche ich nicht genau so ist gut"}, {"id": 302, "start": 2436.96, "end": 2447.32, "text": "ja genau und jetzt habe ich kubectl und guck mal tab tab jetzt habe ich autocomplete ja und"}, {"id": 303, "start": 2447.32, "end": 2456.48, "text": "ihr seht autocomplete hier endpoints autocomplete und der autocomplete jetzt quasi auch kontext"}, {"id": 304, "start": 2456.48, "end": 2461.6, "text": "sensitive infos so an der stelle gibt es halt einen endpoint der kubernetes hei\u00dft und den kann"}, {"id": 305, "start": 2461.6, "end": 2470.68, "text": "ich da eintragen jetzt kann ich zum beispiel describe irgendwie sowas wie hier da gibt es"}, {"id": 306, "start": 2470.68, "end": 2476.48, "text": "halt pots gibt es halt noch kein man kann auch sagen all namespaces dann zeigt er dann alle pots"}, {"id": 307, "start": 2476.48, "end": 2483.84, "text": "an die es gibt mal mal get pots dann erz\u00e4hle ich mal ein bisschen so was zur zur benahmung wie"}, {"id": 308, "start": 2483.84, "end": 2489.64, "text": "kubernetes denkt dass man dinger nennen soll also das hier sind jetzt die laufenden container die"}, {"id": 309, "start": 2489.64, "end": 2495.12, "text": "wir zur zeit haben in unserem kubernetes cluster wobei das nicht stimmt wobei das nicht ganz"}, {"id": 310, "start": 2495.12, "end": 2504.2, "text": "richtig ist ich muss das ich muss das anders erkl\u00e4ren also kubernetes ist zwar ein orchestrierung"}, {"id": 311, "start": 2504.2, "end": 2511.92, "text": "stuhl f\u00fcr container aber kubernetes arbeitet gar nicht auf container basis das klingt jetzt"}, {"id": 312, "start": 2511.92, "end": 2515.88, "text": "erst mal verwirrend aber wenn ich euch das ein bisschen genauer erkl\u00e4ren wird es hoffentlich"}, {"id": 313, "start": 2515.88, "end": 2522.8, "text": "klarer und zwar das was somit kubernetes verwaltet sind zwar unter der haube container aber container"}, {"id": 314, "start": 2522.8, "end": 2530.0, "text": "werden in kubernetes zusammengefasst zu pots ein pot ist quasi so eine logische einheit wieviel"}, {"id": 315, "start": 2531.0, "end": 2544.6, "text": "du meinst wie viele notes mein cluster hat einen lokale vm gerade so also was wollte ich jetzt sagen"}, {"id": 316, "start": 2544.6, "end": 2556.2, "text": "genau kubernetes das kleinste mit dem kubernetes hantiert sind pots pots ist so die kleinste logische"}, {"id": 317, "start": 2556.2, "end": 2563.8, "text": "einheit die man die man mit kubernetes verwalten kann in der regel k\u00f6nnte man sagen dass ein pot"}, {"id": 318, "start": 2563.8, "end": 2569.52, "text": "so was wie ein container ist wobei das nicht stimmt ein pot ist quasi also ist quasi so der"}, {"id": 319, "start": 2569.52, "end": 2577.08, "text": "container f\u00fcr die container oder so dass so ist so die so die der rahmen oder so die kiste um die"}, {"id": 320, "start": 2577.08, "end": 2581.16, "text": "container klingt jetzt bescheuert kann man kann man echt schlecht beschreiben also ich erkl\u00e4re"}, {"id": 321, "start": 2581.16, "end": 2588.44, "text": "ich mal zum beispiel warum man das braucht ich erkl\u00e4re ich mal warum man das braucht und zwar"}, {"id": 322, "start": 2588.44, "end": 2594.56, "text": "mal angenommen du hast jetzt eine anwendung in die net anwendung die verpackst du in container"}, {"id": 323, "start": 2594.56, "end": 2602.08, "text": "image und die l\u00e4sst du dann als pot laufen dann ist der pot genau ein container drin aber du"}, {"id": 324, "start": 2602.08, "end": 2607.52, "text": "k\u00f6nntest jetzt ja in diesem pot zum beispiel noch einen container f\u00fcr metriken haben oder noch"}, {"id": 325, "start": 2607.52, "end": 2617.36, "text": "ein container f\u00fcr secrets oder noch ein container f\u00fcr reverse proxy oder so was also k\u00f6nntest"}, {"id": 326, "start": 2617.36, "end": 2623.36, "text": "ja du k\u00f6nntest ja quasi damit die anwendung von als einheit funktioniert also ein docker"}, {"id": 327, "start": 2623.36, "end": 2631.92, "text": "compose file ist ein pot ja das kommt ungef\u00e4hr hin wobei man sagen muss in so einem docker"}, {"id": 328, "start": 2631.92, "end": 2640.68, "text": "compose file definierst du ja auch mehrere logische services die du miteinander verbindest"}, {"id": 329, "start": 2640.68, "end": 2647.04, "text": "du kannst jetzt mit docker compose file auch eine datenbank reinschreiben ein application server"}, {"id": 330, "start": 2647.04, "end": 2651.12, "text": "und die dann miteinander verbinden das wird schon in einzelne pots machen das du w\u00fcrdest"}, {"id": 331, "start": 2651.12, "end": 2659.68, "text": "quasi nur sachen die logisch zusammengeh\u00f6ren in ein pot packen ja also zum beispiel ein container"}, {"id": 332, "start": 2659.68, "end": 2672.16, "text": "f\u00fcr connection zu deinem secret store oder so was in der richtung also werd mal angenommen"}, {"id": 333, "start": 2672.16, "end": 2677.56, "text": "deine deine anwendung braucht api keys und sonstige geschichten die hast du nicht in"}, {"id": 334, "start": 2677.56, "end": 2682.68, "text": "einem kubernetes cluster sondern irgendwo extern liegen und du willst es auch nicht in kubernetes"}, {"id": 335, "start": 2682.68, "end": 2686.36, "text": "cluster integrieren dann kannst du dir noch einen zweiten container rein passen rein packen der"}, {"id": 336, "start": 2686.36, "end": 2689.6, "text": "nur daf\u00fcr da ist diese secrets zeig mal wie ist denn dieses bild"}, {"id": 337, "start": 2697.12, "end": 2697.62, "text": "ja"}, {"id": 338, "start": 2703.76, "end": 2704.26, "text": "ja"}, {"id": 339, "start": 2707.84, "end": 2713.12, "text": "genau also das ganze nennt sich \u00fcbrigens seit k wenn man noch mehrere container rein"}, {"id": 340, "start": 2713.12, "end": 2720.48, "text": "packt in so einen pot genau so was hier das ist das ist ein sch\u00f6nes beispiel das ist das"}, {"id": 341, "start": 2720.48, "end": 2727.76, "text": "ist ein sch\u00f6nes sch\u00f6nes beispiel ja aber aber chat gpt erkl\u00e4rt er nicht warum das ist ein"}, {"id": 342, "start": 2727.76, "end": 2733.16, "text": "sch\u00f6nes beispiel ja du hast zum beispiel deine anwendung die schreibt logs und jetzt willst"}, {"id": 343, "start": 2733.16, "end": 2740.16, "text": "du diese log files allerdings zu einem zentralen logserver schicken und die anwendung selbst kann"}, {"id": 344, "start": 2740.16, "end": 2744.16, "text": "kein syslog und hat auch keinen bock sich irgendwie mit logs am einsammeln dingern zu"}, {"id": 345, "start": 2744.16, "end": 2749.12, "text": "besch\u00e4ftigen dann w\u00fcrdest du einen pot erstellen der enth\u00e4lt die anwendung und noch einen zweiten"}, {"id": 346, "start": 2749.12, "end": 2754.12, "text": "container der daf\u00fcr da ist die logs einzusammeln und an irgendein zentrales log management system"}, {"id": 347, "start": 2754.12, "end": 2760.04, "text": "zu schicken das ist ein gutes beispiel ja du k\u00f6nntest zum beispiel noch einen dritten container"}, {"id": 348, "start": 2760.04, "end": 2767.32, "text": "dabei machen der metriken bereitstellt f\u00fcr prometheus monitoring oder so was hast du quasi eine logische"}, {"id": 349, "start": 2767.32, "end": 2773.52, "text": "einheit also das hier w\u00e4re dann quasi dein application server pot allerdings enth\u00e4lt er"}, {"id": 350, "start": 2773.52, "end": 2778.44, "text": "eben alle container in diesem pot die die anwendung braucht um ebenso als logische einheit"}, {"id": 351, "start": 2778.44, "end": 2785.36, "text": "gestartet werden zu k\u00f6nnen und das zu machen was sie machen soll eine datenbank w\u00e4re wiederum ein"}, {"id": 352, "start": 2785.36, "end": 2795.56, "text": "zweiter pot das ist eigentlich gar nicht so verkehrt was was ein bisschen doof ist an kuba"}, {"id": 353, "start": 2795.68, "end": 2806.6, "text": "die haben halt f\u00fcr alles irgendwelche gew\u00f6hnungsbed\u00fcrftige bezeichnung ja container"}, {"id": 354, "start": 2806.6, "end": 2812.96, "text": "sind pots was schon irgendwie klar ist weil ein pot mehrere container enthalten kann du hast"}, {"id": 355, "start": 2812.96, "end": 2817.2, "text": "deployments da versteht jeder eigentlich was anderes darunter als irgendwelche komischen"}, {"id": 356, "start": 2817.2, "end": 2821.76, "text": "yaml files die pots definieren aber gut da muss man sich ein bisschen dran gew\u00f6hnen"}, {"id": 357, "start": 2822.52, "end": 2830.28, "text": "was phippe goes to the zoo cloud native computing foundation hat es kuba neta story moment"}, {"id": 358, "start": 2830.28, "end": 2839.52, "text": "jetzt lernen wir was ok pots hier hier werden pots erkl\u00e4rt"}, {"id": 359, "start": 2851.92, "end": 2854.04, "text": "ich verstehe den zusammenhang kann ich so genau"}, {"id": 360, "start": 2865.32, "end": 2869.16, "text": "ich verstehe den zusammenhang zwischen diesem bild und und potz nicht"}, {"id": 361, "start": 2869.16, "end": 2884.44, "text": "wie dem auch sei wir machen jetzt wir machen jetzt mal weiter man muss sich zumindest ein"}, {"id": 362, "start": 2884.44, "end": 2888.76, "text": "bisschen dran gew\u00f6hnen dass sie alles irgendwie irgendwie anders nennen"}, {"id": 363, "start": 2889.32, "end": 2901.68, "text": "und zwar ist es zum beispiel bei kuba neta ist auch so die nennen dort wo der traffic reingeht also"}, {"id": 364, "start": 2901.68, "end": 2907.0, "text": "quasi das was jeder normale mensch irgendwie reverse proxy nennt das nennen die ingress zum"}, {"id": 365, "start": 2907.0, "end": 2914.84, "text": "beispiel und das gegenteil von ingress also da wo der traffic quasi reingeht ist nicht etwa"}, {"id": 366, "start": 2914.84, "end": 2920.72, "text": "outgress sondern egress weil ist halt so"}, {"id": 367, "start": 2926.88, "end": 2932.2, "text": "und da gibt wie gesagt gibt gibt gibt noch mehr sachen wo ganz komisch benannt sind wo man sich"}, {"id": 368, "start": 2932.2, "end": 2937.0, "text": "einfach dran gew\u00f6hnen muss was genau ist minikub minikub ist ein tool wo du lokal"}, {"id": 369, "start": 2937.0, "end": 2944.6, "text": "bei dir kuba netas laufen lassen kannst zum \u00fcben also wir haben das ganze noch nicht gestartet doch"}, {"id": 370, "start": 2944.92, "end": 2949.64, "text": "also nachdem minikub installiert ist es ganz einfach \u00fcber package manager oder als single"}, {"id": 371, "start": 2949.64, "end": 2954.08, "text": "binary runterladen es ist glaube ich auch in go programmiert als single binary runterladen"}, {"id": 372, "start": 2954.08, "end": 2959.32, "text": "starten dann startet ihr minikub am besten dass ihr noch hinten dran wenn ihr lokale"}, {"id": 373, "start": 2959.32, "end": 2964.16, "text": "container registry haben wollt und gut ist dann habt ihr minikub gestartet euer kuba netas l\u00e4uft"}, {"id": 374, "start": 2964.16, "end": 2968.4, "text": "irgendwann mal l\u00e4uft es dann"}, {"id": 375, "start": 2968.4, "end": 2975.24, "text": "und dann k\u00f6nnt ihr auch schon kommandos an euren kuba netas schicken und das ganze machen mit"}, {"id": 376, "start": 2975.24, "end": 2984.84, "text": "kubectl kubectl kubectl get pods zum beispiel so und man sieht hier schon folgendes das ist das wo"}, {"id": 377, "start": 2984.84, "end": 2993.72, "text": "ich gerade am wo ich gerade am erz\u00e4hlen gewesen bin ein port ist die kleinste logische einheit die"}, {"id": 378, "start": 2993.72, "end": 3003.8, "text": "kuba netas verwaltet kuba netas selbst oder in dem fall minikub selbst wenn du startest kommt"}, {"id": 379, "start": 3003.8, "end": 3009.84, "text": "schon mit ein paar vorinstallierten pods um die musst du dich nicht k\u00fcmmern das ist es ja auch"}, {"id": 380, "start": 3009.84, "end": 3017.2, "text": "hier das ist im name space kube system und dort sind interne sind interne pods die man die das"}, {"id": 381, "start": 3017.2, "end": 3024.36, "text": "braucht dass es funktioniert was zum beispiel hier sowas wie core dns das ist das ist wichtig"}, {"id": 382, "start": 3024.36, "end": 3029.76, "text": "daf\u00fcr dass die pods sich untereinander beziehungsweise auch die services aufgel\u00f6st"}, {"id": 383, "start": 3029.76, "end": 3036.08, "text": "werden k\u00f6nnen und im cluster selbst die dienste per namen angesprochen werden k\u00f6nnen und so was"}, {"id": 384, "start": 3036.08, "end": 3039.08, "text": "da muss man sich jetzt nicht so viel gedanken dr\u00fcber machen aber damit kommt das halt also"}, {"id": 385, "start": 3039.08, "end": 3045.76, "text": "sprich man sieht hier laufen schon ein paar container was man hier auch schon sieht das"}, {"id": 386, "start": 3045.76, "end": 3052.44, "text": "brauchen wir sp\u00e4ter noch hier sieht man dass manche pods ready sind es ist so in kuba netas"}, {"id": 387, "start": 3052.44, "end": 3061.96, "text": "dass ein also in docker ist es so du startest das ding so und entweder es krascht oder es l\u00e4uft"}, {"id": 388, "start": 3061.96, "end": 3070.92, "text": "aber so wirklich gro\u00dfartig wissen wann das jetzt ready ist und wann nicht tust du nicht"}, {"id": 389, "start": 3070.92, "end": 3074.8, "text": "ja du kannst den docker zwar einstellen dass er restarten soll wenn es krascht aber"}, {"id": 390, "start": 3074.8, "end": 3080.64, "text": "wann die webanwendung da drin gestartet ist und ich wei\u00df nicht so und nachdem es in kuba netas"}, {"id": 391, "start": 3080.64, "end": 3088.2, "text": "ja unter anderem darum geht dass das ganze m\u00f6glichst ausfallfrei l\u00e4uft haben container"}, {"id": 392, "start": 3088.2, "end": 3090.84, "text": "oder pods muss man sagen sind da sind da keine container"}, {"id": 393, "start": 3090.84, "end": 3102.32, "text": "haben container verschiedene m\u00f6glichkeiten zu \u00fcberpr\u00fcfen ob sie bereit sind da gibt es"}, {"id": 394, "start": 3102.32, "end": 3108.0, "text": "n\u00e4mlich sowas was der andreas gerade schreibt es gibt so was wie readiness und leifnis da kann"}, {"id": 395, "start": 3108.0, "end": 3112.64, "text": "man dann wirklich health checks auf die container machen auf die pods machen und erst wenn die"}, {"id": 396, "start": 3112.64, "end": 3118.48, "text": "wirklich ready sind funktionieren also in dem fall dass ein web response web request durchgeht"}, {"id": 397, "start": 3118.48, "end": 3124.96, "text": "bei einer anwendung erst dann kommen die quasi in den ready state und der interne load balancer"}, {"id": 398, "start": 3124.96, "end": 3134.16, "text": "in kuba netas w\u00fcrde die dann ansteuern genau kompost kann auch so was in der richtung ja ich"}, {"id": 399, "start": 3134.16, "end": 3138.4, "text": "finde hier ist das jetzt das sch\u00f6ner gemacht so also mit was ich \u00fcberlege gerade mit was wir"}, {"id": 400, "start": 3138.4, "end": 3142.28, "text": "anfangen wir machen uns erst mal eine leere datei nennen wir das einfach mal hier touch"}, {"id": 401, "start": 3142.28, "end": 3151.96, "text": "keine ahnung cute punkt jammel und machen wir wishell studio code auf ja wir"}, {"id": 402, "start": 3151.96, "end": 3158.52, "text": "trasten cute wir trasten nicht cute punkt jammel nope so jetzt k\u00f6nnen wir als erstes"}, {"id": 403, "start": 3158.52, "end": 3165.04, "text": "mal die kuba netas extension installieren ja ja reload mal die kuba netas extension ist"}, {"id": 404, "start": 3165.04, "end": 3174.96, "text": "nicht so dolle also ist ehrlich gesagt das einzige was ich von der kuba netas extension"}, {"id": 405, "start": 3174.96, "end": 3185.56, "text": "benutze ist das hier die janta jammel das jammel synthax autocomplete f\u00fcr kuba netas files so"}, {"id": 406, "start": 3185.68, "end": 3196.44, "text": "das n\u00e4chste ist ein config file f\u00fcr kuba netas also so ein jammelfile hier das nennt sich wenn"}, {"id": 407, "start": 3196.44, "end": 3203.4, "text": "ich nicht ganz falsch liege manifest warum weil config datei zu dumm ist weil die das immer gerne"}, {"id": 408, "start": 3203.4, "end": 3209.88, "text": "ein bisschen fancy nennen was kuba netas angeht also ein jammel konfig datei f\u00fcr kuba netas"}, {"id": 409, "start": 3209.88, "end": 3216.56, "text": "nennt sich manifest und in so einem manifest kann man fast beliebig viele dinge reinschreiben die"}, {"id": 410, "start": 3216.56, "end": 3221.4, "text": "man konfigurieren will f\u00fcr seinen kuba netas cluster man kann wie gesagt man kann mehr als"}, {"id": 411, "start": 3221.4, "end": 3227.44, "text": "eins reinschreiben allerdings muss man sich immer \u00fcberlegen man kann zwar jetzt auch gleich man"}, {"id": 412, "start": 3227.44, "end": 3235.2, "text": "kann diese datei immer nur im gesamten einlesen lassen von kuba netas also wenn ich da jetzt 20"}, {"id": 413, "start": 3235.2, "end": 3243.08, "text": "sachen drinne definiere und ich will dass kuba netas den kram \u00fcbernimmt dann immer nur alles auf"}, {"id": 414, "start": 3243.08, "end": 3249.96, "text": "eimer wenn alles in einer datei drin steht also es macht durchaus sinn sachen die zusammengeh\u00f6ren"}, {"id": 415, "start": 3249.96, "end": 3254.92, "text": "zusammen zu definieren also man k\u00f6nnte jetzt zum beispiel wobei da schreiten sich ja die geister"}, {"id": 416, "start": 3254.92, "end": 3262.8, "text": "dar\u00fcber was man wie aufsplitten sollte in config files und ob man wie man es nennt und ob man f\u00fcr"}, {"id": 417, "start": 3262.8, "end": 3268.48, "text": "jeden dings in unterordner macht und dann ein eigenes jammelfile f\u00fcr service deployment was auch"}, {"id": 418, "start": 3268.48, "end": 3273.48, "text": "immer wir schreiben das jetzt erst einmal sinnvoll zum zum \u00fcben schreiben wir das jetzt erstmal"}, {"id": 419, "start": 3273.48, "end": 3281.76, "text": "alles in ein jammelfile also wir brauchen erstmal eine testanwendung w\u00fcrde ich sagen oder wir haben"}, {"id": 420, "start": 3281.76, "end": 3288.0, "text": "hier keine testanwendung wir machen man wir machen mal eine minimal dotnet testanwendung wobei da"}, {"id": 421, "start": 3288.0, "end": 3298.48, "text": "m\u00fcssen wir einen container bauen und sowas hello world wir nehmen den hello world container da"}, {"id": 422, "start": 3298.48, "end": 3305.0, "text": "seht ihr auch schon mal gleich dass wie das aussieht wenn ein container crashed wir machen"}, {"id": 423, "start": 3305.0, "end": 3310.88, "text": "hello world hello world container der beendet sich n\u00e4mlich da werdet ihr gleich sehen als kuba netas"}, {"id": 424, "start": 3310.88, "end": 3316.6, "text": "auch sagt das ding ist das ding l\u00e4uft nicht mehr also deployment so sieht ein kuba netas deployment"}, {"id": 425, "start": 3316.6, "end": 3329.48, "text": "aus ich mache mal kurz die schriftgr\u00f6\u00dfen ticken in ticken gr\u00f6\u00dfer 18 aber wir waren sogar 20"}, {"id": 426, "start": 3329.48, "end": 3335.04, "text": "kommen dass man gescheit was sieht wunderbar also so sieht in deployment in kuba netas aus das ist"}, {"id": 427, "start": 3335.04, "end": 3342.52, "text": "das das ist das template was hier die studio add-on dabei hat also in deployment ist in kuba"}, {"id": 428, "start": 3342.52, "end": 3349.08, "text": "netas eine config die kuba netas sagt welche container es starten soll ganz platt gesagt"}, {"id": 429, "start": 3349.08, "end": 3354.08, "text": "leute ich glaube euch im chat dass ihr die super krassen kuba netas checker seid wahrscheinlich"}, {"id": 430, "start": 3354.08, "end": 3359.04, "text": "mehr als ich aber es hat keinen sinn wenn wir jetzt alle 800.000 kuba netas tools die es gibt"}, {"id": 431, "start": 3359.04, "end": 3365.8, "text": "immer wild durcheinander im chat schmei\u00dfen wie gesagt das ist ein un\u00fcberschaubare landscape an"}, {"id": 432, "start": 3365.8, "end": 3370.52, "text": "tools und sachen man k\u00f6nnte es auch man k\u00f6nnte man k\u00f6nnte auch gar kein manifest schreiben und"}, {"id": 433, "start": 3370.52, "end": 3374.96, "text": "man kann das manifest in python generieren das gibt es n\u00e4mlich auch also wo wir gerade dabei"}, {"id": 434, "start": 3374.96, "end": 3378.92, "text": "sind hier man kann auch so was verwenden guck wenn man keinen wenn man keinen bock auf jammel hat"}, {"id": 435, "start": 3378.92, "end": 3386.08, "text": "kann man auch das komplett erstellen mit jeder x beliebigen programmiersprache was was haben"}, {"id": 436, "start": 3386.08, "end": 3390.68, "text": "die denn ich glaube python java und noch was aber das ist einfach zu viel auf einmal"}, {"id": 437, "start": 3401.52, "end": 3410.44, "text": "deswegen war jetzt erstmal jammel files und gucken uns das ganze an so also was gibt es"}, {"id": 438, "start": 3410.44, "end": 3418.28, "text": "denn so in einem deployment zu sehen erstmal in unseren namen ausdenken cute app super toll"}, {"id": 439, "start": 3418.28, "end": 3427.6, "text": "ich mache noch mal deployment cute app dann haben wir hier so sachen wie metadata das ist"}, {"id": 440, "start": 3427.6, "end": 3432.68, "text": "erstmal nicht so wild metadata kann man alles m\u00f6glich reinschreiben es gibt \u00fcbrigens auch"}, {"id": 441, "start": 3432.68, "end": 3443.88, "text": "noch annotations schwachsinn es gibt labels es gibt labels und es gibt annotations ich habe"}, {"id": 442, "start": 3443.88, "end": 3447.88, "text": "noch keine ahnung beziehungsweise ich habe ich habe noch nicht gecheckt wozu man annotations"}, {"id": 443, "start": 3447.88, "end": 3455.08, "text": "\u00fcberhaupt braucht ehrlich gesagt au\u00dfer dass manche ja nennen wir es mal third party tools"}, {"id": 444, "start": 3455.08, "end": 3461.16, "text": "gerne annotations benutzen normalerweise kommt man eigentlich mit labels mehr oder weniger aus"}, {"id": 445, "start": 3461.16, "end": 3468.6, "text": "kannst du den programm kurz kurz erkl\u00e4ren es gibt noch gar kein programm code das ist das"}, {"id": 446, "start": 3468.6, "end": 3473.24, "text": "template was visual studio macht wenn ich deployment wenn ich deployment hinzuf\u00fcge meiner"}, {"id": 447, "start": 3473.24, "end": 3481.88, "text": "kombiniertes config mehr gibt es da noch nicht also metadata das nennen wir mal cute app also"}, {"id": 448, "start": 3481.88, "end": 3488.44, "text": "quasi wie diese ressource ressource nennt sich das \u00fcbrigens was angelegt wird also diese ganze"}, {"id": 449, "start": 3488.44, "end": 3496.32, "text": "datei nennt sich manifest und was hier angelegt wird ist glaube ich eine ressource und der typ"}, {"id": 450, "start": 3496.32, "end": 3505.32, "text": "der ressource ist in dem fall deployment damit eure jammel files auch garantiert immer funktionieren"}, {"id": 451, "start": 3505.32, "end": 3512.12, "text": "gibt es hier oben eine api version also das hei\u00dft mal angenommen die bringen version 2 irgendwann"}, {"id": 452, "start": 3512.12, "end": 3517.36, "text": "raus was viel viel mehr sachen unterst\u00fctzt und hier oben immer noch api version 1 angegeben habt"}, {"id": 453, "start": 3517.36, "end": 3521.16, "text": "solange die version 1 weiter unterst\u00fctzen funktioniert euer manifest auch weiter"}, {"id": 454, "start": 3521.16, "end": 3526.8, "text": "api version gibt es \u00fcbrigens f\u00fcr f\u00fcr nahezu alle verschiedenen ressource typen"}, {"id": 455, "start": 3526.8, "end": 3537.48, "text": "so wir machen jetzt mal wir machen jetzt mal ein bisschen zackig da kannst du noch selektor"}, {"id": 456, "start": 3537.48, "end": 3543.32, "text": "und labels und so anlegen das ist wird gleich das wird gleich relevanter wenn wir da noch"}, {"id": 457, "start": 3543.32, "end": 3549.04, "text": "notbalancer und so was und so was da vorh\u00e4ngt so template metadata labels k\u00f6nnen wir auch so"}, {"id": 458, "start": 3549.04, "end": 3556.64, "text": "lassen cute app manifest v2 und v3 kann das war das in chrome ja sondern hier schreiben wir"}, {"id": 459, "start": 3556.64, "end": 3560.56, "text": "unser docker image rein beziehungsweise unser container image also immer einfach mal hello"}, {"id": 460, "start": 3560.56, "end": 3566.8, "text": "world dass du diese hello docker container dann kann man noch limits eintragen wie viel ram und"}, {"id": 461, "start": 3566.8, "end": 3572.16, "text": "cpu das verwenden darf ich kann euch ehrlich gesagt nicht genau sagen in welcher einheit die cpu ist"}, {"id": 462, "start": 3572.16, "end": 3580.24, "text": "das ist irgendwas irgendwas super super spezielles das ist nicht einfach nur anteile von einem cpu"}, {"id": 463, "start": 3580.24, "end": 3592.24, "text": "kern das sind irgendwelche megahertz sind nicht h\u00f6chstwahrscheinlich nicht irgendwie irgendwie"}, {"id": 464, "start": 3592.24, "end": 3597.6, "text": "irgendwie cpu zeit oder sowas was auch immer das habe ich nicht so genau gecheckt was das jetzt f\u00fcr"}, {"id": 465, "start": 3597.6, "end": 3603.6, "text": "eine einheit ist zumindest ram ist relativ klar und man sollte auch limits angeben das ist sinnvoll"}, {"id": 466, "start": 3603.6, "end": 3610.84, "text": "weil wenn ihr meine anwendung habt mit memory leak dann frisst ihr euch im zweifelsfall euren"}, {"id": 467, "start": 3610.84, "end": 3615.04, "text": "kompletten note an ram voll wenn ihr da keine limits angegeben habt das hei\u00dft man sollte immer"}, {"id": 468, "start": 3615.04, "end": 3620.04, "text": "man kann durchaus gro\u00dfz\u00fcgige limits angeben muss ja gar nicht sein aber ganz ohne limits ist meistens"}, {"id": 469, "start": 3620.04, "end": 3625.92, "text": "doof weil man kann das auch so einstellen beziehungsweise macht es automatisch man"}, {"id": 470, "start": 3626.0, "end": 3630.96, "text": "angenommen ihr habt memory leak und die anwendung wird immer gr\u00f6\u00dfer immer gr\u00f6\u00dfer immer gr\u00f6\u00dfer und"}, {"id": 471, "start": 3630.96, "end": 3638.76, "text": "irgendwann nach einem gigabyte ram ger\u00e4t es ans limit und kackt ab dann startet kuba netis"}, {"id": 472, "start": 3638.76, "end": 3642.0, "text": "die neu und dann l\u00e4uft das ding wieder eine woche oder so was auch immer bis es wieder abkackt"}, {"id": 473, "start": 3642.0, "end": 3647.6, "text": "wegen memory leaks aber ganz ohne limits ist doof so container port tja geben wir einfach mal"}, {"id": 474, "start": 3647.6, "end": 3653.0, "text": "container port 80 ein wir haben kein container port weil dieses image kein port aufmacht so"}, {"id": 475, "start": 3653.0, "end": 3662.2, "text": "jetzt sagen wir mal kubectl deploy apply nicht deploy apply minus f cute wir machen hier unten"}, {"id": 476, "start": 3662.2, "end": 3675.04, "text": "mal was auf watch kubectl get pots ihr seht es gibt noch keine pots aber jetzt gibt es einen"}, {"id": 477, "start": 3675.04, "end": 3680.96, "text": "pot gibt es n\u00e4mlich einen cute app pot und ihr seht das ding ist gleich wieder abgekackt"}, {"id": 478, "start": 3680.96, "end": 3689.96, "text": "weil es ist kein pot also das ist der standard docker hello world pot den ich hier gestartet"}, {"id": 479, "start": 3689.96, "end": 3696.76, "text": "habe der macht nichts anderes wie hello world ausgeben kuck docker run hello world der macht"}, {"id": 480, "start": 3696.76, "end": 3700.64, "text": "nichts anderes wie hello world ausgeben sich wieder zu beenden deswegen da hier guck hier"}, {"id": 481, "start": 3700.64, "end": 3706.52, "text": "deswegen funktioniert das nat\u00fcrlich nicht kuba neta sieht aha anwendung startet anwendung"}, {"id": 482, "start": 3706.52, "end": 3712.4, "text": "krascht kuba netas denkt die anwendung krascht weil die l\u00e4uft ja nicht mehr und jetzt versucht"}, {"id": 483, "start": 3712.4, "end": 3718.76, "text": "er die neu zu starten und das krascht immer wieder doch doch der hat gepullt das geht das"}, {"id": 484, "start": 3718.76, "end": 3723.44, "text": "geht nur relativ relativ zackig das ist ja auch nichts nix dabei sind das docker image"}, {"id": 485, "start": 3723.44, "end": 3728.52, "text": "wof\u00fcr muss man docker auch installiert sein nein du brauchst docker nicht installiert haben ich"}, {"id": 486, "start": 3728.52, "end": 3734.56, "text": "habe einfach bei mir lokal docker installiert und das sind streng genommen es sind docker images ja"}, {"id": 487, "start": 3734.56, "end": 3741.44, "text": "aber streng genommen sind docker images ja gar keine docker images mehr sondern sind oci images"}, {"id": 488, "start": 3741.44, "end": 3748.12, "text": "oder wie dieser kram jetzt hei\u00dft open container images also das ist was hei\u00dft ein oci open"}, {"id": 489, "start": 3748.12, "end": 3756.48, "text": "container image open container initiative image alles klar w\u00e4re auch zu einfach gewesen also das"}, {"id": 490, "start": 3756.48, "end": 3763.64, "text": "ist eine spezifikation von container images die auch docker verwendet aber mittlerweile auch ganz"}, {"id": 491, "start": 3763.64, "end": 3772.8, "text": "viele andere container tools aus dem container \u00f6kosystem also es muss kein docker image in"}, {"id": 492, "start": 3772.8, "end": 3779.84, "text": "dem sinn sein aber bietet sich jetzt in dem fall an so jetzt haben wir ein hello world container"}, {"id": 493, "start": 3779.84, "end": 3790.12, "text": "der niemals ready wird weil er immer krascht wie gesagt logisch weil naja macht ja macht ja auch"}, {"id": 494, "start": 3790.12, "end": 3798.48, "text": "nix der beendet sich auch einfach wieder so bevor wir ein netzwerk container starten k\u00f6nnen wir ja"}, {"id": 495, "start": 3798.48, "end": 3809.76, "text": "machen gut im eich traffic container traffic kurmei kommen wir mal starten wir bauen gleich noch eine"}, {"id": 496, "start": 3809.76, "end": 3814.28, "text": "kleine eigene dotnet anwendung das dauert zwei minuten doch jetzt sehen wir mal hier das zum"}, {"id": 497, "start": 3814.28, "end": 3822.6, "text": "beispiel traffic kurmei das ist was mit einem webserver wir applyen die datei wieder ihr erkennt"}, {"id": 498, "start": 3822.6, "end": 3827.76, "text": "ein gewisses muster wenn ihr schon mal beispielsweise euch den terraform stream"}, {"id": 499, "start": 3827.76, "end": 3831.88, "text": "angeguckt habt von mir das war ja oder generell wenn die terraform oder solche"}, {"id": 500, "start": 3831.88, "end": 3839.44, "text": "infrastructure management tools verwendet habt ihr erkennt ein gewisses muster es gibt"}, {"id": 501, "start": 3839.44, "end": 3845.68, "text": "eine config datei dessen state dann applied wird wobei man sagen muss terraform ist da etwas"}, {"id": 502, "start": 3846.68, "end": 3856.52, "text": "als die reinen kopernet des manifestes weil da werden teilweise nicht die states getrackt also"}, {"id": 503, "start": 3856.52, "end": 3861.24, "text": "wenn ich das hier jetzt umbenenne da kann es durchaus sein dass der pot weiter l\u00e4uft"}, {"id": 504, "start": 3861.24, "end": 3872.56, "text": "so mal gucken zumindest jetzt starten wir mal anderen container so und wir sehen schon der"}, {"id": 505, "start": 3872.56, "end": 3877.2, "text": "alte container wurde gestartet und dann der neue container wurde gestartet der alte container wurde"}, {"id": 506, "start": 3877.2, "end": 3881.84, "text": "gestoppt ich nenne mal container es ist eigentlich ein pot steht man auch hier oben aber irgendwie"}, {"id": 507, "start": 3881.84, "end": 3886.72, "text": "kommt mir container einfacher von den lippen wurde gestartet und der scheint jetzt auch zu"}, {"id": 508, "start": 3886.72, "end": 3895.8, "text": "funktionieren denn da ist running der ist ab des abend running und das passt ja eigentlich so weit"}, {"id": 509, "start": 3895.8, "end": 3904.0, "text": "ganz gut jetzt muss man sich nat\u00fcrlich \u00fcberlegen ok ich bin \u00fcbrigens gar nicht sicher ob das"}, {"id": 510, "start": 3904.0, "end": 3910.04, "text": "ding standardm\u00e4\u00dfig auf pot 80 l\u00e4uft das ist eine gute frage doch kann man probieren mal aus"}, {"id": 511, "start": 3910.04, "end": 3922.56, "text": "wei\u00df ich gar nicht was macht das standardm\u00e4\u00dfig pot 80 ok alles klar dann ups na geil jetzt"}, {"id": 512, "start": 3922.56, "end": 3932.12, "text": "ha eine sache wo ich nicht wei\u00df wie das in meinem neuen terminal geht wie switche ich die"}, {"id": 513, "start": 3932.12, "end": 3944.64, "text": "terminals jetzt lull einfach gar nicht aber irgendwie geht das ich war noch nie in der"}, {"id": 514, "start": 3944.64, "end": 3949.04, "text": "dass ich musste das noch nie ausprobieren so jetzt ist nat\u00fcrlich die sache gut wir haben"}, {"id": 515, "start": 3949.24, "end": 3955.0, "text": "wir haben jetzt ein pot laufen mit einem service drin auf pot 80 wir kommen da allerdings nicht"}, {"id": 516, "start": 3955.0, "end": 3961.84, "text": "dran also selbstverst\u00e4ndlich mal karl lokal host oder so da geht nichts ich kann mir mal zum"}, {"id": 517, "start": 3961.84, "end": 3969.52, "text": "beispiel die mini cube ip angucken und dann kann ich sagen karl ip auf mini cube geht aber auch"}, {"id": 518, "start": 3969.52, "end": 3976.24, "text": "nett also der l\u00e4uft zwar und wir wissen auch dass da was auf pot das wird funktionieren ja"}, {"id": 519, "start": 3976.24, "end": 3983.56, "text": "wir k\u00f6nnen wir k\u00f6nnen auch mini cube tunnel und so ged\u00f6ns machen kann man alles machen aber man"}, {"id": 520, "start": 3983.56, "end": 3988.12, "text": "sieht der container ist nicht so wirklich erreichbar damit ein container erreichbar ist"}, {"id": 521, "start": 3988.12, "end": 3998.28, "text": "m\u00fcssen wir erst einen service davor bauen einen service kann man sich so als art ja"}, {"id": 522, "start": 3998.28, "end": 4002.64, "text": "nie kann man nicht wirklich laut balancer kann man nicht so als eingang kann man nicht so als"}, {"id": 523, "start": 4002.64, "end": 4009.48, "text": "eingang zu diesem zu diesem zu diesen pots vorstellen so und jetzt kommt das was ich am"}, {"id": 524, "start": 4009.48, "end": 4016.0, "text": "anfang gesagt habe man kann in so einen kubernetes manifest nahezu beliebig viele ressourcen"}, {"id": 525, "start": 4016.0, "end": 4024.96, "text": "definieren und wir machen das jetzt zu \u00fcbungszwecken so dass wir den service f\u00fcr dieses deployment also"}, {"id": 526, "start": 4024.96, "end": 4029.4, "text": "im prinzip f\u00fcr die anwendung hier unten drunter definieren ich finde das ich finde das an der"}, {"id": 527, "start": 4029.4, "end": 4034.32, "text": "stelle nicht verkehrt das geh\u00f6rt hier auch direkt zusammen das kann man kann man finde ich schon"}, {"id": 528, "start": 4034.32, "end": 4041.56, "text": "machen an der stelle da streiten sich tats\u00e4chlich da jetzt die geister dran wie man das macht und"}, {"id": 529, "start": 4041.56, "end": 4048.12, "text": "auch wie man das benennt ja ich kenne leute die machen ordner also gut ordner ist prinzipiell"}, {"id": 530, "start": 4048.12, "end": 4052.12, "text": "f\u00fcr einzelne anwendung nicht verkehrt machen ordner machen da drunter dann immer so was wie"}, {"id": 531, "start": 4052.12, "end": 4061.08, "text": "deployment extra service sonst wie manche machen auch eine datei mit dem namen von dem jeweiligen"}, {"id": 532, "start": 4061.08, "end": 4067.84, "text": "jeweilige anwendung dieser drin laufen lassen wollen und schreiben dann deployments und service"}, {"id": 533, "start": 4067.84, "end": 4074.4, "text": "in eine datei allerdings ingress in der anderen ist ein bisschen ist die musik zu laut wirklich"}, {"id": 534, "start": 4074.72, "end": 4077.92, "text": "passt doch eigentlich oder bin ich zu leise"}, {"id": 535, "start": 4083.72, "end": 4088.76, "text": "ich glaube die ist an sich einfach nur kratten bisschen sehr intensiv"}, {"id": 536, "start": 4088.76, "end": 4107.72, "text": "ok komm wir machen dass alle happy sind so jetzt ist es minimal leiser aber immer noch gut"}, {"id": 537, "start": 4107.72, "end": 4137.2, "text": "so also damit diese ports verf\u00fcgbar sind oder wahrscheinlich m\u00fcsste man eher sagen"}, {"id": 538, "start": 4137.2, "end": 4141.76, "text": "dass die pots aus diesem deployment verf\u00fcgbar sind also nicht nur innerhalb des clusters sondern"}, {"id": 539, "start": 4141.76, "end": 4148.68, "text": "auch von au\u00dfen muss man da ein service davon und service ist quasi so der eingang zu einem"}, {"id": 540, "start": 4148.68, "end": 4154.92, "text": "deployment denkt das kann man ganz gut sagen es gibt verschiedene m\u00f6glichkeiten wie man"}, {"id": 541, "start": 4154.92, "end": 4163.56, "text": "services definieren kann also verschiedene typen von von services der in dem fall jetzt ganz"}, {"id": 542, "start": 4163.56, "end": 4168.12, "text": "praktische service w\u00e4re so was wie den load balancer zum beispiel weil wir machen auch gleich"}, {"id": 543, "start": 4168.12, "end": 4173.44, "text": "noch mehrere mehrere pots hier aktuell ist es ja nur ein potter l\u00e4uft so also sagen wir jetzt hier"}, {"id": 544, "start": 4173.44, "end": 4180.48, "text": "mal service jetzt muss ich mal kurz \u00fcberlegen meta meta metadata kann das gleiche sein das ist"}, {"id": 545, "start": 4180.48, "end": 4186.4, "text": "\u00fcbrigens auch da streiten sich auch die geister dran ob das hier nicht irgendwie app hei\u00dfen soll"}, {"id": 546, "start": 4186.4, "end": 4192.36, "text": "und das irgendwie service da kann man sich da kann man sich auch dar\u00fcber streiten wie man das machen"}, {"id": 547, "start": 4192.36, "end": 4206.88, "text": "will so port port muss rein wo ich m\u00f6chte dass dieser service erreichbar ist von au\u00dfen in dem"}, {"id": 548, "start": 4206.88, "end": 4214.48, "text": "fall sagen wir auch mal pot 80 und taget port ist auch 80 kann man im kubernetes manifest auch"}, {"id": 549, "start": 4215.08, "end": 4221.6, "text": "ja klar warum nicht das ist ja ganz normal textart ganz normales jammel so das hei\u00dft wir legen"}, {"id": 550, "start": 4221.6, "end": 4230.0, "text": "jetzt ein service an auf port 80 der auf diesen service ich nenne es mal balance balance an der"}, {"id": 551, "start": 4230.0, "end": 4239.92, "text": "stelle oder nicht so und hier ist das entscheidende hier im selektor entscheidet man jetzt wohin dieser"}, {"id": 552, "start": 4239.92, "end": 4246.48, "text": "service seine requests weiter leitet also nur weil das in der gleichen datei drinne steht"}, {"id": 553, "start": 4246.48, "end": 4251.96, "text": "bedeutet es nicht automatisch dass das zusammen geh\u00f6rt also hier wo ich den selektor angebe"}, {"id": 554, "start": 4251.96, "end": 4269.32, "text": "selektor ab cute ab und ihr seht hier das hier ist ich vermute mal das hier ist das relevante"}, {"id": 555, "start": 4269.32, "end": 4282.68, "text": "zumindest hier drauf match der also der guckt danach was es f\u00fcr deployments gibt was es f\u00fcr"}, {"id": 556, "start": 4282.68, "end": 4290.56, "text": "pots gibt mit diesem passenden tag dazu und dann balance der da drauf also sprich ist es egal in"}, {"id": 557, "start": 4290.56, "end": 4302.08, "text": "welchen dateien die stehen also wir sagen wir wollen alle cute so und jetzt damit das"}, {"id": 558, "start": 4302.08, "end": 4307.72, "text": "ganze hier testweise funktioniert m\u00fcssen wir noch ein typ angeben man sieht es gibt mehrere typ es"}, {"id": 559, "start": 4307.72, "end": 4313.32, "text": "gibt cluster ip ich glaube cluster ip ist der default wenn ich nicht ganz falsch liege"}, {"id": 560, "start": 4313.32, "end": 4325.72, "text": "so mal gucken ob ich das aus dem kopf halbwegs hinkriege also external name wei\u00df ich jetzt gar"}, {"id": 561, "start": 4325.72, "end": 4331.16, "text": "nicht was hei\u00dft cluster ip hat hei\u00dft dieser service kriegt eine ip aus dem cluster zugewiesen"}, {"id": 562, "start": 4331.16, "end": 4340.72, "text": "also irgendeine ip aus der aus der cluster range not port bedeutet auf dem jeweiligen cluster knoten"}, {"id": 563, "start": 4340.72, "end": 4351.6, "text": "wo der container gerade l\u00e4uft kriegt er kriegt er was zugewiesen und also random port zwischen"}, {"id": 564, "start": 4351.6, "end": 4363.2, "text": "30.000 und 35.000 oder so und in dem fall will ich load balance haben und jetzt applyen wir das"}, {"id": 565, "start": 4363.2, "end": 4368.56, "text": "ganze mal und dann hoffe ich dass ich nichts vergessen habe und der kram funktioniert"}, {"id": 566, "start": 4368.56, "end": 4380.4, "text": "so jetzt k\u00f6nnen wir zum beispiel hier unten mal sagen kubectl get service und man sieht da"}, {"id": 567, "start": 4380.4, "end": 4386.96, "text": "aha gucke mal da wir haben ein load balancer angelegt mit dieser cluster ip per external"}, {"id": 568, "start": 4386.96, "end": 4394.88, "text": "ip pending kann ruhig pendeln soviel es will und im port mapping das hei\u00dft wenn ich jetzt"}, {"id": 569, "start": 4394.88, "end": 4401.08, "text": "einen curl mache mini cube ip k\u00f6nnen wir uns merken wenn ich jetzt einen curl drauf mache"}, {"id": 570, "start": 4407.12, "end": 4409.44, "text": "da h\u00e4tte ich jetzt eigentlich gedacht dass es funktioniert"}, {"id": 571, "start": 4409.44, "end": 4418.56, "text": "keck wait"}, {"id": 572, "start": 4435.04, "end": 4439.36, "text": "das geht mit mini cube ich habe das selbst so mit mini cube schon gemacht"}, {"id": 573, "start": 4439.84, "end": 4443.84, "text": "da hat ja auch eine klasse ne ne die klasse die klasse ip da m\u00fcsstest du dich tunnen"}, {"id": 574, "start": 4443.84, "end": 4451.84, "text": "ne ne ne ne ne das funktioniert so glaube ich oder das ist immer jetzt jetzt jetzt lernen wir"}, {"id": 575, "start": 4451.84, "end": 4460.68, "text": "was okay wir probieren mal was aus braucht der lb nicht noch eine externe ip"}, {"id": 576, "start": 4460.68, "end": 4470.84, "text": "ne daten garten port mapping das sollte eigentlich"}, {"id": 577, "start": 4483.32, "end": 4488.6, "text": "ich meine wir k\u00f6nnten jetzt weiter gehen und noch noch den ingress davor machen aber"}, {"id": 578, "start": 4488.6, "end": 4491.8, "text": "sollte das nicht einfach so gehen jetzt"}, {"id": 579, "start": 4491.8, "end": 4501.28, "text": "ok ok also \u00fcbrigens ich zeige mal dass hier das hier funktioniert an der stelle nicht ja"}, {"id": 580, "start": 4501.28, "end": 4507.4, "text": "die du kannst die dinger nicht erreichen du kannst aber zum beispiel sagen mini cube ssh"}, {"id": 581, "start": 4507.4, "end": 4515.64, "text": "und dann m\u00fcsste es da drinne wahrscheinlich jetzt auch nicht"}, {"id": 582, "start": 4518.6, "end": 4523.64, "text": "ne ne ne ich brauche kein ingress wenn ich das hier ok mal gucken wir das mit cluster ip ist"}, {"id": 583, "start": 4523.64, "end": 4527.68, "text": "ich h\u00e4tte jetzt eigentlich wecken k\u00f6nnen das funktioniert so egal probieren wir mal aus apply"}, {"id": 584, "start": 4527.68, "end": 4536.0, "text": "ja guck mal mal hier unten was er macht cluster ip das das wird das wird so nat\u00fcrlich auch"}, {"id": 585, "start": 4536.0, "end": 4540.84, "text": "nicht funktionieren ich bin mir das muss das muss mit laut balancer noch funktionieren"}, {"id": 586, "start": 4549.6, "end": 4554.84, "text": "ja not port k\u00f6nnen wir auch ausprobieren dass in dem fall ja immer immer das gleiche"}, {"id": 587, "start": 4554.84, "end": 4560.92, "text": "in dem fall macht es ja keinen unterschied ist ja immer der gleiche note"}, {"id": 588, "start": 4560.92, "end": 4571.0, "text": "mal irgendwas habe ich irgendwas habe ich verkehrt gemacht irgendwas ergibt gerade keinen sinn"}, {"id": 589, "start": 4579.6, "end": 4587.24, "text": "ich habe wir k\u00f6nnen weitermachen und den und den reverse proxy davor pappen"}, {"id": 590, "start": 4597.24, "end": 4604.0, "text": "ach ich wei\u00df was ich mache was ich falsch mache ich benutze es doch auch falsch ich benutze es"}, {"id": 591, "start": 4604.48, "end": 4609.64, "text": "ich habe alles richtig gemacht ich habe alles richtig gemacht ich bin bescheuert also apply"}, {"id": 592, "start": 4609.64, "end": 4617.44, "text": "ist es stimmt alles passt mal auf es stimmt alles ich habe das nur falsch gemacht so"}, {"id": 593, "start": 4617.44, "end": 4625.12, "text": "nat\u00fcrlich die cluster ip ist ja nicht erreichbar intern deswegen muss ich jetzt folgendes machen"}, {"id": 594, "start": 4625.12, "end": 4639.0, "text": "mini cube service cute app und siehe da das service funktioniert da h\u00e4tte ich drauf"}, {"id": 595, "start": 4639.0, "end": 4643.28, "text": "gemusst ich h\u00e4tte ja auf die gem\u00e4bte auf den gem\u00e4bten port gehen m\u00fcssen guck auf den port"}, {"id": 596, "start": 4643.28, "end": 4650.48, "text": "h\u00e4tte ich gehen m\u00fcssen der map der port 30.000 430 466 auf port 80 von dem von dem container"}, {"id": 597, "start": 4650.48, "end": 4658.28, "text": "so so ist richtig jetzt funktioniert so das ist das ist mein service der hier l\u00e4uft"}, {"id": 598, "start": 4658.28, "end": 4670.08, "text": "das ist dieser hallo welt container von von traffic das ist der mini cube ip mit dem"}, {"id": 599, "start": 4670.08, "end": 4677.56, "text": "porter weitergeleitet wird ne load balance hast recht wahrscheinlich br\u00e4uchte ich das br\u00e4uchte"}, {"id": 600, "start": 4677.72, "end": 4681.16, "text": "das nicht wir k\u00f6nnen zwar wir k\u00f6nnen es mal weglassen und gucken ob es dann funktioniert"}, {"id": 601, "start": 4681.16, "end": 4696.36, "text": "nope siehst du funktioniert nicht notport w\u00fcrde funktionieren notport w\u00fcrde in dem"}, {"id": 602, "start": 4696.36, "end": 4704.08, "text": "fall wahrscheinlich auch funktionieren hallo mein gottes auto komplett wieder buggy ups"}, {"id": 603, "start": 4707.56, "end": 4716.32, "text": "ja das funktioniert auch also laut balancer braucht man an der stelle nicht wirklich"}, {"id": 604, "start": 4727.0, "end": 4731.68, "text": "und der service type notport auch externer ja das m\u00fcssen wie gesagt ich will es ja da gar"}, {"id": 605, "start": 4732.28, "end": 4738.96, "text": "so zumindest man sieht das service funktioniert damit was man jetzt ja schon sieht was ein"}, {"id": 606, "start": 4738.96, "end": 4743.24, "text": "bisschen doof ist ich meine wer kann hier wer hantiert damit den dinger wer kann damit rum"}, {"id": 607, "start": 4743.24, "end": 4748.12, "text": "hantieren mit den ports und und so also ich meine woher will jetzt zum beispiel jemand der auf"}, {"id": 608, "start": 4748.12, "end": 4756.48, "text": "mein service zugreift woher will der wissen dass er das \u00fcber port 3 2 2 5 9 machen muss das kann"}, {"id": 609, "start": 4756.48, "end": 4760.88, "text": "er nicht wissen so was man jetzt machen k\u00f6nnte ist man stellt das hier auf typ load balancer"}, {"id": 610, "start": 4760.88, "end": 4769.2, "text": "und mit einer wirklich externen erreichbaren ip dann m\u00fcsstest du aber die jedes mal wenn"}, {"id": 611, "start": 4769.2, "end": 4775.56, "text": "du das \u00fcber den cloud anbieter machst da m\u00fcsste es m\u00fcsste sich quasi \u00fcber jedes mal m\u00fcsstest du"}, {"id": 612, "start": 4775.56, "end": 4780.24, "text": "dann eine externe ip ressource anlegen in deiner cloud und hast dann am ende ganz viele externe"}, {"id": 613, "start": 4780.6, "end": 4786.68, "text": "viel sinnvoller ist es an der stelle so was wie ein reverse proxy davor zu pappen was wir auch"}, {"id": 614, "start": 4786.68, "end": 4790.88, "text": "schon gemacht haben mit docker compose mit docker mit zeug haben wir im stream schon \u00f6fters gemacht"}, {"id": 615, "start": 4790.88, "end": 4798.72, "text": "und zwar ein reverse proxy der dann auch am besten mit einem g\u00fcltigen let's encrypt zertifikat"}, {"id": 616, "start": 4798.72, "end": 4806.88, "text": "ausgestattet ist und der \u00fcber port 80 und 443 erreichbar ist das hei\u00dft neben dieser"}, {"id": 617, "start": 4806.88, "end": 4814.4, "text": "definition von den pots und dem service brauchen wir jetzt noch was f\u00fcr eingehenden traffic und"}, {"id": 618, "start": 4814.4, "end": 4822.0, "text": "das nennt sich bei kubernetes ingress das gegenteil von ingress ist egress nicht outgress so da"}, {"id": 619, "start": 4822.0, "end": 4829.08, "text": "gibt es verschiedene programme die man verwenden kann man kann traffic verwenden man kann engine"}, {"id": 620, "start": 4829.08, "end": 4838.08, "text": "x verwenden oder proxy oder sowas wir verwenden jetzt bei engine x aus dem einzigen grund weil"}, {"id": 621, "start": 4838.08, "end": 4847.32, "text": "bei mini cube schon in engine x addon f\u00fcr ingress dabei ist das macht man folgenderma\u00dfen also wenn"}, {"id": 622, "start": 4847.32, "end": 4855.0, "text": "ihr eine richtig fette kubernetes installation habt die bei jedem cloud anbieter l\u00e4uft oder so da"}, {"id": 623, "start": 4855.0, "end": 4859.4, "text": "k\u00f6nnt ihr nat\u00fcrlich daneben was ihr wollt aber mini cube bringt verschiedene addons mit und da"}, {"id": 624, "start": 4859.4, "end": 4866.4, "text": "ist zum beispiel dabei sowas wie ein ingress und das ist in dem fall engine x das hat noch"}, {"id": 625, "start": 4866.4, "end": 4870.44, "text": "mehrere praktische addons dabei zum beispiel registry die benutzen wir auch gleich noch wir"}, {"id": 626, "start": 4870.44, "end": 4881.24, "text": "machen n\u00e4mlich eine eigene dotnet anwendung die wir dann deployen hier \u00fcber zu der lokalen"}, {"id": 627, "start": 4881.8, "end": 4887.88, "text": "lokalen container registry ich muss noch mal kurz \u00fcberlegen in welcher reihenfolge wir das machen"}, {"id": 628, "start": 4887.88, "end": 4894.64, "text": "also ich w\u00fcrde sagen wir machen jetzt als erstes mal"}, {"id": 629, "start": 4894.64, "end": 4908.76, "text": "wir machen das in zwei stufen wir machen als erstes den ingress dann machen wir die dotnet an"}, {"id": 630, "start": 4908.76, "end": 4913.68, "text": "das drei zeilen dort netter es dauert nicht lang die dotnet anwendung und eine lokale registry"}, {"id": 631, "start": 4913.68, "end": 4920.24, "text": "und dann machen wir g\u00fcltige let's encrypt zertifikate f\u00fcr den ingress ich glaube das"}, {"id": 632, "start": 4920.24, "end": 4928.16, "text": "ist eine gute eine gute reinfolge das so zu machen ok das hei\u00dft als erstes enable wir mal"}, {"id": 633, "start": 4928.16, "end": 4942.04, "text": "unseren engine x addons enable ingress so und jetzt muss das muss ich ein bisschen"}, {"id": 634, "start": 4942.04, "end": 4949.68, "text": "abgucken leute das kriege ich n\u00e4mlich aus dem kopf nicht gebacken so also wir haben unsere"}, {"id": 635, "start": 4949.68, "end": 4954.96, "text": "pots bei der gelegenheit ihr seht hier vielleicht kommt mal mal im watch ihr seht hier wir haben"}, {"id": 636, "start": 4954.96, "end": 4960.48, "text": "aktuell nur einen pot laufen also quasi einen pot mit einem container drin das ist nat\u00fcrlich"}, {"id": 637, "start": 4960.48, "end": 4964.4, "text": "jetzt nicht sonderlich ausfallsicher man kann sagen das ist nat\u00fcrlich eh nicht ausfallsicher"}, {"id": 638, "start": 4964.4, "end": 4970.0, "text": "weil das alles auf meiner vm l\u00e4uft aber geben wir mal von aus es w\u00e4re nicht so ich habe im"}, {"id": 639, "start": 4970.0, "end": 4976.24, "text": "anfang gesagt eigentlich das kombiniert ist daf\u00fcr da um mehrere instanzen so von containern f\u00fcr"}, {"id": 640, "start": 4976.24, "end": 4981.2, "text": "uns zu verwalten deswegen kann man auch sagen man m\u00f6chte mehrere davon laufen haben da geht"}, {"id": 641, "start": 4981.2, "end": 4986.48, "text": "man hier hinten den moment wo muss man da hin templates ach fuck ich kann mir das immer nicht"}, {"id": 642, "start": 4986.48, "end": 4993.48, "text": "merken in spex spex muss man das irgendwo muss man replica reinschreiben man alter wo muss"}, {"id": 643, "start": 4993.48, "end": 5008.72, "text": "denn das wo muss denn das hin das muss doch unter spex oder ja das muss doch unter spex"}, {"id": 644, "start": 5008.72, "end": 5017.84, "text": "bin im deployment oder nicht"}, {"id": 645, "start": 5017.84, "end": 5035.6, "text": "ach da in die spex meine g\u00fcte mal ja genau ja ich bin gro\u00dfer jammel fan was das angeht also da"}, {"id": 646, "start": 5035.6, "end": 5042.88, "text": "kann man jetzt eingeben wie viele pots standardm\u00e4\u00dfig laufen sollen sagen wir mal zum beispiel f\u00fcnf"}, {"id": 647, "start": 5042.88, "end": 5049.04, "text": "ja komm vier wenn ich das jetzt noch mal apply werdet ihr feststellen auf einmal hoch guck mal da"}, {"id": 648, "start": 5049.04, "end": 5055.88, "text": "damit ja vier container das hei\u00dft unsere anwendung l\u00e4uft jetzt vier mal und wenn ich jetzt hier"}, {"id": 649, "start": 5056.44, "end": 5073.16, "text": "minikube service aufmach und funktioniert das auch kein ding aber es gibt noch mehr m\u00f6glichkeiten"}, {"id": 650, "start": 5073.16, "end": 5089.82, "text": "der chrome kecht ja scheint so wir k\u00f6nnen das ganze auch noch mal stimmt aber machen wir das"}, {"id": 651, "start": 5090.82, "end": 5107.14, "text": "guckt ihr seht jetzt auch die ip \u00e4ndert sich immer seht ihr das guck 12 6 12 10 11 also ihr"}, {"id": 652, "start": 5107.14, "end": 5112.9, "text": "seht die ip \u00e4ndert sich immer ich komme immer auf unterschiedlichen pots raus guck mal 69 pots"}, {"id": 653, "start": 5112.9, "end": 5117.98, "text": "machen wir jetzt mal jetzt wird hyperscaled hier apply ich hoffe das geht \u00fcberhaupt in meiner"}, {"id": 654, "start": 5117.98, "end": 5124.42, "text": "fremden gescheit b\u00e4mm jetzt haben wir die anwendung 69 mal laufen wenn ich jetzt in k\u00f6ln mache wir"}, {"id": 655, "start": 5124.42, "end": 5129.34, "text": "m\u00fcssen kurz warten bis alles erstellt wurde das ist leute ist das high performer mindset"}, {"id": 656, "start": 5137.34, "end": 5138.38, "text": "jetzt habe ich"}, {"id": 657, "start": 5143.38, "end": 5147.22, "text": "so guckt ich lande jetzt jedes mal auf einem anderen oder fast jedes mal auf"}, {"id": 658, "start": 5147.22, "end": 5154.58, "text": "einem anderen pot jetzt hier an der ip oben so und jetzt guckt mal was wo man jetzt schon mal"}, {"id": 659, "start": 5154.58, "end": 5159.14, "text": "sieht dass es sehr praktisch ist so ein container orchestrierungstool zu verwenden es gibt nat\u00fcrlich"}, {"id": 660, "start": 5159.14, "end": 5164.9, "text": "auch m\u00f6glichkeiten das automatisch zu skalieren anhand der last der ram auslastung und sonst was"}, {"id": 661, "start": 5164.9, "end": 5169.42, "text": "das machen wir heute nicht das n\u00e4chste mal geben wir auf was realistisches runter ich sag jetzt mal"}, {"id": 662, "start": 5169.42, "end": 5176.78, "text": "vier jetzt sage ich apply und passt auf es wird automatisch runter skaliert auf nur vier laufende"}, {"id": 663, "start": 5176.78, "end": 5189.18, "text": "pots warum ich linux verwende weil man gerade so netzwerk und technische sachen viel besser"}, {"id": 664, "start": 5189.18, "end": 5197.9, "text": "unter linux machen kann minikube unter windows geht auch dann macht er unter der haube glaube"}, {"id": 665, "start": 5198.06, "end": 5204.3, "text": "wie vm in der dann aber h\u00f6chstwahrscheinlich linux l\u00e4uft weil letztendlich sind es ja alles"}, {"id": 666, "start": 5204.3, "end": 5211.9, "text": "linux container und ich finde so sachen unter linux zu machen tausendmal angenehmer als unter"}, {"id": 667, "start": 5211.9, "end": 5218.06, "text": "windows allein schon dass man ein gescheites terminal hat ja das neue terminal unter windows"}, {"id": 668, "start": 5218.06, "end": 5227.38, "text": "ist halbwegs ertr\u00e4glich aber nix gegen ordentlich ordentliche linux umgebung so jetzt habt ihr"}, {"id": 669, "start": 5227.38, "end": 5231.78, "text": "gesehen man kann den kram hier auch ordentlich rauf und runter und runter skalieren wie man will"}, {"id": 670, "start": 5231.78, "end": 5235.1, "text": "am besten ist nat\u00fcrlich sp\u00e4ter man macht das automatisch aber das gucken wir uns mal in einem"}, {"id": 671, "start": 5235.1, "end": 5247.58, "text": "anderen stream an da habe ich auch gerade gesagt vor zwei minuten so soll das mal \u00fcberlegen was"}, {"id": 672, "start": 5247.58, "end": 5253.34, "text": "wollte ich als n\u00e4chstes machen genau also wir machen jetzt wir machen jetzt den ingress controller"}, {"id": 673, "start": 5253.34, "end": 5258.26, "text": "also es ist immer noch das problem dass ich hier in k\u00f6ln machen muss mit so einem komischen port"}, {"id": 674, "start": 5258.26, "end": 5264.74, "text": "normalerweise will man ja dass jemand so drauf zugreifen am besten noch mit dem dns namen ja"}, {"id": 675, "start": 5264.74, "end": 5270.46, "text": "darauf zugreifen kann also gar nicht gar nicht mal mit einem dns vor allem aber auf jeden fall \u00fcber"}, {"id": 676, "start": 5270.46, "end": 5275.3, "text": "port 80 und dann auf dem richtigen service rauskommt das hei\u00dft wir brauchen jetzt noch"}, {"id": 677, "start": 5275.3, "end": 5282.7, "text": "irgendein service der das ob dessen ingress ingress template hat ja wir brauchen das noch"}, {"id": 678, "start": 5282.7, "end": 5291.7, "text": "irgendein service der eben am eingang sitzt das entgegen nimmt und dann weiter leitet an"}, {"id": 679, "start": 5291.7, "end": 5299.98, "text": "den service wiederum an die pots das ganze nennt sich mal kubernetes nennt sich das ingress also"}, {"id": 680, "start": 5299.98, "end": 5306.86, "text": "nennen wir das hier mal cute app ingress wei\u00df ja nicht ob es minus in ingress wir k\u00f6nnen es"}, {"id": 681, "start": 5306.86, "end": 5318.46, "text": "nennen es aber wieder cute so wobei das tats\u00e4chlich sinnvoll ist das vielleicht ja anderweitig zu"}, {"id": 682, "start": 5318.46, "end": 5325.5, "text": "bedenken weil es k\u00f6nnte ja mehrere eing\u00e4nge geben aber wir machen es jetzt einfach mal so so host"}, {"id": 683, "start": 5330.98, "end": 5338.3, "text": "da k\u00f6nnen wir jetzt in dns name eintragen was auch sinnvoll ist ja dann nehmen wir jetzt nehmen"}, {"id": 684, "start": 5338.3, "end": 5350.3, "text": "wir jetzt zum beispiel mal kackel punkt keck w punkt services prefix alles service bei service"}, {"id": 685, "start": 5350.3, "end": 5355.78, "text": "kommt jetzt der name rein das service ist was wir hier oben definiert haben das ist cute app und"}, {"id": 686, "start": 5355.78, "end": 5364.02, "text": "bei port port 80 so und da muss man jetzt und da muss man jetzt ein bisschen gucken hier geht"}, {"id": 687, "start": 5364.02, "end": 5375.98, "text": "es drum um also um den target service nicht nicht hier irgendwie vom vom ingress controller dann"}, {"id": 688, "start": 5375.98, "end": 5379.58, "text": "brauchen wir noch ein paar andere sachen damit das ganze funktioniert das ist so magic kubernetes"}, {"id": 689, "start": 5379.58, "end": 5384.86, "text": "zeug und das muss ich jetzt abgucken weil das kriege ich aus dem kopf nicht hin wir verwenden"}, {"id": 690, "start": 5384.86, "end": 5392.1, "text": "als eingangs proxy engine x das hei\u00dft ich muss das wie gesagt das gucke ich ab das war ich aus"}, {"id": 691, "start": 5392.1, "end": 5401.54, "text": "dem kopf nicht ingress class name muss ich eintragen n\u00e4mlich engine x warum weil es"}, {"id": 692, "start": 5401.54, "end": 5416.7, "text": "nicht funktioniert jammel brauchste ultra wide screen monitore so und wenn ich jetzt alles"}, {"id": 693, "start": 5416.7, "end": 5428.06, "text": "richtig gemacht habe was ich mal hoffe dann sollten wir jetzt den ingress bekommen der"}, {"id": 694, "start": 5428.06, "end": 5440.38, "text": "sich f\u00fcr diese domain zust\u00e4ndig f\u00fchlt und dahin weiter leitet apply ok cube ctl get ingress"}, {"id": 695, "start": 5440.38, "end": 5455.06, "text": "what the fuck ingress ja haben wir host kackel punkt keck w services port 80 damit das damit"}, {"id": 696, "start": 5455.06, "end": 5461.06, "text": "das ganze funktioniert lokal m\u00fcssen wir folgendes machen und zwar in die host datei dieses ding hier"}, {"id": 697, "start": 5461.06, "end": 5470.78, "text": "mal eintragen also kacke punkt services aus dem kopf ip ist zuerst ja ip ist zuerst genau"}, {"id": 698, "start": 5470.78, "end": 5483.74, "text": "ip ist zuerst einer ip mit mehreren amt punkt 49 punkt 2 ist glaube ich die mini cube ip ok"}, {"id": 699, "start": 5483.74, "end": 5500.3, "text": "guck mal mal pink pink ok karl moment der wahrheit es funktioniert chrome was nein chrome und siehe"}, {"id": 700, "start": 5500.3, "end": 5508.78, "text": "da kackel kacke punkt services ist verf\u00fcgbar unter der domain und wir kommen auf unseren"}, {"id": 701, "start": 5508.78, "end": 5519.82, "text": "verschiedenen verschiedenen pots raus also der der traffic fluss ist jetzt so ich muss den namen"}, {"id": 702, "start": 5519.82, "end": 5526.58, "text": "aufl\u00f6sen in dem fall hier kackel punkt keck w punkt services da kommt die ip von meinem kubanet"}, {"id": 703, "start": 5526.58, "end": 5534.66, "text": "ist klasse da raus 1 2 1 6 8 4 9 punkt 2 dann geht das hier in den engine x ingress controller"}, {"id": 704, "start": 5534.66, "end": 5541.62, "text": "rein der leitet es weiter auf dieses backer der leitet wohlgemerkt alles weiter man kann auch"}, {"id": 705, "start": 5541.62, "end": 5547.9, "text": "sagen ich m\u00f6chte blo\u00df ab hier weiter leiten und sowas er leitet alles weiter an dieses backer mit"}, {"id": 706, "start": 5547.9, "end": 5554.82, "text": "dem namen cute app auf port 80 also auf den service cute app port 80 den service haben wir"}, {"id": 707, "start": 5554.82, "end": 5562.18, "text": "hier definiert target port port das ist das worauf der ingress controller das ganze weiter"}, {"id": 708, "start": 5562.18, "end": 5569.62, "text": "leitet und das service selbst verteilt das dann wieder auf alle m\u00f6glichen pots so hei\u00dft im"}, {"id": 709, "start": 5569.62, "end": 5575.7, "text": "endeffekt es funktioniert auch immer noch wenn ich hier reinschreibe 69 pots machen wir es"}, {"id": 710, "start": 5575.7, "end": 5591.94, "text": "mal apply damit skaliert das ganze hoch diagonales 5 hat hyperskalierungs cloud native high IQ zeug"}, {"id": 711, "start": 5591.94, "end": 5599.82, "text": "wie jetzt hoch skaliert und wenn ich jetzt wieder ein k\u00f6ln mache auf keck w punkt services sehen"}, {"id": 712, "start": 5599.82, "end": 5607.98, "text": "wir kriegen fast immer eine andere ip und wir landen jetzt auf einem von 96 69 verschiedenen"}, {"id": 713, "start": 5607.98, "end": 5617.22, "text": "pots kann euch das sogar zeigen dass das relativ viel viel ab kann was jetzt kein wunder ist wir"}, {"id": 714, "start": 5617.22, "end": 5625.74, "text": "k\u00f6nnen mal einen htp benchmark starten k\u00f6nnen man hatte tp benchmark starten sowas wie hey f\u00fcr"}, {"id": 715, "start": 5625.74, "end": 5626.5, "text": "eine minuten"}, {"id": 716, "start": 5633.06, "end": 5641.98, "text": "warum geht altpunkt nicht was sind das z mal zehn sekunden zehn sekunden volle pulle und ihr seht"}, {"id": 717, "start": 5641.98, "end": 5651.26, "text": "guck mal meine cpu last hier oben ist jetzt einfach mal ist jetzt einfach mal 100% oh ja"}, {"id": 718, "start": 5652.14, "end": 5655.22, "text": "habe ich hatte tp vergessen"}, {"id": 719, "start": 5665.7, "end": 5674.06, "text": "ihr seht auch meine laut geht \u00fcbelst hoch ja und wir haben 195.000 request geschickt alle hatten"}, {"id": 720, "start": 5674.06, "end": 5682.98, "text": "hatte tp status 200 hat wohl funktioniert ich bin mir jetzt gerade aus dem kopf nicht sicher wie"}, {"id": 721, "start": 5682.98, "end": 5693.02, "text": "man sich kommando zeilen m\u00e4\u00dfig die auslastung anzeigen lassen kann gibt es kubectl top oder"}, {"id": 722, "start": 5693.02, "end": 5698.98, "text": "so dinge also man kann zusatztool sowas wie lenz und sowas verwenden aber mal gucken ob kann man"}, {"id": 723, "start": 5698.98, "end": 5713.74, "text": "kubectl top gibt sowas hot node node minikube matrix api not verf\u00fcgbar ok"}, {"id": 724, "start": 5714.3, "end": 5725.9, "text": "minikube add-ons enable matrix server ist das so aber"}, {"id": 725, "start": 5732.34, "end": 5735.94, "text": "ok ok"}, {"id": 726, "start": 5735.94, "end": 5747.7, "text": "wo ich das restarten oder so das k\u00f6nnte jetzt in dem minikube geschichte sein dass das nicht will"}, {"id": 727, "start": 5747.7, "end": 5752.78, "text": "matrix server ist enabled"}, {"id": 728, "start": 5763.78, "end": 5765.02, "text": "der ram ist voll quatsch"}, {"id": 729, "start": 5766.46, "end": 5774.02, "text": "glaube ich nicht ach so ja doch das k\u00f6nnte tats\u00e4chlich sein ich war mal kurz vier aber"}, {"id": 730, "start": 5774.02, "end": 5784.06, "text": "ich glaube nicht dass da voll ist weil so dieser container braucht ja nix ich glaube da hat keine"}, {"id": 731, "start": 5784.06, "end": 5795.18, "text": "schmerzen so und wir sehen hier ist autocomplete das ok naja ich war mir k\u00f6nnen einfach mal"}, {"id": 732, "start": 5795.18, "end": 5806.74, "text": "minikube k\u00f6nnen das ding mal neu starten moment moment insecure konti registries"}, {"id": 733, "start": 5806.74, "end": 5813.7, "text": "ich glaube dann einfach probleme mit der matrix api wir starten wir starten das ding mal neu"}, {"id": 734, "start": 5813.7, "end": 5821.34, "text": "vielleicht geht es dann muss man manchmal machen ich hatte schon \u00f6fters das minikube"}, {"id": 735, "start": 5821.34, "end": 5824.82, "text": "lokal manche sachen ich wollte da muss man das neu starten oder funktioniert"}, {"id": 736, "start": 5824.82, "end": 5837.14, "text": "dass wir das mal neu starten dann gucken so weil als n\u00e4chstes gucken wir uns mal an wie man das"}, {"id": 737, "start": 5837.14, "end": 5845.66, "text": "mit lokaler image registry macht weil aktuell verwenden wir hier \u00f6ffentliche public verf\u00fcgbare"}, {"id": 738, "start": 5845.66, "end": 5850.54, "text": "docker images das ist ja meistens nicht der fall also wenn man eigene anwendung laufen lassen"}, {"id": 739, "start": 5850.54, "end": 5854.7, "text": "will da will man in der regel nicht diese anwendung public in einem docker container"}, {"id": 740, "start": 5854.7, "end": 5862.42, "text": "beziehungsweise in container image ins internet stellen zumindest viele wollen das nicht jetzt geht's"}, {"id": 741, "start": 5870.98, "end": 5871.98, "text": "in nebel"}, {"id": 742, "start": 5871.98, "end": 5881.38, "text": "ok minikube hat keinen bock"}, {"id": 743, "start": 5881.38, "end": 5888.06, "text": "minikube not found ok"}, {"id": 744, "start": 5888.06, "end": 5900.3, "text": "kann ich sowas machen wie get node also minikube wie dem auch sei pot ich kann"}, {"id": 745, "start": 5900.66, "end": 5908.58, "text": "mir anzeigen lassen kann man das so watchen lassen oder muss man das von hand machen wir"}, {"id": 746, "start": 5908.58, "end": 5913.94, "text": "f\u00fchren jetzt noch mal den kram hier aus also moment ne wobei es doch eigentlich ok"}, {"id": 747, "start": 5919.94, "end": 5926.94, "text": "keine ahnung na gut die selbst k\u00f6nnen ja gar nicht so viel sachen hier abkommen dass ich"}, {"id": 748, "start": 5926.94, "end": 5932.54, "text": "das sehe bin ich ein bisschen doof dass es nicht geht aber"}, {"id": 749, "start": 5932.54, "end": 5939.02, "text": "ach jetzt funktioniert not minikube auf einmal ok alles klar nice gut zu wissen"}, {"id": 750, "start": 5939.02, "end": 5942.62, "text": "hat wahrscheinlich einfach ein bisschen gedauert bis es gestartet ist und sagt"}, {"id": 751, "start": 5942.62, "end": 5956.9, "text": "jetzt w\u00fcrde ich eigentlich erwarten dass der ein bisschen mehr ausgelastet ist ist er nicht"}, {"id": 752, "start": 5956.9, "end": 5964.9, "text": "hatte aber kein bock drauf denkt sich nope was was interessieren mich 128.000 naja gut machen"}, {"id": 753, "start": 5964.9, "end": 5968.74, "text": "wir mal machen wir einfach mal mehr potts ist ja apply"}, {"id": 754, "start": 5968.74, "end": 5981.54, "text": "kubectl get potts haben wieder jede menge potts am start das ding ist hat einfach nur"}, {"id": 755, "start": 5981.54, "end": 5988.82, "text": "eine krasse krasse verzirkt ne wobei das ist jetzt jetzt pot erstellen so und jetzt"}, {"id": 756, "start": 5988.98, "end": 6001.98, "text": "nochmal attp benchmark mein ganzer rechner ist voll man es k\u00f6nnte auch sein dass das"}, {"id": 757, "start": 6001.98, "end": 6009.34, "text": "irgendwie richtig funktioniert weil das im container l\u00e4uft keine ahnung wei\u00df eh nicht"}, {"id": 758, "start": 6009.34, "end": 6014.54, "text": "wie man sich das gescheit auf kommando zeile anzeigen l\u00e4sst anscheinend so aber wirklich"}, {"id": 759, "start": 6015.1, "end": 6022.46, "text": "tut es ja nicht wirklich ja cpu 15 prozent na gut durchaus m\u00f6glich dass es nicht mehr sein kann"}, {"id": 760, "start": 6022.46, "end": 6031.38, "text": "als 15 prozent weil ich ja nur in zwei kerne docker container habe f\u00fcr mein minikube ach guck"}, {"id": 761, "start": 6031.38, "end": 6037.82, "text": "jetzt geht es ein bisschen hoch also wirklich wirklich wirklich toll zur \u00fcbersicht ist das ist"}, {"id": 762, "start": 6038.22, "end": 6045.34, "text": "gut aber jetzt war mal wieder mit dem was gescheites jetzt machen wir eine lokale container"}, {"id": 763, "start": 6045.34, "end": 6052.18, "text": "registry normalerweise ist es ja so wenn ihr anwendungen programmiert habt oder wenn ihr"}, {"id": 764, "start": 6052.18, "end": 6057.9, "text": "in einem unternehmen seid das der anwendung entwickelt hat und das auf kubanetis deployed"}, {"id": 765, "start": 6058.46, "end": 6070.3, "text": "dann ist das ja keine open source anwendung die irgendwo im internet verf\u00fcgbar ist ja das"}, {"id": 766, "start": 6070.3, "end": 6075.1, "text": "kenne ich das kenne ich habe ich aber noch nicht benutzt ich kenne das ich habe ich habe erst"}, {"id": 767, "start": 6075.1, "end": 6079.38, "text": "gestern mir ein paar kubanetis frontends angeguckt"}, {"id": 768, "start": 6086.02, "end": 6093.7, "text": "so und dementsprechend willst du ja auch keine container images bauen mit deiner"}, {"id": 769, "start": 6093.7, "end": 6100.38, "text": "close source super secret versicherungs banken anwendung und hier auf docker hab"}, {"id": 770, "start": 6100.38, "end": 6107.94, "text": "\u00f6ffentlich ins internet pushen so was du nat\u00fcrlich machen kannst ist wenn die firma in"}, {"id": 771, "start": 6107.94, "end": 6115.54, "text": "der cloud ist und in einem cloud anbieter vertraut kann man eine container registry verwenden also"}, {"id": 772, "start": 6115.54, "end": 6121.86, "text": "ich sag mal ihr kennt docker hab wahrscheinlich docker hab ist so die \u00e4lteste container registry"}, {"id": 773, "start": 6121.86, "end": 6127.18, "text": "die es gibt docker hab ist nicht die einzige github hat beispielsweise auch eine eigene"}, {"id": 774, "start": 6128.18, "end": 6134.98, "text": "docker hab kennen die meisten aber so docker hab ist halt public oder man meldet sich an und"}, {"id": 775, "start": 6134.98, "end": 6138.34, "text": "kriegt da auch ein private account aber es ist immer noch in der cloud das ist im internet also"}, {"id": 776, "start": 6138.34, "end": 6144.66, "text": "vielleicht m\u00f6chte man nicht seine anwendung oder die das unternehmen die anwendung ins internet"}, {"id": 777, "start": 6144.66, "end": 6153.14, "text": "stellen entweder benutzt man dann irgendeine registry wie docker hab allerdings blo\u00df angemeldet"}, {"id": 778, "start": 6153.14, "end": 6161.38, "text": "und private oder github private registry oder man hostet sich eine container registry selber ich"}, {"id": 779, "start": 6161.38, "end": 6167.06, "text": "pers\u00f6nlich w\u00fcrde eher dazu tendieren die container registry nicht selber zu hosten sondern die von"}, {"id": 780, "start": 6167.06, "end": 6173.38, "text": "jeweiligen cloud anbieter zu nehmen also wenn man zum beispiel bei amazon ist bei azure oder bei"}, {"id": 781, "start": 6173.38, "end": 6179.62, "text": "google oder geben wir mal bei google wenn ihr gcp google cloud kunde seid dann k\u00f6nnt ihr auch da"}, {"id": 782, "start": 6179.62, "end": 6186.38, "text": "eine container registry verwenden f\u00fcr unseren fall werden wir jetzt eine lokale container registry"}, {"id": 783, "start": 6186.38, "end": 6193.86, "text": "mit mit einem add-on was im minikube eingebaut ist verwenden das ist ganz praktisch dann muss"}, {"id": 784, "start": 6193.86, "end": 6199.18, "text": "ich mich hier nirgendswo anmelden docker hab technisch sondern kann das hier alles lokal machen"}, {"id": 785, "start": 6199.18, "end": 6205.5, "text": "dann machen wir eine kleine dotnet anwendung und die packen wir dann anstelle hier von who am i"}, {"id": 786, "start": 6205.5, "end": 6211.5, "text": "traffic container packen wir in einen docker container bzw. bauen container image und pushen"}, {"id": 787, "start": 6211.5, "end": 6217.02, "text": "das zu unserer lokalen container registry und bei der gelegenheit gucken wir uns dann auch noch"}, {"id": 788, "start": 6217.02, "end": 6226.9, "text": "ein paar einheiten an damit man so was updaten kann ohne dass die anwendung dabei ausf\u00e4llt"}, {"id": 789, "start": 6226.9, "end": 6232.74, "text": "so wir machen jetzt erstmal hier in den ordner nennen wir mal app so dotnet habe ich \u00fcberhaupt"}, {"id": 790, "start": 6232.74, "end": 6245.06, "text": "habe ich \u00fcberhaupt asp dotnet runtime installiert asp habe ich gar nicht null dachte mir noch"}, {"id": 791, "start": 6245.06, "end": 6251.1, "text": "irgendwie ich gehe mal kurz durch den chat durch was was da so gekommen ist ich habe wieder die"}, {"id": 792, "start": 6251.1, "end": 6258.98, "text": "h\u00e4lfte nicht gelesen wenn firmenleute mit kubernetes erfahrung suchen wo ist dann in dem"}, {"id": 793, "start": 6258.98, "end": 6262.62, "text": "job genau die schwierigkeit dass die firmen wahrscheinlich gar nicht wissen was sie wollen"}, {"id": 794, "start": 6262.62, "end": 6269.06, "text": "ich glaube viele denken sie wollen kubernetes weil man das kennt und damit ja automatisch alles"}, {"id": 795, "start": 6269.06, "end": 6276.5, "text": "agil wird und skaliert automatisch und alles super verf\u00fcgbar ist das ist aber ein trugschluss also"}, {"id": 796, "start": 6276.5, "end": 6283.42, "text": "man ist wahrscheinlich mit bekannter technologie mit leuten die sich damit auskennen besser als"}, {"id": 797, "start": 6283.42, "end": 6287.42, "text": "wenn man jetzt \u00fcbers kniepricht kubernetes f\u00fcr irgendeinen anwendungszweck wo es vielleicht"}, {"id": 798, "start": 6287.42, "end": 6296.1, "text": "noch gar nicht mal gro\u00dfartig sinnvoll ist ja sie juzu dankesch\u00f6n f\u00fcr den sub also ich glaube die"}, {"id": 799, "start": 6296.1, "end": 6298.82, "text": "gr\u00f6\u00dfte schwierigkeit ist dass die firmen wahrscheinlich nicht so genau wissen was sie"}, {"id": 800, "start": 6298.82, "end": 6304.3, "text": "wollen und ansonsten das f\u00fcr mich komplizierteste was kubernetes angeht ist eindeutig in dem"}, {"id": 801, "start": 6304.3, "end": 6308.62, "text": "\u00f6kosystem den \u00fcberblick zu behalten wie gesagt ich habe es am anfang gesagt ich bin auch nicht"}, {"id": 802, "start": 6308.62, "end": 6315.02, "text": "die pot sind running ich bin auch nicht das super krasse kubernetes checker ja und das"}, {"id": 803, "start": 6315.02, "end": 6324.3, "text": "\u00f6kosystem ist wirklich unglaublich komplex es gibt f\u00fcr es gibt f\u00fcr alles acht verschiedene"}, {"id": 804, "start": 6324.3, "end": 6331.82, "text": "l\u00f6sungen allein schon allein schon wie speichert man apk und passw\u00f6rter in kubernetes da gibt es"}, {"id": 805, "start": 6331.82, "end": 6337.26, "text": "f\u00fcnf sechs verschiedene varianten von einfach bis super von ein bisschen einfacher und super"}, {"id": 806, "start": 6337.26, "end": 6353.94, "text": "kompliziert ne habe ich nicht da als secrets stores die besser nicht weil als secrets steht"}, {"id": 807, "start": 6353.94, "end": 6360.5, "text": "das im klartext in dein jammel files drinnen das kann man machen aber sobald du den jammel"}, {"id": 808, "start": 6360.5, "end": 6366.46, "text": "files ins git einchecks stehen deine secrets im klartext drinnen so da gibt es verschiedene"}, {"id": 809, "start": 6366.46, "end": 6371.9, "text": "m\u00f6glichkeiten es gibt so sachen wie siehe secrets dann speichert man das ganze verschl\u00fcsselt"}, {"id": 810, "start": 6371.9, "end": 6382.9, "text": "verschl\u00fcsselt im git und entsperrt das quasi per key beim kubernetes cluster start dann gibt"}, {"id": 811, "start": 6382.9, "end": 6388.18, "text": "es irgendwas komisches von mozilla was quasi irgendwie so ein makro f\u00fcr jammel files ist"}, {"id": 812, "start": 6388.18, "end": 6394.1, "text": "dann gibt es sowas wie wollt und so da wird es aber schon langsam richtig kompliziert also"}, {"id": 813, "start": 6394.1, "end": 6404.06, "text": "gibt es unglaublich viel geschiss drumherum also gibt es ganz viel also da die ich glaube es"}, {"id": 814, "start": 6404.06, "end": 6410.02, "text": "gibt keinen den \u00fcberblick beh\u00e4lt ja da gibt es so sachen wie helm wo dran steht es ist ein"}, {"id": 815, "start": 6410.02, "end": 6415.86, "text": "paket ein package manager f\u00fcr kubernetes und du guckst dir mal helm chart definitionen an"}, {"id": 816, "start": 6415.86, "end": 6421.46, "text": "und musst erst mal kotzen wenn du die dateien siehst also da gibt es dann ja auch es ist auch"}, {"id": 817, "start": 6421.46, "end": 6427.06, "text": "nicht das einzige was es gibt gibt es auch wieder drei verschiedene sachen dann kannst du im"}, {"id": 818, "start": 6427.06, "end": 6432.14, "text": "zweifelsfall auch einfach sagen auch ich schei\u00df auf jammel und kannst es direkt \u00fcber terraform"}, {"id": 819, "start": 6433.14, "end": 6452.58, "text": "kubernetes provider und das ist also dadurch zu blicken ist tats\u00e4chlich f\u00fcr mich f\u00fcr mich"}, {"id": 820, "start": 6452.58, "end": 6462.18, "text": "das schwierigste ich blicke da auch nicht durch soll ich ganz ehrlich also es gibt es gibt so"}, {"id": 821, "start": 6462.18, "end": 6469.58, "text": "viele sachen wo ich keine ahnung von hab an sich tools was es f\u00fcr m\u00f6glichkeiten gibt und"}, {"id": 822, "start": 6469.58, "end": 6475.1, "text": "und sp\u00e4ter wenn es dann richtung service mesh und solche geschichten geht habe ich mich auch"}, {"id": 823, "start": 6475.1, "end": 6480.66, "text": "nur am rande mit besch\u00e4ftigt da wird sein ganz abgedreht also muss sagen so die basic funktion"}, {"id": 824, "start": 6480.66, "end": 6484.62, "text": "von kubernetes sowas wie das was wir gemacht haben das ist tats\u00e4chlich gar nicht so schwer"}, {"id": 825, "start": 6484.62, "end": 6491.42, "text": "wenn man sich ein bisschen an die namen gew\u00f6hnt hat wie die den kram nennen aber das was damit"}, {"id": 826, "start": 6491.42, "end": 6495.86, "text": "dann noch zus\u00e4tzlich kommt macht es kompliziert eine sache werden wir vielleicht gleich noch"}, {"id": 827, "start": 6495.86, "end": 6501.34, "text": "sehen mit manager und und zertifikate anlegen damit dann schon wieder ein bisschen komplizierter so"}, {"id": 828, "start": 6501.42, "end": 6510.66, "text": "wir bauen jetzt mal eine eigene net anwendung die wir benutzen als application application"}, {"id": 829, "start": 6510.66, "end": 6516.06, "text": "wie gesagt leute kein dotnet programmier stream das wird was ganz kleines was im"}, {"id": 830, "start": 6516.06, "end": 6525.18, "text": "prinzip nur aus drei zeilen oder so besteht also dort net new web"}, {"id": 831, "start": 6525.18, "end": 6537.46, "text": "so eine neue net anwendung und verwendest du das wim plugin wischl zu kontinue wenn ich"}, {"id": 832, "start": 6537.46, "end": 6544.74, "text": "wimm aufmache ist es der reale wimm noch nicht mal neowimm tats\u00e4chlich da stinkt normale wimm"}, {"id": 833, "start": 6545.34, "end": 6553.86, "text": "das ok ja ich traste traste alles hier voller trust"}, {"id": 834, "start": 6553.86, "end": 6564.34, "text": "dollen anwendung am start ja das habe ich da habe ich da habe ich jetzt gar nicht dran"}, {"id": 835, "start": 6564.34, "end": 6570.02, "text": "gedacht leute ja die ganzen cluster und storage m\u00f6glichkeiten gibt es ja auch noch du musst ja"}, {"id": 836, "start": 6570.02, "end": 6574.46, "text": "das m\u00fcssen wir uns ja auch noch angucken hier nicht heute im stream aber an der n\u00e4chsten"}, {"id": 837, "start": 6574.46, "end": 6580.82, "text": "stein guckt mal meine anwendungen hier sind ja komplett stateless aktuell es sollen sie im"}, {"id": 838, "start": 6580.82, "end": 6587.74, "text": "besten fall auch sein in kuba jedes ja aber irgendwo m\u00fcssen die daten ja hin der einfachste fall ist"}, {"id": 839, "start": 6587.74, "end": 6591.22, "text": "es gibt irgendwo den datenbank service und du connectest dich dahin liest und schreibst ein"}, {"id": 840, "start": 6591.22, "end": 6598.18, "text": "gutes aber manchmal brauchst du auch persistente dateien oder config files oder config eintr\u00e4ge"}, {"id": 841, "start": 6598.18, "end": 6602.34, "text": "mit environment variable und sowas also aber sp\u00e4testens bei dateien f\u00e4ngt es dann so an"}, {"id": 842, "start": 6602.34, "end": 6608.38, "text": "wo wie bekommst du wie bekommst du falls in den container da gibt es auch 1000 varianten von"}, {"id": 843, "start": 6608.38, "end": 6614.94, "text": "clusterfile system bis s3 du kannst dar\u00fcber s3 machen gibt es 1000 varianten und gerade diese"}, {"id": 844, "start": 6614.94, "end": 6621.3, "text": "vielfalt und dann die auswahl aus dieser vielfalt was lohnt sich denn jetzt f\u00fcr den jeweiligen"}, {"id": 845, "start": 6621.3, "end": 6626.9, "text": "anwendungsfall anwendungsfall ja das ist das ist moment ist glaube ich nicht die landkarte die"}, {"id": 846, "start": 6626.9, "end": 6637.82, "text": "ich kenne doch das ist die landkarte ich kenne ja die ist wirklich nice ja und das ist beim"}, {"id": 847, "start": 6637.82, "end": 6644.38, "text": "weitem nicht alles was da drin steht wenn ich reingucke gibt es viele sagen es ist vieles"}, {"id": 848, "start": 6644.38, "end": 6648.94, "text": "weil ich noch nie davon geh\u00f6rt habe und auch einiges was hier noch fehlt also da den \u00fcberblick zu"}, {"id": 849, "start": 6648.94, "end": 6654.38, "text": "behalten was es alles gibt und dann das passende auszuw\u00e4hlen f\u00fcr den anwendungsfall den man hat"}, {"id": 850, "start": 6654.38, "end": 6659.82, "text": "das ist fast das schwierigste an der ganzen kubanides geschichte und was man ehrlicherweise"}, {"id": 851, "start": 6659.82, "end": 6669.3, "text": "auch sagen muss was auch schwieriger ist als mit vms ist so ich meine ich habe jetzt hier"}, {"id": 852, "start": 6669.3, "end": 6674.86, "text": "drei pots laufen alles sch\u00f6n und gut aber wenn dann mal irgendwas nicht l\u00e4uft in so einem"}, {"id": 853, "start": 6674.86, "end": 6679.46, "text": "kubanides cluster rauszufinden warum da jetzt also mal so eine anwendung zu debuggen in so einem"}, {"id": 854, "start": 6679.46, "end": 6684.74, "text": "kubanides cluster ist gar nicht mal so einfach im prinzip k\u00f6nnt ihr eigentlich nur folgendes"}, {"id": 855, "start": 6684.74, "end": 6691.02, "text": "machen logs f\u00fcr den container k\u00f6nnt ihr euch angucken so viel mehr dbug m\u00f6glichkeiten habt"}, {"id": 856, "start": 6691.02, "end": 6696.42, "text": "ihr nicht es gibt noch die m\u00f6glichkeit sich rein zu connecten in den container das k\u00f6nnen wir uns"}, {"id": 857, "start": 6696.42, "end": 6703.1, "text": "k\u00f6nnen wir uns auch gleich angucken also man kann ja mit docker was ist docker exek minus it"}, {"id": 858, "start": 6703.1, "end": 6711.1, "text": "und bin w\u00e4sche also wir machen das mal docker beispiel ja ich krieg das immer nicht aus dem"}, {"id": 859, "start": 6711.1, "end": 6729.06, "text": "kopf hin was habe ich verkehrt gemacht docker exek und wenn er schon l\u00e4uft docker exek wenn"}, {"id": 860, "start": 6729.1, "end": 6735.38, "text": "er schon l\u00e4uft docker exek kann man sich ja in lokale docker container rein verbinden das"}, {"id": 861, "start": 6735.38, "end": 6743.46, "text": "gleiche gibt es auch gibt es auch cube ctl exek oder kann man sich versuchen in wei\u00df ich aber aus"}, {"id": 862, "start": 6743.46, "end": 6747.54, "text": "dem kopf \u00fcberhaupt nicht wird es funktionieren kann man sich versuchen rein zu connecten in die"}, {"id": 863, "start": 6747.54, "end": 6750.02, "text": "container wenn es da zum beispiel eine shell gibt und sowas"}, {"id": 864, "start": 6750.02, "end": 6763.26, "text": "es minus minus kommandok die haben aber die haben aber aktuell keine shell aber nicht dass du"}, {"id": 865, "start": 6763.26, "end": 6769.62, "text": "trotz ist es trotzdem schwierig so was so was zu dbuggen also da muss ich sagen das ist immer noch"}, {"id": 866, "start": 6769.62, "end": 6777.1, "text": "wenn du so eine vm hast wurde dich rein connecten kannst und da dann rum wurschteln"}, {"id": 867, "start": 6777.1, "end": 6784.02, "text": "kannst drinne angenehmer so wirklich rauszufinden wo ist der fehler zu gucken ok warum funktioniert"}, {"id": 868, "start": 6784.02, "end": 6790.06, "text": "jetzt die namensaufl\u00f6sung nicht und so was also so ein container deployment zu dbuggen finde ich"}, {"id": 869, "start": 6790.06, "end": 6795.94, "text": "pers\u00f6nlich eine ganze ecke anspruchsvoller ich habe das leider wieder zugemacht anspruchsvoller"}, {"id": 870, "start": 6795.94, "end": 6802.9, "text": "als eine vm wo was drinne l\u00e4uft hei\u00dft die frage wo kommen deine locks raus deine locks sind ja"}, {"id": 871, "start": 6802.9, "end": 6809.06, "text": "wahrscheinlich da nicht lokal in dem container sondern auf irgendeiner lock aggregierungs loki"}, {"id": 872, "start": 6809.06, "end": 6815.18, "text": "oder irgendwas oder lock sammel plattform oder elastix und was gibt es denn da alles mir f\u00e4llt"}, {"id": 873, "start": 6815.18, "end": 6823.78, "text": "es muss loki ein von dem was ich verwenden w\u00fcrde kibana gibt es da noch und und zeugs"}, {"id": 874, "start": 6823.78, "end": 6835.14, "text": "jenkins nicht jenkins ist kein lock lock sammel plattform das ist das falsche was ich hier auf"}, {"id": 875, "start": 6835.14, "end": 6841.46, "text": "mache das ist nicht gecheckt dass ich hier moment ich mache das writer ist wieder zu"}, {"id": 876, "start": 6841.46, "end": 6858.98, "text": "pepega ok hat es hat es wieder nicht gecheckt hier max nepos minikube ja trust ok wir machen"}, {"id": 877, "start": 6859.98, "end": 6870.58, "text": "anwendung wo ich noch ein paar Sachen zeigen kann hatten wir hier noch nie rider offen auf"}, {"id": 878, "start": 6870.58, "end": 6876.02, "text": "der neuen vm ich glaube nicht ich glaube das ist neu ok sekunde da m\u00fcssen wir erst mal was"}, {"id": 879, "start": 6876.02, "end": 6892.86, "text": "einstellen file settings short cuts ja fast fast ein action braucht wieder den standard"}, {"id": 880, "start": 6892.86, "end": 6911.82, "text": "short cuts ok ja ja remove und editor font ja mal wegen jet planes mono soll mir recht sein ok"}, {"id": 881, "start": 6911.82, "end": 6927.98, "text": "so jetzt jetzt sieht man ja hier was ach ja und sie pass mal auf leute flashbang"}, {"id": 882, "start": 6927.98, "end": 6942.7, "text": "ich war noch ein bisschen gr\u00f6\u00dfer so also wir machen jetzt wirklich eine ganz simple minimale"}, {"id": 883, "start": 6942.7, "end": 6958.66, "text": "dotnet anwendung also hello world wollen wir nicht wir wollen return cute chatter"}, {"id": 884, "start": 6958.66, "end": 6968.86, "text": "so und dann m\u00fcssen wir noch noch einstellen hier war name gleich wir machen das auch gleich mit"}, {"id": 885, "start": 6968.86, "end": 6987.34, "text": "environment variablen dass man auch was weiteres lernen kann environment variablen get so wunderbar"}, {"id": 886, "start": 6987.34, "end": 6995.18, "text": "reicht gut das ist unser unsere minimal dotnet web anwendung das reicht auch mehr brauchen wir"}, {"id": 887, "start": 6995.18, "end": 7000.7, "text": "zum testen an der stelle auch nicht reicht voll und ganz aus wir machen jetzt noch zwei sachen oder"}, {"id": 888, "start": 7000.7, "end": 7008.74, "text": "besser gesagt eine sache die sehr n\u00fctzlich ist und zwar sagen wir noch ich glaube bilder"}, {"id": 889, "start": 7008.74, "end": 7023.18, "text": "bilder ad health checks nee service ad health checks und dann bei ad ad nee ach fuck wie war"}, {"id": 890, "start": 7023.18, "end": 7032.22, "text": "das map map health checks und zwar das ganze unter slash helft wisst ihr warum helft und"}, {"id": 891, "start": 7033.22, "end": 7038.98, "text": "weil aus irgendwelchen gr\u00fcnden die kubanete leute sich gedacht haben es ist eine tolle idee"}, {"id": 892, "start": 7038.98, "end": 7044.42, "text": "das mit z hinten dran zu schreiben wahrscheinlich weil das andere schon zu oft belegt war oder so"}, {"id": 893, "start": 7044.42, "end": 7050.66, "text": "keine ahnung worum das herkommt aber die finden es cooler dass das mit slash helft vielleicht"}, {"id": 894, "start": 7050.66, "end": 7056.9, "text": "wei\u00df das deck overflow auch warum das so ist historisch die it comes from google internal best"}, {"id": 895, "start": 7056.9, "end": 7073.58, "text": "practices ok leute ihr habt es geh\u00f6rt helft ist google best practice excellent danke google so"}, {"id": 896, "start": 7073.58, "end": 7079.62, "text": "das ist unsere dotnet anwendung das war es auch schon mehr brauchen wir nicht was wir jetzt machen"}, {"id": 897, "start": 7079.62, "end": 7088.38, "text": "m\u00fcssen ist ein image bauen f\u00fcr diese dotnet anwendung und das mache ich ganz einfach ich"}, {"id": 898, "start": 7088.38, "end": 7096.14, "text": "klaue mir den image weil habe ich n\u00e4mlich schon vorbereitet kioko dankesch\u00f6n f\u00fcr den sub und"}, {"id": 899, "start": 7096.14, "end": 7104.3, "text": "sorry falls ich irgendjemanden \u00fcbersehen habe mit subs subscriptions ad docker support linux docker"}, {"id": 900, "start": 7104.3, "end": 7112.46, "text": "file das kommt alles weg weil das rider template kacke ist und jetzt b\u00e4rm soll ich das noch mal"}, {"id": 901, "start": 7112.46, "end": 7116.22, "text": "kurz erkl\u00e4ren oder wollen wir das docker feil einfach zur kenntnis nehmen und nicht mehr"}, {"id": 902, "start": 7116.22, "end": 7122.34, "text": "reingucken kann ich den stream als arbeits- oder weiterbildungszeit absetzen ich w\u00fcrde sagen schon"}, {"id": 903, "start": 7122.34, "end": 7128.06, "text": "wir machen hier schon wichtige wichtige dinge so weil mit diesem docker feil kann ich jetzt aus"}, {"id": 904, "start": 7128.06, "end": 7134.82, "text": "meiner dotnet anwendung ein container bauen und diesen image bauen und dieses image kann ich in"}, {"id": 905, "start": 7134.82, "end": 7140.42, "text": "eine container registry pushen und auf diese container registry kann ich dann an der stelle"}, {"id": 906, "start": 7140.42, "end": 7145.54, "text": "hier von kubernetes drauf zugreifen und meine eigene anwendung aus meiner eigenen container"}, {"id": 907, "start": 7145.54, "end": 7152.82, "text": "registry ziehen und nicht mehr aus einer \u00f6ffentlichen container registry im internet ja so sieht es aus"}, {"id": 908, "start": 7152.82, "end": 7158.82, "text": "wir k\u00f6nnen mal kurz schauen ob mein bild funktioniert docker bild punkt"}, {"id": 909, "start": 7158.82, "end": 7167.94, "text": "ich spreche mal kurz ab docker bild minus t cute app"}, {"id": 910, "start": 7167.94, "end": 7172.38, "text": "schauen wir mal"}, {"id": 911, "start": 7179.14, "end": 7179.66, "text": "irgendwann"}, {"id": 912, "start": 7183.3, "end": 7185.06, "text": "ist das dann auch fertig gebaut"}, {"id": 913, "start": 7191.34, "end": 7197.42, "text": "so docker run it's minus minus m cute app"}, {"id": 914, "start": 7197.42, "end": 7204.42, "text": "jawoll da sind wir port 80 karl localhost"}, {"id": 915, "start": 7204.42, "end": 7216.3, "text": "achso ich habe keinen ports weitergeleitet lull pk 80 80 cute chatter gibt es nicht guck mal"}, {"id": 916, "start": 7216.3, "end": 7222.9, "text": "keiner unsere anwendung hatten back wir brauchen noch wenn wenn die environment variable nicht"}, {"id": 917, "start": 7222.9, "end": 7236.06, "text": "gesetzt ist brauchen wir noch default value excellent ok cute app noch mal bilden keine cute"}, {"id": 918, "start": 7236.06, "end": 7252.34, "text": "chatters bekannt ok run cute app also wunderbar ok das ist unsere absolut high IQs maximum IQ"}, {"id": 919, "start": 7252.46, "end": 7257.1, "text": "also bessere web anwendung geht nicht mehr und wenn ich jetzt slash health mache dann"}, {"id": 920, "start": 7257.1, "end": 7261.74, "text": "kommt healthy zur\u00fcck und wenn ich weglasse kommt nichts weil bei google sagt da muss"}, {"id": 921, "start": 7261.74, "end": 7267.46, "text": "ein z hinten dran gut das ist unsere web anwendung fertig aus das war es mit c sharp f\u00fcr heute"}, {"id": 922, "start": 7267.46, "end": 7277.9, "text": "demn\u00e4chst wieder mehr hier test machen das n\u00e4chste mal wobei man keine tests braucht"}, {"id": 923, "start": 7277.9, "end": 7281.34, "text": "wenn man immer fehlerfreien code schreibt dann braucht man keine doku keine tests"}, {"id": 924, "start": 7281.34, "end": 7291.14, "text": "und keine kommentare weil es geht ja eh also sagen wir unsere unsere unsere unsere anwendung"}, {"id": 925, "start": 7291.14, "end": 7301.86, "text": "und jetzt m\u00fcssen wir diese anwendung pushen in unsere lokale container registry ich habe"}, {"id": 926, "start": 7301.86, "end": 7309.54, "text": "hier schon mal in wei\u00dfer voraussicht mein kubanete ist gestartet mit insecure registries"}, {"id": 927, "start": 7309.54, "end": 7315.34, "text": "weil naja von meiner container registries kein g\u00fcltiges SSL zertifikat als erstes"}, {"id": 928, "start": 7316.34, "end": 7333.38, "text": "mal enable registry und irgendwann ist es dann durch ich kann euch auch mal zeichnen cube ctl"}, {"id": 929, "start": 7333.38, "end": 7342.86, "text": "get ports for all namespaces und da seht ihr hier kommt mal da cube system code s etcd bla bla bla"}, {"id": 930, "start": 7342.86, "end": 7352.1, "text": "und hier ist die container registry also die addons sind unter der haube auch nur container"}, {"id": 931, "start": 7352.1, "end": 7359.5, "text": "warum nicht pots tja muss man mal google fragen wie das sein kann gut und jetzt m\u00fcssen wir in"}, {"id": 932, "start": 7359.5, "end": 7370.18, "text": "diese lokale container registry unsere app pushen das machen wir nicht mit docker ich kann euch"}, {"id": 933, "start": 7370.18, "end": 7377.3, "text": "auch sagen warum wir das nicht mit docker machen weil docker es nicht mag wenn die registry kein"}, {"id": 934, "start": 7377.3, "end": 7384.86, "text": "g\u00fcltiges SSL zertifikat hat das muss man das kann man irgendwie konfigurieren bei docker fragt"}, {"id": 935, "start": 7384.86, "end": 7391.34, "text": "mich nicht wo also von der idee her w\u00fcrde man w\u00fcrde man das bilden und dann w\u00fcrde man sagen"}, {"id": 936, "start": 7391.34, "end": 7405.26, "text": "docker push und dann die registry ja man muss in irgendeine konfig feier kann man es reinschreiben"}, {"id": 937, "start": 7405.26, "end": 7413.54, "text": "aber wir nehmen einfach potman weil potman hat eine konfig option daf\u00fcr potman ist quasi"}, {"id": 938, "start": 7413.54, "end": 7420.14, "text": "ein docker klon von rett hat mit ein paar sachen die docker nicht kann und umgedreht"}, {"id": 939, "start": 7420.14, "end": 7427.5, "text": "so weil jetzt kann man sagen potman bild und jetzt kommt es darauf an wie man das"}, {"id": 940, "start": 7427.5, "end": 7433.94, "text": "image nennt damit das ganze funktioniert warum nicht pot woman fragt mal twitter"}, {"id": 941, "start": 7433.94, "end": 7440.22, "text": "heute ist heute ist equal payday wei\u00dft du das eigentlich leute"}, {"id": 942, "start": 7440.22, "end": 7448.34, "text": "equal payday ist heute jetzt da sollte man schon mal potman in pot woman wenigstens mal"}, {"id": 943, "start": 7448.34, "end": 7457.9, "text": "zumindest f\u00fcr heute umbenennen so also damit das ganze funktioniert damit man sein sein image in"}, {"id": 944, "start": 7458.5, "end": 7464.26, "text": "die lokale container registry pushen kann muss man das taggen und zwar richtig taggen und zwar"}, {"id": 945, "start": 7464.26, "end": 7472.58, "text": "mit der ip und mit der ip beziehungsweise dem namen von der container registry und mit dem"}, {"id": 946, "start": 7472.58, "end": 7479.06, "text": "image wie das wie das ganze wie das ganze hei\u00dfen also ich muss mal sagen wie potman bild wie"}, {"id": 947, "start": 7479.06, "end": 7484.38, "text": "gesagt mit docker geht das auch wenn es g\u00fcltiges zertifikat vor der registry hat also potman"}, {"id": 948, "start": 7484.38, "end": 7490.18, "text": "bild 192 wobei wir machen es anders wir machen das wir machen wir machen den script kommen wir"}, {"id": 949, "start": 7490.18, "end": 7501.7, "text": "sind wir sind mal super ordentlich wir machen bild.sh bin bash registry gleich hoffe ich schreibe"}, {"id": 950, "start": 7501.7, "end": 7515.26, "text": "das ist jetzt richtig registry gleich mini cube ip auf port 5000 l\u00e4uft die registry das wei\u00df ich"}, {"id": 951, "start": 7515.26, "end": 7521.02, "text": "weil ich nachgeguckt habe und wir k\u00f6nnen noch so was wie einen tag hinten angeben vielleicht"}, {"id": 952, "start": 7521.02, "end": 7527.94, "text": "vielleicht gar nicht so bl\u00f6d wenn wir tag hinten angeben so und jetzt kommt das was wir an potman"}, {"id": 953, "start": 7527.94, "end": 7535.74, "text": "kommandos ausf\u00fchren ist n\u00e4mlich potman bild minus t registry also in dem fall die ip und"}, {"id": 954, "start": 7535.74, "end": 7541.86, "text": "der port und der port name von der app und jetzt k\u00f6nnen wir noch einen tag hinten dran machen"}, {"id": 955, "start": 7541.86, "end": 7548.14, "text": "so und wenn das fertig ist k\u00f6nnen wir sagen potman push und das muss ich mir jetzt kopieren weil"}, {"id": 956, "start": 7548.14, "end": 7555.46, "text": "ich das aus dem kopf nicht wei\u00df minus minus tls verify gleich falls und dann funktioniert das"}, {"id": 957, "start": 7555.46, "end": 7560.62, "text": "auch alles \u00fcber htp beziehungsweise auch mit ung\u00fcltigen zertifikaten und jetzt hier den"}, {"id": 958, "start": 7560.62, "end": 7567.1, "text": "gleichen krempel ich habe \u00fcbrigens auch einen punkt vergessen so wenn ich das script jetzt"}, {"id": 959, "start": 7567.1, "end": 7574.22, "text": "richtig gebaut habe dann sollte der uns jetzt einen container image bilden und in die registry"}, {"id": 960, "start": 7574.22, "end": 7594.5, "text": "pushen ok was habe ich denn verkehrt gemacht potman bild minus t registry ah ich habe den"}, {"id": 961, "start": 7594.5, "end": 7604.54, "text": "tag vergessen anzugeben ok ja also 1.0.0 hrq app ok jetzt ok er baut und ihr seht es sieht fast"}, {"id": 962, "start": 7604.54, "end": 7610.78, "text": "aus wie bei docker nur dass jetzt potman ist wieso der code war richtig funktioniert ohne"}, {"id": 963, "start": 7610.78, "end": 7622.98, "text": "jegliche \u00e4nderung so image ist gepusht nice und was ich jetzt machen kann ist folgendes ich"}, {"id": 964, "start": 7623.06, "end": 7631.02, "text": "trage jetzt an der stelle einfach ein localhost ich kann an der stelle jetzt localhost verwenden"}, {"id": 965, "start": 7631.02, "end": 7635.3, "text": "weil die registry innerhalb des kubanier des clusters l\u00e4uft es funktioniert an der stelle"}, {"id": 966, "start": 7635.3, "end": 7646.18, "text": "localhost 5000 slash cute app doppelpunkt versionsnummer 1.0.0 und jetzt werdet ihr"}, {"id": 967, "start": 7646.18, "end": 7653.62, "text": "feststellen ich zeige jetzt mal watch mein karl keckel"}, {"id": 968, "start": 7660.02, "end": 7665.74, "text": "ihr seht hier das ist der aktuelle service der da l\u00e4uft das ist noch der hallo welt service das"}, {"id": 969, "start": 7665.74, "end": 7676.06, "text": "ist noch nicht noch nicht unsere web anwendung was ist k\u00f6ln minus s glaube ich f\u00fcr weniger"}, {"id": 970, "start": 7676.06, "end": 7681.02, "text": "output genau so das ist noch nicht unser unser web anwendung ich habe jetzt in dem fall hier in"}, {"id": 971, "start": 7681.02, "end": 7688.54, "text": "unserer in unserem manifest unserem kubanier das manifest das image ausgetauscht was ich als"}, {"id": 972, "start": 7688.54, "end": 7692.34, "text": "anwendung verwenden will vorher hatte ich hier diese traffic hello world ab und jetzt habe ich"}, {"id": 973, "start": 7692.34, "end": 7698.34, "text": "unsere eigene anwendung da rein gepackt die wir die wir vorher als container image verpackt haben"}, {"id": 974, "start": 7698.34, "end": 7703.18, "text": "und in diese registry gepusht haben und wenn jetzt alles funktioniert ich hoffe es mal wir"}, {"id": 975, "start": 7703.18, "end": 7712.94, "text": "machen hier wir machen hier parallel noch mal ein get pods dass wir n\u00e4mlich sehen wenn es fehler"}, {"id": 976, "start": 7712.94, "end": 7725.74, "text": "gibt studio muss mal kurz weg und jetzt sage ich apply und vorher immer raus apply b\u00e4r so alte"}, {"id": 977, "start": 7725.74, "end": 7731.06, "text": "container werden neue container werden gestartet alte container werden gestattet cute chat eier"}, {"id": 978, "start": 7731.06, "end": 7737.78, "text": "zack ich habe jetzt quasi neue anwendung deployed ohne das request kaputt gegangen sind zwischendurch"}, {"id": 979, "start": 7737.78, "end": 7742.42, "text": "wie euch vielleicht aufgefallen ist das nat\u00fcrlich schon nice gelb"}, {"id": 980, "start": 7742.42, "end": 7755.3, "text": "machen wir das mal weg also wir wissen jetzt dass mit dem image funktioniert so jetzt haben wir"}, {"id": 981, "start": 7755.3, "end": 7759.26, "text": "hier unsere eigene anwendung drauf laufen jetzt brauchen wir noch ein paar andere geschichten"}, {"id": 982, "start": 7759.74, "end": 7767.5, "text": "dass das ordentlich funktioniert weil aktuell ist es so und um euch das mal zu demonstrieren"}, {"id": 983, "start": 7767.5, "end": 7771.66, "text": "beziehungsweise dass man ich zeige es auch mal dass man sich besser vorstellen kann machen wir mal"}, {"id": 984, "start": 7771.66, "end": 7776.18, "text": "sexo machen wir mal 69 container"}, {"id": 985, "start": 7776.18, "end": 7784.82, "text": "ihr werdet feststellen gleich gibt es wieder jede menge container"}, {"id": 986, "start": 7784.82, "end": 7797.38, "text": "w\u00e4hrend das aufbauen wir machen mal eine ein ein dirty hack in unserem build script den sehe ich"}, {"id": 987, "start": 7797.38, "end": 7805.94, "text": "hier nehme ich gerade dr\u00fcben dass ich das auch aus anderen verzeichnissen ausf\u00fchren kann so"}, {"id": 988, "start": 7805.94, "end": 7816.5, "text": "abbild und jetzt machen wir mal eine neue versionsnummer von der app ja also zum"}, {"id": 989, "start": 7816.5, "end": 7820.94, "text": "beispiel okay ich \u00e4ndere ich \u00e4ndere der form halber auch noch was ja es gibt ja west version"}, {"id": 990, "start": 7820.94, "end": 7830.58, "text": "1.0.1 von dieser app und zwar ist es cute chatter ja ich will ich will hier wobei irgendwas"}, {"id": 991, "start": 7830.58, "end": 7852.46, "text": "ist es jetzt okay schreibt man das \u00fcberhaupt so ist es jetzt noch noch mehr alles und das will"}, {"id": 992, "start": 7852.46, "end": 7857.66, "text": "ich jetzt einchecken also ich baue neues image draus aus der anwendung mit diesem mit diesem fix"}, {"id": 993, "start": 7858.26, "end": 7869.54, "text": "image version 1.0.1 pusht das wieder in die registry so und jetzt apply ich das und was"}, {"id": 994, "start": 7869.54, "end": 7877.94, "text": "wird jetzt passieren leute es wird nix passieren es wird nix passieren weil es hat sich ja nix"}, {"id": 995, "start": 7877.94, "end": 7887.02, "text": "ge\u00e4ndert es hat sich ja nix ge\u00e4ndert warum sind meine pots eigentlich alle pending da unten"}, {"id": 996, "start": 7887.02, "end": 7895.1, "text": "ich glaube ich habe zu viele kann es sein dass ich mit den ressourcen am limit bin dass das meine"}, {"id": 997, "start": 7895.1, "end": 7901.46, "text": "dass ich zu viele pots habe okay moment wir skalieren es mal runter auf was sinnvolles auf"}, {"id": 998, "start": 7901.46, "end": 7920.54, "text": "10 oder so also wenn ich das jetzt applye und wieder curl mache werdet ihr vielleicht"}, {"id": 999, "start": 7920.54, "end": 7928.42, "text": "feststellen es ist immer noch die alte message warum na ja weil die versionsnummer die hier"}, {"id": 1000, "start": 7928.42, "end": 7934.38, "text": "drin ist immer noch 1.0 ist ich will ja 1.0.1 haben so und jetzt wird euch gleich ein problem"}, {"id": 1001, "start": 7934.38, "end": 7941.98, "text": "auffallen wenn ich das jetzt applye dann wird er updaten dann wird er updaten von dieser von"}, {"id": 1002, "start": 7941.98, "end": 7948.66, "text": "dem tag 1.0.0 auf 1.0.1 und wenn ihr mal genau guckt was er hier unten macht wird euch vielleicht"}, {"id": 1003, "start": 7948.66, "end": 7959.3, "text": "was auffallen der startet neue container und l\u00e4sst dann die alten container l\u00f6st dann die"}, {"id": 1004, "start": 7959.3, "end": 7971.1, "text": "alten container aber euch wird auffallen das zeug ist relativ schnell ready das zeug ist relativ"}, {"id": 1005, "start": 7971.1, "end": 7978.98, "text": "schnell ready das komisch und das ist aber auch doof weil wenn jetzt im falschen momenten ein"}, {"id": 1006, "start": 7978.98, "end": 7985.38, "text": "request rankommt k\u00f6nnte es ja theoretisch sogar sein dass der request auf dem container landet"}, {"id": 1007, "start": 7985.38, "end": 7993.86, "text": "der noch gar nicht richtig gestartet ist das hei\u00dft wir m\u00fcssen jetzt nicht nur den container"}, {"id": 1008, "start": 7993.86, "end": 7999.38, "text": "starten sondern wir m\u00fcssen auch checken ob der container bereit ist ob der container bereit ist"}, {"id": 1009, "start": 7999.38, "end": 8008.86, "text": "machen wir mit den health checks hier drinnen wenn du eine fehlerhafte version deploys"}, {"id": 1010, "start": 8008.86, "end": 8015.82, "text": "der ist ja quasi ach so jaja jaja ach das ist eine gute idee das kann ich auch zeigen das kann"}, {"id": 1011, "start": 8015.82, "end": 8021.1, "text": "ich auch zeigen guckt mal ich skaliere das mal ein bisschen runter auf sechs st\u00fcck erst mal"}, {"id": 1012, "start": 8021.1, "end": 8027.54, "text": "wir werfen keine exception ich trage einfach eine tag ein den es nicht gibt also ich gehe"}, {"id": 1013, "start": 8027.58, "end": 8033.18, "text": "mal kurz runter auf sechs das sieht man also mal angenommen ich sage jetzt hier nehm mal"}, {"id": 1014, "start": 8033.18, "end": 8040.86, "text": "image version 1.0 2 die es noch nicht gibt und ich applye das jetzt dann werdet ihr feststellen"}, {"id": 1015, "start": 8040.86, "end": 8049.54, "text": "der versucht es zu applyen aber er image pull oder macht nicht weiter und die alten laufen weiter"}, {"id": 1016, "start": 8049.54, "end": 8053.26, "text": "das hei\u00dft ich habe obwohl ich eine fehlerhafte konfig haben das ist wirklich auch eine coole"}, {"id": 1017, "start": 8054.02, "end": 8061.54, "text": "obwohl ich eine fehlerhafte konfig habe ein weiterhin funktionierenden service zwar die"}, {"id": 1018, "start": 8061.54, "end": 8069.9, "text": "alte version aber egal ich meine besser als kaputt kaputt w\u00e4re glaube ich das bl\u00f6deste"}, {"id": 1019, "start": 8069.9, "end": 8076.26, "text": "was was an der stelle passieren k\u00f6nnte so wenn ich das als wieder r\u00fcckg\u00e4ngig mache mal wieder"}, {"id": 1020, "start": 8076.26, "end": 8083.3, "text": "version 1 rein sache play dann checkt er das ganze auch und er geht wieder auf sechs replikas hoch"}, {"id": 1021, "start": 8083.3, "end": 8088.02, "text": "also ich habe angeheben ich m\u00f6chte sechs davon laufen lassen und das ist wirklich cool und das"}, {"id": 1022, "start": 8088.02, "end": 8095.34, "text": "im gro\u00dfen stil ist eigentlich der grund schlechthin also dieses solche features in der richtung ist"}, {"id": 1023, "start": 8095.34, "end": 8100.54, "text": "nicht explizit dieses ist der grund schlechthin warum kuba netis sich \u00fcberhaupt so durchgesetzt"}, {"id": 1024, "start": 8100.54, "end": 8109.34, "text": "hat du kannst halt wunderbar deinen ganzen workload \u00fcber mehrere notes skalieren und du"}, {"id": 1025, "start": 8109.34, "end": 8116.82, "text": "kannst relativ gut ausfallfrei deine software managen damit das ist eigentlich der haupts\u00e4chliche"}, {"id": 1026, "start": 8116.82, "end": 8122.74, "text": "grund warum warum kuba netis so beliebt ist oder sich auch so durchgesetzt hat gab ja noch ein"}, {"id": 1027, "start": 8122.74, "end": 8133.26, "text": "paar andere andere dinge es gab dann zwischenzeitlich mal docker swarm was ja immer noch gibt kaum"}, {"id": 1028, "start": 8133.26, "end": 8137.34, "text": "jemand benutzt da gibt es noch ein paar andere geschichten wo mir grad nichts ein gibt es noch"}, {"id": 1029, "start": 8137.34, "end": 8142.5, "text": "nomad gibt es noch das ist aber glaube ich auch noch mehr mit vms dabei f\u00e4llt jemand nehmen eine"}, {"id": 1030, "start": 8142.5, "end": 8148.58, "text": "gute kuba netis alternative ein hat der canonical nicht auch mal den versuch mit lxd gestartet"}, {"id": 1031, "start": 8148.58, "end": 8153.18, "text": "irgendwie sowas zu bauen was keiner haben wollte"}, {"id": 1032, "start": 8153.18, "end": 8166.26, "text": "mesos mesos gibt es noch richtig apache war das apache openshift wobei ist openshift noch"}, {"id": 1033, "start": 8166.26, "end": 8173.98, "text": "deutlich mehr nicht noch deutlich mehr dabei dass du da auch ne moment das ist nicht openshift das"}, {"id": 1034, "start": 8173.98, "end": 8179.94, "text": "ist openshift also openshift ist die telekom cloud das wei\u00df ich die telekom cloud ist"}, {"id": 1035, "start": 8179.94, "end": 8187.54, "text": "openshift aber das hei\u00dft ja nicht dass es das hei\u00dft ja nicht dass es schlecht sein muss wie"}, {"id": 1036, "start": 8187.54, "end": 8198.02, "text": "hei\u00dft das andere wo noch vms und so dabei sind open stack meine ich ich meine open stack und"}, {"id": 1037, "start": 8198.02, "end": 8203.5, "text": "die telekom cloud ist open stack darum war das nicht openshift open stack meine ich genau"}, {"id": 1038, "start": 8203.5, "end": 8220.26, "text": "ja richtig richtig so sieht es aus so was wollte ich was wollte ich denn jetzt noch machen ja also"}, {"id": 1039, "start": 8220.26, "end": 8226.54, "text": "ihr habt jetzt ja gesehen es sind jetzt ganz viele services neu gestartet worden aber die"}, {"id": 1040, "start": 8226.58, "end": 8231.74, "text": "dinge waren sofort ready das will man ja nicht das hei\u00dft wir haben ja hier einen health check"}, {"id": 1041, "start": 8231.74, "end": 8240.9, "text": "definiert service fabric okay mir f\u00e4llt schon au\u00dfer kuba netis gar nicht gar nicht mehr so viel"}, {"id": 1042, "start": 8240.9, "end": 8246.7, "text": "ein so man kann jetzt an der stelle sagen hier wo man wo man replikas eintr\u00e4gt kann ich auch noch"}, {"id": 1043, "start": 8246.9, "end": 8262.7, "text": "sagen strategy type rolling ich glaube rolling update glaube ich ist sogar der default da bin"}, {"id": 1044, "start": 8262.7, "end": 8267.86, "text": "ich mir aber nicht ganz sicher rolling update und dann kann man das braucht man eigentlich gar"}, {"id": 1045, "start": 8267.86, "end": 8279.98, "text": "nicht ja da kann man dann noch so was sagen wie max unavailable 1 und max max search 1 oder man"}, {"id": 1046, "start": 8279.98, "end": 8285.7, "text": "kann auch 2 in dem fall so kann man jetzt einstellen wie er updates machen soll also"}, {"id": 1047, "start": 8285.7, "end": 8293.78, "text": "rolling update ist glaube ich relativ selbsterkl\u00e4rend und wobei das ja noch nicht alles ist was wir"}, {"id": 1048, "start": 8293.78, "end": 8302.78, "text": "machen m\u00fcssen und hier kann man jetzt ausw\u00e4hlen wie er vorgehen soll beim rolling upgrade also von"}, {"id": 1049, "start": 8302.78, "end": 8312.06, "text": "diesen sechs replikas wie viele d\u00fcrfen nicht erreichbar sein w\u00e4hrend dem update und wie"}, {"id": 1050, "start": 8312.06, "end": 8318.98, "text": "viele d\u00fcrfen mehr da sein als target also mit diesen settings w\u00fcrde folgendes passieren er"}, {"id": 1051, "start": 8318.98, "end": 8326.42, "text": "k\u00f6nnte zum beispiel einen neuen container erzeugen schon im vorfeld bevor er den alten"}, {"id": 1052, "start": 8326.42, "end": 8332.62, "text": "stoppt und dann k\u00f6nnen alten stoppen so dass kurzzeitig f\u00fcnf nur laufen und das zwischenzeitlich"}, {"id": 1053, "start": 8332.62, "end": 8338.78, "text": "kurz auch mal sieben laufen k\u00f6nnen was ja wichtig ist weil bei einem update m\u00fcssen ja neue container"}, {"id": 1054, "start": 8338.78, "end": 8342.54, "text": "gestartet werden alte container gestoppt werden ist halt die frage wie man das macht man kann"}, {"id": 1055, "start": 8342.54, "end": 8348.54, "text": "nat\u00fcrlich auch hier mehr eintragen wenn man damit dacor ist dass auch mal von diesen replikas weniger"}, {"id": 1056, "start": 8348.54, "end": 8353.78, "text": "laufen hiermit kann man das quasi ein bisschen feintunen wie man dieses update wie man dieses"}, {"id": 1057, "start": 8353.78, "end": 8360.5, "text": "update durchf\u00fchren will wenn ich jetzt wenn ich jetzt sag wie ging das jetzt die deployment"}, {"id": 1058, "start": 8360.5, "end": 8375.62, "text": "cube cube rollout cube ctl rollout restart wobei ich brauche ich gar nicht ich kann es einfach"}, {"id": 1059, "start": 8375.62, "end": 8392.02, "text": "machen doch anscheinend ja ok cube ctl rollout restart deployment das sieht man jetzt macht"}, {"id": 1060, "start": 8392.02, "end": 8402.14, "text": "er das halt entsprechend dieser policy das maximum einer eben hier immer abgecreated wird aber wenn"}, {"id": 1061, "start": 8402.14, "end": 8408.46, "text": "ich jetzt hier zum version runter runter gehe auf version eins sach apply dann seht ihr ok"}, {"id": 1062, "start": 8408.46, "end": 8414.74, "text": "der erzeugt zwei neue und er stoppt immer kann auch immer ein zwei austauschen immer ein mehr"}, {"id": 1063, "start": 8414.74, "end": 8423.14, "text": "und ein weniger kann man das ein bisschen feintunen was er was er machen darf so was aber viel"}, {"id": 1064, "start": 8423.14, "end": 8430.38, "text": "wichtiger ist wir brauchen noch health checks weil ihr seht jedes mal wenn der container"}, {"id": 1065, "start": 8430.38, "end": 8435.18, "text": "gestartet ist er sofort ready das stimmt ja gar nicht der container ist gar nicht sofort ready"}, {"id": 1066, "start": 8435.18, "end": 8440.66, "text": "der muss ja auch erst mal starten die web anwendung muss starten ich mein das geht schnell ja mein"}, {"id": 1067, "start": 8440.66, "end": 8452.54, "text": "potman run cute cute app ach so das hie\u00df ja irgendwie anders was wei\u00df ich"}, {"id": 1068, "start": 8462.54, "end": 8467.14, "text": "so ihr seht so ein container startet schnell ist mal exemplarisch lokal bei mir mit docker"}, {"id": 1069, "start": 8467.22, "end": 8473.18, "text": "der startet schnell aber es ist trotzdem eine halbe sekunde wo der container gestartet ist aber noch"}, {"id": 1070, "start": 8473.18, "end": 8477.98, "text": "nicht bereit ist requests anzunehmen und es k\u00f6nnte ja auch durchaus l\u00e4nger dauern ok keine frage"}, {"id": 1071, "start": 8477.98, "end": 8484.1, "text": "so und daf\u00fcr gibt es solche sachen wie ready probe und lifeboard oder life nes probe und"}, {"id": 1072, "start": 8484.1, "end": 8490.62, "text": "readiness probe das muss man das muss man abgucken wo das hin muss unter unter ports"}, {"id": 1073, "start": 8490.62, "end": 8496.66, "text": "muss unterhalb von ports also dahin muss das an der stelle"}, {"id": 1074, "start": 8511.66, "end": 8515.66, "text": "als wer cringe das kann ich dir nicht erz\u00e4hlen habe ich keine gro\u00dfe erfahrung drinne"}, {"id": 1075, "start": 8515.66, "end": 8524.9, "text": "ich w\u00fcrde mal sagen mit datenbank migrations aber du willst die postgres version updaten"}, {"id": 1076, "start": 8524.9, "end": 8529.26, "text": "oder was also willst du nicht deine datenbank intern die struktur updaten du willst die"}, {"id": 1077, "start": 8529.26, "end": 8538.1, "text": "datenbank version updaten oder wie das eigentlich erwarten dass deine anwendung sowohl mit der"}, {"id": 1078, "start": 8538.1, "end": 8543.5, "text": "alten als auch mit der neuen version funktioniert weil ansonsten ansonsten w\u00e4re das easy du"}, {"id": 1079, "start": 8543.5, "end": 8548.82, "text": "updatest zuerst die datenbank wenn du die datenbank version updaten willst du updatest zuerst die"}, {"id": 1080, "start": 8548.82, "end": 8556.46, "text": "datenbank dann die anwendung und dann ist gut das schema ok du willst so was wie eine datenbank"}, {"id": 1081, "start": 8556.46, "end": 8560.58, "text": "migration oder so haben das ist eine gute frage habe ich keine erfahrung mit wie man das gescheit"}, {"id": 1082, "start": 8560.58, "end": 8569.9, "text": "synchronisiert jetzt bestimmt irgendwelche super tollen best practice sachen f\u00fcr aber ich kann"}, {"id": 1083, "start": 8569.98, "end": 8576.46, "text": "da nichts sinnvolles zu sagen ich kann jetzt h\u00f6chstens was zusammen reiben was mir da so"}, {"id": 1084, "start": 8576.46, "end": 8586.9, "text": "auf dem stegreif einf\u00e4llt aber ich wenn du das schema updatest dann erweiterst du das schema"}, {"id": 1085, "start": 8586.9, "end": 8592.26, "text": "ja in der regel oder machst du das schema incompatibel also geht die alte anwendung"}, {"id": 1086, "start": 8592.26, "end": 8595.22, "text": "noch mit der neuen schema version oder nicht wahrscheinlich nicht"}, {"id": 1087, "start": 8600.38, "end": 8607.54, "text": "wenn das so wenn das so voneinander abh\u00e4ngt m\u00fcsste ich mir gedanken zu machen kann ich dir so"}, {"id": 1088, "start": 8607.54, "end": 8614.18, "text": "vielleicht an der chat ahnung wir haben hier viele 5 hat cloud native kubernetes 6 herz vielleicht"}, {"id": 1089, "start": 8614.18, "end": 8622.34, "text": "f\u00e4llt da jemand was zu ein wahrscheinlich kommen die leute jetzt wahrscheinlich kommen die leute"}, {"id": 1090, "start": 8622.34, "end": 8627.7, "text": "jetzt an und sagen ha ha null gar keine sql datenbanken mehr verwenden sondern einfach nur"}, {"id": 1091, "start": 8627.7, "end": 8637.86, "text": "noch cloud native data basis wobei das halt nicht realistisch ist das ist dann wenn du die"}, {"id": 1092, "start": 8637.86, "end": 8641.74, "text": "falschen leute was sie kommen dann immer mit so was so ja warum verwendest du auch nicht"}, {"id": 1093, "start": 8641.74, "end": 8647.14, "text": "klaut technologie in k\u00fcrze bescheuern wer definiert was klaut technologie sachen ist und was nicht"}, {"id": 1094, "start": 8647.14, "end": 8655.82, "text": "also das ist das ist was du hast zwerggr\u00fcnde ist ein sehr praktischer und realer anwendungsfall"}, {"id": 1095, "start": 8655.82, "end": 8662.22, "text": "und darauf gibt es bestimmt auch eine gute l\u00f6sung habe ich aber keine erfahrung mit kann"}, {"id": 1096, "start": 8662.22, "end": 8669.38, "text": "ich dir gerade nicht sagen zu also das naheliegendste ist nat\u00fcrlich du musst sicherstellen dass die alte"}, {"id": 1097, "start": 8669.38, "end": 8674.5, "text": "anwendung auch mit dem neuen schema funktioniert also dass du kannst das schema zwar erweitern aber"}, {"id": 1098, "start": 8674.5, "end": 8680.34, "text": "dass es nicht incompatibel zur alten anwendung machen dann funktioniert es dann ist easy dann"}, {"id": 1099, "start": 8680.34, "end": 8687.26, "text": "kannst du zuerst die datenbank updaten und dann die anwendung und wenn das nicht so ist f\u00e4llt"}, {"id": 1100, "start": 8687.26, "end": 8695.22, "text": "jetzt so spontan nicht so viel ein weil da m\u00fcsstest du ja sicherstellen dass du irgendwie so einen"}, {"id": 1101, "start": 8695.22, "end": 8702.42, "text": "rolling update machst so die datenbank updaten dann wobei es halt je nach datenbank auch so ist"}, {"id": 1102, "start": 8702.42, "end": 8707.82, "text": "dass du gar keine unterschiedlichen datenbank anspricht sondern alle datenbanken in dem cluster"}, {"id": 1103, "start": 8707.82, "end": 8714.18, "text": "\u00fcber halt ein connection string wenn du so willst dass du gar nicht explizit sagen kannst ich will"}, {"id": 1104, "start": 8714.18, "end": 8723.14, "text": "jetzt nur auf diese tja da gibt es bestimmt coole m\u00f6glichkeiten zu bis jetzt hatte ich das muss"}, {"id": 1105, "start": 8723.14, "end": 8726.3, "text": "ich mich mit diesem problem noch nicht besch\u00e4ftigen wenn es soweit ist sag ich dir bescheid"}, {"id": 1106, "start": 8726.3, "end": 8737.5, "text": "ich w\u00fcrde einfach sagen ist job der dba es k\u00fcmmert dich nicht drum dass sie das machen"}, {"id": 1107, "start": 8737.5, "end": 8740.38, "text": "sag einfach nicht mein problem easy"}, {"id": 1108, "start": 8747.06, "end": 8750.62, "text": "ja aber kommen wir mal zur container geschichte zur\u00fcck"}, {"id": 1109, "start": 8750.62, "end": 8760.82, "text": "damit ich mitbekomme wann meine anwendung wirklich gestartet ist gibt es readiness probes und es"}, {"id": 1110, "start": 8760.82, "end": 8771.9, "text": "gibt dieses autocomplete alter ist es schlecht man leifnis probe gibt es das eine ist wenn"}, {"id": 1111, "start": 8771.9, "end": 8776.26, "text": "der container ready also wenn der container gestartet ist zum ersten mal und das zweite"}, {"id": 1112, "start": 8776.26, "end": 8784.86, "text": "ist ob der container noch weiterhin funktioniert und ich copy paste mir das jetzt hieraus aus"}, {"id": 1113, "start": 8784.86, "end": 8791.78, "text": "dem beispiel weil ich keinen bock habe das nochmal alles zu tippen so sieht das ganze"}, {"id": 1114, "start": 8791.78, "end": 8798.7, "text": "dann aus aber gibt sogar eine hilfe dazu der guckt ob der ob der container ready ist und"}, {"id": 1115, "start": 8798.7, "end": 8803.94, "text": "der guckt ob der container noch am leben ist und wenn diese das ist der unterschied wenn"}, {"id": 1116, "start": 8803.98, "end": 8811.26, "text": "diese leifnis probe fehlschl\u00e4gt dann restartet kubernetes den container kann auch sehr praktisch"}, {"id": 1117, "start": 8811.26, "end": 8815.14, "text": "sein falls die anwendung nicht komplett krascht sondern intern blo\u00df in so einem"}, {"id": 1118, "start": 8815.14, "end": 8820.34, "text": "zustand ger\u00e4t dass sie nicht mehr richtig funktioniert also leifnis probe startet"}, {"id": 1119, "start": 8820.34, "end": 8826.42, "text": "kubernetes im zweifelsfall den container neu und hier kann unter anderem dieser ready check"}, {"id": 1120, "start": 8826.42, "end": 8831.86, "text": "entscheiden wann das denn in den load balancer aufgenommen wird es gibt auch startup probe ja"}, {"id": 1121, "start": 8831.86, "end": 8837.74, "text": "aber das braucht man in der regel nicht genau so und wenn ich das jetzt apply dann werdet ihr"}, {"id": 1122, "start": 8837.74, "end": 8846.02, "text": "vielleicht gleich was feststellen guckt mal das geht jetzt deutlich langsamer seht ihr das"}, {"id": 1123, "start": 8846.02, "end": 8856.98, "text": "das das geht jetzt deutlich langsamer alles und warum weil er halt immer f\u00fcnf sekunden wartet"}, {"id": 1124, "start": 8856.98, "end": 8862.22, "text": "bis das ding auch wirklich am leben ist also der wartet f\u00fcnf sekunden macht dann diesen health"}, {"id": 1125, "start": 8862.22, "end": 8867.86, "text": "check ob es am leben seht es dauert deutlich l\u00e4nger aber jetzt haben wir den gro\u00dfen vorteil"}, {"id": 1126, "start": 8867.86, "end": 8874.14, "text": "jetzt gehen keine requests mehr keine neuen requests mehr verloren also wenn ihr einen ganz"}, {"id": 1127, "start": 8874.14, "end": 8881.02, "text": "ung\u00fcnstigen moment erwischt dass ihr gerade weiter geleitet worden seid auf einen server und der"}, {"id": 1128, "start": 8881.02, "end": 8886.46, "text": "wird gerade weg gestartet oder so dann kann es sein dass man immer noch einen fehler kriegt aber"}, {"id": 1129, "start": 8886.46, "end": 8891.5, "text": "es verschwindet gering und das kann ich euch jetzt auch zeigen ich mache mal einen ich mache mal"}, {"id": 1130, "start": 8891.5, "end": 8896.86, "text": "den benchmark ok ich mache ich mache eine neue version wir gehen wieder zur\u00fcck zu cute cute"}, {"id": 1131, "start": 8896.86, "end": 8904.74, "text": "chatter ich mache ich mache eine neue version 1.0.2 von unserem container pusht das in die"}, {"id": 1132, "start": 8904.74, "end": 8910.22, "text": "registry update die container version und jetzt passt mal auf ich mache einen benchmark"}, {"id": 1133, "start": 8910.22, "end": 8918.74, "text": "hey haben wir waren 30 sekunden dann war mal l\u00e4nger eine minute aber eine minute eine minute"}, {"id": 1134, "start": 8918.74, "end": 8927.3, "text": "holle pulle requests auf diesen service ich fange jetzt an und jetzt mache ich das update apply"}, {"id": 1135, "start": 8927.3, "end": 8935.18, "text": "wir k\u00f6nnen wir k\u00f6nnen nebenbei nebenbei zugucken cute"}, {"id": 1136, "start": 8936.18, "end": 8942.18, "text": "pots ja ihr seht die pots werden neu gestartet und werden hinzugenommen und alles"}, {"id": 1137, "start": 8942.18, "end": 8951.94, "text": "latest funktioniert nicht apply l\u00e4uft immer nur dann wenn sich das manifest ge\u00e4ndert hat latest"}, {"id": 1138, "start": 8951.94, "end": 8962.1, "text": "funktioniert nicht es gibt tricks dass es mit latest funktioniert aber das stammt mich funktioniert"}, {"id": 1139, "start": 8962.1, "end": 8969.5, "text": "das mit latest nicht so unser rolling update ist fertig und jetzt bin ich mal gespannt ob"}, {"id": 1140, "start": 8969.5, "end": 8981.46, "text": "requests verloren gegangen sind das l\u00e4uft noch ok irgendwie das zum irgendwie spackt es rum wir"}, {"id": 1141, "start": 8981.46, "end": 8987.94, "text": "warten mal kurz die minute ab es sp\u00e4mt es sp\u00e4mt immer noch heftig so requests und guck mal wir"}, {"id": 1142, "start": 8987.94, "end": 8996.82, "text": "haben eine millionen requests gemacht und nur und jetzt halt euch mal von einer millionen requests"}, {"id": 1143, "start": 8996.82, "end": 9006.58, "text": "sind nur w\u00e4hrend diesem rolling update 39 oder 40 requests mal als nicht erreichbar zur\u00fcckgekommen"}, {"id": 1144, "start": 9006.58, "end": 9013.78, "text": "von einer millionen request 40 und wir haben unsere anwendung geupdatet im hintergrund ich"}, {"id": 1145, "start": 9013.78, "end": 9021.14, "text": "wei\u00df wir hatten mal so einen krassen manager dude der gesagt h\u00e4tte jetzt 40 requests dann sollten"}, {"id": 1146, "start": 9021.14, "end": 9025.3, "text": "wir uns jetzt mal auf die konzentrieren die nicht funktioniert haben haben wir vielleicht logs wo"}, {"id": 1147, "start": 9025.3, "end": 9031.3, "text": "wir die bangen k\u00f6nnen warum diese requests fehlgeschlagen sind aber ich glaube das ist"}, {"id": 1148, "start": 9031.3, "end": 9038.46, "text": "eine ganz gute ausbeute von einer millionen requests 40 requests fehlgeschlagen und das"}, {"id": 1149, "start": 9038.46, "end": 9047.62, "text": "w\u00e4hrend wir unsere anwendung geupdatet haben also das ist schwierig hinzukriegen anderweitig"}, {"id": 1150, "start": 9068.46, "end": 9082.02, "text": "h\u00e4ttest du 100 prozent wenn du kein crawling ab du meinst in rolling update machen w\u00fcrdest"}, {"id": 1151, "start": 9082.02, "end": 9091.1, "text": "die rolling update ist das beste was man machen kann du hast als du hast als auswahl sollst du"}, {"id": 1152, "start": 9091.1, "end": 9100.66, "text": "recreate ich bin den haus gebrauche ist das nix es gibt so verr\u00fcckte leute auf youtube die"}, {"id": 1153, "start": 9100.66, "end": 9106.98, "text": "haben drei kubernetes cluster daheim ist unsinnig und was man an der stelle auch noch mal dazu sagen"}, {"id": 1154, "start": 9106.98, "end": 9111.78, "text": "muss wir haben jetzt ja ihren kubernetes cluster selbst erstellt und verwalten den kubernetes"}, {"id": 1155, "start": 9111.78, "end": 9121.5, "text": "cluster selbst normalerweise musst du dich mit so grundlegenden sachen wie skalierung und so gar"}, {"id": 1156, "start": 9121.5, "end": 9125.86, "text": "nicht gro\u00dfartig auseinandersetzen wenn du ein managed cluster also nicht einfach nur ein managed"}, {"id": 1157, "start": 9125.86, "end": 9130.9, "text": "cluster sondern so wenn du den autopilot cluster von google beispielsweise wenn es da brauchst du"}, {"id": 1158, "start": 9130.9, "end": 9136.1, "text": "nur noch deployments machen damit m\u00fcssen wir uns auch noch besch\u00e4ftigen die n\u00e4chsten tage"}, {"id": 1159, "start": 9136.1, "end": 9144.38, "text": "oder versions updates versions updates von kubernetes und solche geschichte da musste"}, {"id": 1160, "start": 9144.38, "end": 9148.7, "text": "dich nicht mit besch\u00e4ftigen wenn du managed cluster verwenden also vieles von diesen basic"}, {"id": 1161, "start": 9148.7, "end": 9156.14, "text": "sachen musst du gar nicht selbst machen zumal ich meine wenn du den benchmark jetzt ausf\u00fchrst sind"}, {"id": 1162, "start": 9156.14, "end": 9162.98, "text": "dann alle 100 prozent kann schauen wir mal ich lasse noch mal laufen kann durchaus sein dass die"}, {"id": 1163, "start": 9163.3, "end": 9172.02, "text": "meine cpu ist ja auch voll am anschlag dass die ein oder anderen requests einfach nicht nicht"}, {"id": 1164, "start": 9172.02, "end": 9178.86, "text": "richtig durchgehen also managed cluster k\u00fcmmern sich um versions updates von kubernetes und so"}, {"id": 1165, "start": 9178.86, "end": 9183.54, "text": "was schon da hat man viel arbeit nicht mehr was aber eigentlich auch ganz gut ist weil ganz ehrlich"}, {"id": 1166, "start": 9183.54, "end": 9189.46, "text": "da h\u00e4ngt halt sehr viel so lowlevel kram dran wo es schwierig ist das alles das alles richtig"}, {"id": 1167, "start": 9189.46, "end": 9196.94, "text": "zu machen und was auch ein gewisses tiefgreifendes know-how erfordert alle staub"}, {"id": 1168, "start": 9196.94, "end": 9205.34, "text": "ja das waren die control set ich habe kein control set gemacht ich habe terminal auf und zu gemacht"}, {"id": 1169, "start": 9219.82, "end": 9225.9, "text": "ja jetzt habe ich eine millionen responses und zwar alles 200 ja also wir hatten tats\u00e4chlich"}, {"id": 1170, "start": 9225.9, "end": 9234.82, "text": "40 requests verlust in unserem upgrade gibt schlimmeres oder von eine millionen requests"}, {"id": 1171, "start": 9234.82, "end": 9242.9, "text": "ok chat wie hat es euch bis jetzt gefallen kubernetes stells"}, {"id": 1172, "start": 9250.46, "end": 9256.1, "text": "wie gesagt ich bin ich ich bin selbst nicht ich bin ja selbst nicht das super super"}, {"id": 1173, "start": 9256.1, "end": 9263.82, "text": "oberkombiniertes checker aber ich glaube so mit lokaler registry und so was hier das war"}, {"id": 1174, "start": 9263.82, "end": 9269.78, "text": "schon ganz cool dann lasst uns doch jetzt noch mal kurz eine viertelstunde \u00fcberlegen was wir"}, {"id": 1175, "start": 9269.78, "end": 9283.34, "text": "als n\u00e4chstes machen also als n\u00e4chstes eine sache die mir eingefallen ist ja environment"}, {"id": 1176, "start": 9283.34, "end": 9297.54, "text": "variablen genau n so volume secrets cert manager das will ich unbedingt zeichen mit"}, {"id": 1177, "start": 9297.54, "end": 9308.1, "text": "let's encrypt lokal dns challenge na service mesh das ist jetzt schon ein"}, {"id": 1178, "start": 9308.1, "end": 9335.18, "text": "bisschen zu etwas dann vielleicht irgendwann noch mal ja und ends backends traffic gibt"}, {"id": 1179, "start": 9335.18, "end": 9338.62, "text": "naja bisschen ja ok"}, {"id": 1180, "start": 9338.62, "end": 9348.38, "text": "geht aber ich finde den namen so dass es da hat man eigentlich was recht logisches also was"}, {"id": 1181, "start": 9348.38, "end": 9353.86, "text": "naheliegendes so nach dem motto man checkt seine config in geht ein wieder super cool benannt"}, {"id": 1182, "start": 9353.86, "end": 9361.46, "text": "dazu kommt da nat\u00fcrlich aber auch noch so dinger wie automatische tests und also man k\u00f6nnte man"}, {"id": 1183, "start": 9361.78, "end": 9366.5, "text": "zum beispiel sogar solche dinger machen wie man ganz abgedreht weiterspinnen man k\u00f6nnte ja zum"}, {"id": 1184, "start": 9366.5, "end": 9372.1, "text": "beispiel sagen ok man hat noch eine test suite f\u00fcr diese test suite f\u00fcrs jetzt super krasse"}, {"id": 1185, "start": 9372.1, "end": 9382.82, "text": "dort net anwendung und man l\u00e4sst es dann irgendwie automatisch noch bilden und l\u00e4sst die tests"}, {"id": 1186, "start": 9382.82, "end": 9389.38, "text": "durchlaufen und l\u00e4sst sich dinger machen dann automatisch noch auf irgendeinem ce server das"}, {"id": 1187, "start": 9389.38, "end": 9396.94, "text": "image bauen dass man das nicht lokal machen muss so was in der richtung gibt ja gibt ja viele"}, {"id": 1188, "start": 9396.94, "end": 9401.86, "text": "viele dinger man kann ganz abgedrehte sachen machen ja also irgendwann was wir uns auf jeden"}, {"id": 1189, "start": 9401.86, "end": 9406.42, "text": "fall noch angucken ist das finde ich pers\u00f6nlich eine coole sache weil du damit weil du hiermit"}, {"id": 1190, "start": 9406.42, "end": 9413.98, "text": "kannst du solche manifest erstellen in pyson zum beispiel finde ich pers\u00f6nlich eine super"}, {"id": 1191, "start": 9413.98, "end": 9423.54, "text": "geschichte ist eigentlich overkill und komplett unsinnig f\u00fcr vieles aber ich pers\u00f6nlich finde es"}, {"id": 1192, "start": 9423.54, "end": 9434.38, "text": "sehr nice weil man halt zum beispiel so sachen definieren kann guck hier hier hier kann man zum"}, {"id": 1193, "start": 9434.38, "end": 9440.1, "text": "beispiel sagen hier cube service load balancer cube deployment also das was wir eben im manifest"}, {"id": 1194, "start": 9440.1, "end": 9445.42, "text": "definiert haben kann man hier drin der ja ich wei\u00df man kann auch hellen und so was verwenden"}, {"id": 1195, "start": 9445.42, "end": 9449.94, "text": "ich pers\u00f6nlich w\u00fcrde dann aber fast sagen mir pers\u00f6nlich ist das hier finde ich den ansatz hier"}, {"id": 1196, "start": 9449.94, "end": 9457.54, "text": "cooler weil man dann vollst\u00e4ndige programmier sprache und ich mache halt lieber das im source"}, {"id": 1197, "start": 9457.54, "end": 9465.62, "text": "code als irgendwelche jamme files und description files und so bauen also das ist cool ja hellen"}, {"id": 1198, "start": 9465.62, "end": 9470.86, "text": "m\u00fcssen m\u00fcssen wir uns aber der vollst\u00e4ndigkeit halber auf jeden fall angucken weil fast jedes"}, {"id": 1199, "start": 9470.86, "end": 9478.74, "text": "zweite tutorial hei\u00dft ja in hellen repo enden und ausf\u00fchren zum beispiel secret manager secret"}, {"id": 1200, "start": 9478.74, "end": 9488.38, "text": "manager sagt das auch customize habe ich keine ahnung von kein plan vielleicht k\u00f6nnen wir uns"}, {"id": 1201, "start": 9488.38, "end": 9492.46, "text": "irgendwann auch mal terraform noch in kombination mit dem kubernetes provider angucken habe ich"}, {"id": 1202, "start": 9492.46, "end": 9506.74, "text": "jetzt auch noch nicht gemacht dann gucken wir uns nat\u00fcrlich noch google cloud managed kubernetes"}, {"id": 1203, "start": 9506.74, "end": 9513.94, "text": "und auto autopilot an das ist der eigentliche grund warum ich mich in den letzten tagen wieder"}, {"id": 1204, "start": 9513.94, "end": 9518.26, "text": "ein bisschen mehr damit besch\u00e4ftigt habe weil ich wei\u00df dass bei meinem neuen arbeitgeber google"}, {"id": 1205, "start": 9518.26, "end": 9522.3, "text": "cloud mit wahrscheinlich managed kubernetes oder autopilot cluster vielleicht darf sogar ich"}, {"id": 1206, "start": 9522.3, "end": 9525.5, "text": "mir aussuchen was ich haben will einsetzen wer das hei\u00dft da muss ich ein bisschen gucken"}, {"id": 1207, "start": 9525.5, "end": 9535.66, "text": "metal lb ich habe keine ahnung was das ist ich kenne kong kong ist auch so ein"}, {"id": 1208, "start": 9535.66, "end": 9539.66, "text": "lautballon kein lautballon so ein api gateway ist das harambe"}, {"id": 1209, "start": 9545.18, "end": 9550.02, "text": "ich habe keine ahnung was es mir sagen soll ehrlich gesagt netwerk lautballon"}, {"id": 1210, "start": 9550.9, "end": 9559.66, "text": "was ja selbstverst\u00e4ndlich da habe ich ja ganz vergessen das m\u00fcssen wir noch mal hier"}, {"id": 1211, "start": 9559.66, "end": 9572.22, "text": "matrix ja matrix auch nat\u00fcrlich da m\u00fcssen wir uns noch sidecar container angucken f\u00fcr so"}, {"id": 1212, "start": 9572.22, "end": 9581.66, "text": "monitoring und logs also ihr seht schon die einfache die bild in kubernetes sachen die"}, {"id": 1213, "start": 9581.66, "end": 9589.86, "text": "sind an sich recht easy ja kommt man recht schnell mit aber das ist ein loch ohne boden was da alles"}, {"id": 1214, "start": 9589.86, "end": 9595.06, "text": "dran h\u00e4ngt dann ja volumes volumes ja haben wir ja schon"}, {"id": 1215, "start": 9603.22, "end": 9610.7, "text": "und das was da dran ist wirklich ein fast ohne boden so bild in sachen den kubernetes jammel"}, {"id": 1216, "start": 9610.7, "end": 9615.74, "text": "eintragen apply ja kein ding aber dieses ganze zeug was hier noch dabei kommt das ist halt"}, {"id": 1217, "start": 9615.74, "end": 9624.22, "text": "extrem viel was es da gibt ja autodiscovery ja das sollten wir uns auch mal angucken ja ja genau"}, {"id": 1218, "start": 9624.22, "end": 9633.34, "text": "ne traffic prominent engine ich bin ich bin team engine ich habe traffic selbst auch schon"}, {"id": 1219, "start": 9633.34, "end": 9643.34, "text": "verwendet ich muss sagen ganz im ernst ich bleibe bei engine ich benutze kadi wenn super simpel"}, {"id": 1220, "start": 9643.34, "end": 9648.82, "text": "sein muss traffic von hand konfigurieren ist eh abfakt und traffic macht halt in so einem"}, {"id": 1221, "start": 9648.82, "end": 9653.34, "text": "umfeld hier schon irgendwie sinn weil es auch viele service discovery funktionen ganz ehrlich"}, {"id": 1222, "start": 9653.38, "end": 9658.7, "text": "aber es ist so easy in engine x ingress zu machen warum soll ich ja traffic verwenden"}, {"id": 1223, "start": 9658.7, "end": 9664.46, "text": "glaubt glaube ich einen bogen erst mal drum machen"}, {"id": 1224, "start": 9671.14, "end": 9671.46, "text": "ja"}, {"id": 1225, "start": 9671.46, "end": 9677.1, "text": "schreiben wir nochmal"}, {"id": 1226, "start": 9677.1, "end": 9688.3, "text": "dass wir uns das dann auch grafisch angucken k\u00f6nnen dahinter wenn euch noch was einf\u00e4llt"}, {"id": 1227, "start": 9688.3, "end": 9692.54, "text": "k\u00f6nnt ihr ruhig auch im n\u00e4chsten stream schreiben da k\u00f6nnen wir uns das noch angucken also wir"}, {"id": 1228, "start": 9692.54, "end": 9696.3, "text": "machen jetzt auf jeden fall erst mal weiter mit zert manager und environments und volumes und so"}, {"id": 1229, "start": 9696.3, "end": 9697.3, "text": "was im n\u00e4chsten stream ist"}, {"id": 1230, "start": 9697.3, "end": 9709.66, "text": "ach du schei\u00dfe das ist nicht nice so nice jetzt f\u00e4ngt dieses wortling schei\u00dfe schon in deutschland"}, {"id": 1231, "start": 9709.66, "end": 9720.1, "text": "an oder was hat er seine adresse geleakt oder oder was war da mega f\u00fcr ein arsch man"}, {"id": 1232, "start": 9727.3, "end": 9738.26, "text": "ok leute ich wann machst du tiktok sein ja ich macht ich werd krasser kubernetes cloud native"}, {"id": 1233, "start": 9738.26, "end": 9744.42, "text": "influencer auf auf tiktok leute es ist fast 22 ich muss jetzt eh mal ins bett au\u00dferdem muss ich"}, {"id": 1234, "start": 9744.42, "end": 9753.54, "text": "mal ganz dringend kacken also das wird eine serie ja nicht unbedingt am st\u00fcck nicht unbedingt am"}, {"id": 1235, "start": 9753.58, "end": 9761.26, "text": "st\u00fcck jetzt kubernetes jeden stream bis wir mit allem durch den immer mal wieder machen wir machen"}, {"id": 1236, "start": 9761.26, "end": 9767.78, "text": "wir ja ok leute wir sehen uns dann bis denn zu"}]}