Das war's für heute.
Bis zum nächsten Mal.
Tschüss!
Das war's für heute.
Bis zum nächsten Mal.
Tschüss!
Das war's für heute.
Bis zum nächsten Mal.
Tschüss!
Das war's für heute.
Bis zum nächsten Mal.
Tschüss!
Das war's für heute.
Bis zum nächsten Mal.
Tschüss!
Das war's für heute.
Bis zum nächsten Mal.
Tschüss!
Bis zum nächsten Mal.
Tschüss!
Das war's für heute.
Bis zum nächsten Mal.
Tschüss!
Das war's für heute.
Bis zum nächsten Mal.
Tschüss!
Das war's für heute.
Bis zum nächsten Mal.
Tschüss!
Tschüss!
Das war's für heute.
Bis zum nächsten Mal.
Tschüss!
Das war's für heute.
Bis zum nächsten Mal.
Tschüss!
Tschüss!
Tschüss!
Tschüss!
Tschüss!
Tschüss!
Tschüss!
Tschüss!
Tschüss!
Tschüss!
Tschüss!
Oh, nice!
Pfeifert, ich bin nicht ausgelockt aus Google Cloud.
Das ist schon mal gut.
Da muss ich mich nämlich nicht wieder einloggen.
Unown Error.
Kekkel. Kekkeltest 123.
Perfekt.
Also.
Ich mach nochmal das Intro, von dem was ich gestern gemacht habe.
Also wir haben ja vor zwei Tagen
ein bisschen rumgebastelt mit Google Cloud
und Terraform. Ich erzähl euch ja gleich nochmal kurz,
was Terraform ist.
Wollen wir da rumkonfigurieren.
Und da war ja ein bisschen das Problem,
dass ich selbst, mir ist doch nicht so
genau im Vorfeld angeguckt hab,
wie man den Terraform Google Cloud Provider
benutzt. Also sprich, wir haben mit Terraform
Google Cloud provisioniert.
Und ich hab mir das jetzt ein bisschen angeguckt.
Und heute zeig ich euch, wie man das richtig
macht. Damit ich nicht
alle Lorbeeren selbst ernte
hier, ich hab mir ein YouTube-Tutorial
angeguckt.
Von irgendeinem Anton.
Das hier.
Hab ich mir angeguckt
und hab mich davon ein bisschen inspiriert.
Also wir machen es allerdings nicht genau so,
wie er das macht.
Ich werd die Terraform Datei ein bisschen
anders benennen und auch ein bisschen anderen Inhalt
reinschreiben. Aber das war ein sehr gutes
Tutorial für Terraform, Kupaletes
Cluster und so ein bisschen Basic Usage.
Von Terraform plus Google Cloud.
Weil es ist doch schon
anders als Azure, muss man sagen.
Terraform, jetzt nochmal zur Erklärung,
was das Ganze ist. Bei der Erklärung selbst
von denen ihrer Seite blickt man ja nicht wirklich
durch, was die einem sagen wollen.
Was ist Terraform?
Terraform
ist ein
Tool, mit dem man
sich Cloud-Infrastruktur
provisioniert.
Kann man sich jetzt vielleicht
ein bisschen schwierig
was drunter vorstellen. Ganz
praktisches Beispiel,
wenn ihr bei irgendeinem Cloud-Anbieter,
sei es bei Hetzner, sei es bei
Digital Ocean, sei es bei Google Cloud.
Das ist eigentlich vollkommen egal.
Wenn ihr dort beispielsweise VMs
installieren wollt, Netzwerke anlegen
wollt, bei irgendeinem Cloud-Provider,
dann könnt ihr das entweder von Hand
im Web-Interface machen, was
halt nicht reproduzierbar ist. Das müsst ihr dann jedes
Mal machen und dürft keine Zwischenschritte vergessen
und sowas. Oder
ihr macht das mit Terraform.
Terraform ist quasi ein Cloud-
übergreifendes Tool,
wo man Cloud-Ressourcen anlegen
kann, Cloud-Infrastruktur anlegen
kann, sodass es reproduzierbar
ist und man nicht alles
immer wieder von Hand
machen muss. Terraform hat noch ein weiteres
cooles Feature, wobei das manchmal auch
Probleme...
Ist Terraform der große Bruder von Ansible?
Nee, sind zwei unterschiedliche Tools.
Kann ich aber auch noch gleich was zu sagen,
weil das gerne gefragt wird.
Also, um das jetzt
hier nochmal abzuschließen,
Terraform hat noch ein weiteres sehr nice
Feature, was wir vielleicht auch gleich sehen
werden. Und zwar
Terraform merkt sich,
welche Infrastruktur man damit angelegt
hat und man kann damit die Infrastruktur auch wieder
löschen. Also sprich, wenn ihr Terraform
benutzt, um bei Hetzner in der Cloud
10 VMs zu erzeugen
und ihr wollt die dann irgendwann wieder
löschen, dann könnt ihr einfach sagen Terraform
Destroy, das ist das Gegenteil
von Terraform Apply,
dann guckt es in seinen State rein und sieht
aha, ich habe vorher diese und diese
VMs aufgesetzt, das sind heißt
diese VMs mit diesen internen IDs und
sowas, die muss ich jetzt auch wieder löschen.
Und wenn jemand von Hand
sich einloggt und dann in der Cloud an den
VMs rumspielt und ihr Terraform
wieder ausführt, dann merkt Terraform, dass da einer
was von Hand geändert hat und bietet das
anders wieder rückgängig zu machen. Also
es ist ein sehr nützliches Tool.
Es gibt noch mehr Tools
in dieser Richtung. Terraform ist nicht das
einzige, sollte man sagen.
Auch sehr beliebt
ist Pulumi. Das
ist
von den Features her ungefähr
gleich mit Terraform.
Allerdings
man
benutzt es mit
einer
Handelsüblichen normalen
Programmiersprache. Also in Terraform
definiert man seine Ressourcen so.
In so einer komischen, das ist kein
Jaml, das ist HCL nennt sich, das sieht
ein bisschen aus wie Jaml vielleicht.
In so einer extra
Definitionssprache macht man das
in Terraform. Und in Pulumi
macht man das zum Beispiel in
C Sharp
oder in, machen wir hier mal
Getting Started, gucken wir uns mal so was an.
Ja, in Pulumi macht man
das zum Beispiel da.
Es wäre natürlich sehr nice, wenn man
ein schönes Beispiel hätte.
Ja gut, genau hier, hier sieht man es.
In Pulumi würde man das
gleiche dann machen, bloß dass man das zum Beispiel
in JavaScript, Python, Go, C Sharp
oder halt auch in so einem
Jaml Dialekt machen kann.
Ist deutlich vielseitiger
Pulumi als Terraform, dadurch dass es eben
ein Framework
ist für
die handelsüblichen Programmiersprachen, die es gibt.
Also wenn man ganz abgedrehte Sachen
machen will, ist das wahrscheinlich mit Pulumi einfacher,
dafür ist der Einstieg auch komplizierter.
Terraform hat das auch erkannt, wir machen jetzt
mal so ne Grundumüberblick.
Terraform hat das auch erkannt und hat
CDKTF ins Leben gerufen.
Das ist Terraform,
allerdings nicht mehr in ihrem
komischen eigenen
Dialekt hier, in ihrem eigenen Jaml
Dialekt, sondern Terraform
auch in handelsüblichen Programmiersprachen.
Ist auch ne coole Geschichte, da hab ich selbst noch nie
benutzt. Haben wir bei uns im ToDo
drinne stehen, werden wir irgendwann demnächst mal ausprobieren.
Das Ganze, egal
jetzt ob Terraform oder Pulumi,
nennt man
Neudeutsch Info,
bei Neudeutsch ist vielleicht ein bisschen verkehrt.
Wenn man
hips sein will,
auch auf Neuenglisch nennt
man das Ganze Infrastructure as
Code. Warum?
Man sieht es ja hier schon, man beschreibt
nur noch, was in der Cloud angelegt
werden soll, z.B. so ein Netzwerk
hier mit dieser IP Range
und Terraform übersetzt
das dann in die passenden Cloud Ressourcen.
Man muss die Cloud Ressourcen nicht selbst
anlegen. Ich könnte auch
bei Google in das Web Interface gehen
in Netzwerke
und dann sagen
Create
Network.
Könnte ich auch machen, ich könnte es aber auch
Terraform machen lassen.
D.h. dementsprechend
Infrastructure as Code, man schreibt,
beschreibt halt, was man haben will.
In Source Code Dateien und Terraform
Applied das dann, das nennen die
auch so, auf die jeweilige Cloud Umgebung.
Man könnte das auch für Azure machen oder
für Digital Ocean oder für
AWS oder sowas. Wobei man
auch sagen muss, Terraform
ist keine
Abstraktionsschicht
zwischen den Cloud Providern.
Also man definiert in Terraform
jetzt nicht ein Netzwerk oder
eine VM oder ein Kubernetes Cluster
jedes Mal gleich, egal
ob für Azure oder für Google. Also
man muss schon die Cloud
spezifischen Sachen machen, nur dass man es halt
in Terraform reproduzierbar machen kann und
das ist das, wofür man das verwendet.
Dann sollte man noch eine Sache erwähnen.
Wie gesagt, letztendlich unter der Haube ist es
eigentlich ziemlich wurscht, was man verwendet, ob
es jetzt Terraform ist oder Polumi ist
oder hier das neue
Terraform in Programmiersprachen
verwendet. Wichtig ist, dass das Team
damit klarkommt,
wofür man das benutzt bzw.
in dem man ist oder für die man das erstellt.
Anwendungsentwickler selbst mögen
wahrscheinlich ein bisschen lieber Polumi,
wohingegen die klassischen
Teams wahrscheinlich eher
die Terraform Geschichte
mögen. Man muss
sagen, Terraform ist das deutlich
verbreitere Tool versus Polumi.
Also Terraform ist viel
verbreit, Terraform ist quasi so der
wie nennt man das so schön
so der Industriestandard,
falls es da sowas gibt. Also
Terraform ist das bekanntere und das
verbreitere von beiden Tools. Und wenn
ihr Jobs sucht, ist Terraform
Knowledge häufiger
gerne gesehen als
Polumi Knowledge, wobei man halt auch
sagen muss, wenn man Polumi kann, kommt man
relativ schnell in Terraform rein und umgedreht.
Genau, so viel zum Thema Infrastructure
as Code. Jetzt mal zu der eigentlichen Frage
wie das mit Ansible aussieht.
Terraform und Ansible sind
zwei unterschiedliche Tools.
Und zwar Ansible
ist ein Tool, um
Server zu konfigurieren.
Und Terraform ist ein Tool, um Infrastruktur
anzulegen.
Kann man sich quasi so vorstellen,
mit Terraform erzeuge ich meine Cloud
Ressourcen, unter anderem auch VMs.
Und mit Ansible konfiguriere
ich die VMs dann, wenn es notwendig
ist. Der Trend geht ja dahin,
dass Cloud Ressourcen
möglichst konfigurationsfrei
sein sollten, so möglichst
immutable sein sollen, was natürlich Schwachsinn ist in der
Realität, tritt das auch nicht ein.
Aber so
geht zumindest der Trend
hin, was ja auch sinnvoll ist.
Also wenn man jetzt beispielsweise 20
virtuelle Maschinen in der Cloud
erzeugen will, dann würde man das mit
Terraform erzeugen. Und wenn die noch
etwas spezielle, kompliziertere
Konfiguration brauchen, dann
würde ich das mit Terraform
konfigurieren.
Mit Ansible konfigurieren, nicht mit Terraform.
Mit Terraform professionieren, mit Ansible konfigurieren.
Die Tools haben gewisse Überschneidungen.
Man kann teilweise Cloudinfrastruktur
auch mit Ansible anlegen. Man kann
teilweise ein bisschen so mit Inline
Bash Scripts und sowas auch mit Terraform
dann Sachen
konfigurieren. Sagen wir mal, so
10-20% überschneiden sich vielleicht.
Aber grundsätzlich sind das eigentlich schon
ziemlich unterschiedliche Tools.
Terraform ist zum Anlegen, zum
provisionieren und Ansible ist zum
konfigurieren. So, das ist so der grundlegende
Unterschied. Das heißt, wenn man
Cloudinfrastruktur mit Terraform anlegt,
ist sie noch nicht unbedingt in dem Zustand,
wie man das haben will, zum Beispiel auf VMs
und da, wenn man es braucht, kommt
dann sowas wie Ansible ins Spiel.
Also, beides sehr nützliche Tools und
auch sowohl Terraform als auch
Ansible recht gefragt.
Wenn man sich mal so
ja, so
Skillanforderungen anguckt,
die Arbeitnehmer
aktuell, die Arbeitgeber aktuell
suchen,
ist sowohl Ansible als auch Terraform ganz gut dabei.
Pulumi, was Infrastructure
as Code angeht, ein bisschen weniger,
wobei ich persönlich, wenn ich für mich
privat machen würde, ja, ich
persönlich mag Pulumi mehr als
Terraform. Also Geschmackssache und
CDKTF habe ich euch ausprobiert.
So, soviel mal zu
ganz schnellen Übersicht, was Infrastructure
as Code angeht, mag jetzt vielleicht ein bisschen
abstrakt sein, wir gucken uns das gleich an.
Ich erzähle euch noch was dazu.
Ein Tool will ich
aber jetzt an der Stelle auch noch erwähnen,
wenn wir gerade dabei sind, und zwar ist das
Crossplane. Das ist gerade
im Kubernetes, ich, aber da lasse
ich mich jetzt nicht
großartig drüber aus.
Gerade im Kubernetes Umfeld sehe ich
da in sowas in der Richtung die Zukunft.
Also sprich,
Crossplane ist auch ein Tool,
nur ganz zwei Sätze zu, weil ich den Ansatz
wirklich sehr, sehr nice finde und ich hoffe,
dass ich damit auch mal
im Business Umfeld was machen kann.
Wenn man eh
plant, seine Sachen auf dem Kubernetes
Cluster zu deployen, dann kann man
ja auch theoretisch die Konfiguration anstatt
über Terraform auch
in Kubernetes Config erstellen.
Also da brauche ich keine Terraform Files
und keine Kubernetes Yammels, sondern ich kann
für alles Kubernetes
Yammels machen.
Und
Crossplane bietet dafür die Möglichkeit zum
Spiel, nur mal so jetzt,
was man damit cooles machen kann. Wir gucken
uns das heute nicht im Stream an. Es kann doch
sein, dass ich eine Anwendung habe, die ich
auf dem Kubernetes Cluster deployen will, die
allerdings nicht rein
aus Kubernetes Ressourcen besteht. Zum
Beispiel, ich habe ein Kubernetes Deployment,
was im Prinzip ein Container Image ist
mit der Anwendung drin. Das braucht aber
zusätzlich noch eine Datenbank, die nicht
im Kubernetes Cluster laufen soll.
Zum Beispiel hier Cloud SQL
bei Google.
Wie verbinde ich das Ganze jetzt zusammen?
Das ist so mit der klassischen Infrastructure
SS Code
nicht so ohne Weiteres möglich. Du brauchst
einmal ein Kubernetes Yammel Deployment
und du brauchst irgendwas, was deine
Cloud SQL Datenbank
professioniert.
Also du müsstest dann quasi
Terraform und
dein Kubernetes Teuch irgendwie
verbinden. Geht,
ja Flux kann das ein bisschen, kann das
mit der Cloud SQL
Terraform Controller.
Aber es ist nicht so schön.
Das ist das, wo Crossplane dazu kommt.
Crossplane kann das. Du kannst
quasi dann ein Deployment konfigurieren
für Kubernetes und gleichzeitig
allerdings auch im gleichen
Config-File oder im gleichen Kontext
auch noch sagen, ich möchte bei Google
eine Datenbank provisionieren.
Ich sehe da persönlich die Zukunft
im Kubernetes Umfeld eindeutig bei
Crossplane, weniger
bei Terraform. Terraform wird immer noch
wichtig bleiben,
allein schon zum initialen Aufbau und
ganzer Cloud Umgebung. Das wird man ja mit
Crossplane auch nicht machen.
Aber gerade das Handling von Anwendungen
und Deployments in Kubernetes,
was externe Ressourcen
zusätzlich braucht, da sehe ich die Zukunft
bei Crossplane. Ich habe mir das
in den letzten paar Wochen über schon ein bisschen
angeguckt, noch nie großartig was mitgemacht,
aber das sieht wirklich gut aus.
Hat noch ein paar Nachteile versus Terraform,
aber ich persönlich denke, da geht
die Zukunft hin, was
Kubernetes Deployments angeht.
Aber das gucken wir uns heute nicht an. Heute machen wir
den Klassiker Terraform,
den man auch auf absehbare
Zeit immer noch brauchen wird.
Also Crossplane wird nie Terraform ersetzen,
sondern eher im Kubernetes Umfeld
ein bisschen die Sachen übernehmen.
Aber irgendwann muss man ja mal initial den Cluster
hinstellen. Ohne Kubernetes Cluster kannst du ja auch
darauf nichts ausführen. Und dafür wird es
nach wie vor für immer auf
absehbare Zeit zumindest wahrscheinlich Terraform
oder Pulumi oder sowas in der Richtung bleiben.
Das heißt, Terraform ist ein sehr wichtiger
Skill, wenn man im
DevOps-Bereich,
im Server-Administrationsbereich,
aber auch im Entwicklungsbereich unterwegs ist.
Terraform ist aktuell
überall.
So.
Soviel
zur Einleitung.
Aber ich bin gerade ein bisschen aus den Fingern
gesaugt. Aber, Jett, ich glaube, man hat alles so weit
verstanden, oder? Ich verstehe, dass viele den Stream
auf der Arbeit nebenbei laufen haben und nicht so viel
schreiben können, Jett. Aber ich glaube,
konnte mir, konnte, konnte man halbwegs,
halbwegs holen. Nee, für auf absehbare
Zeit, ja. Für immer ist
nix. Gerade, gerade in der IT.
Aber Terraform hat sich so etabliert,
dass ich nicht sehe, dass irgendwas
großartig Terraform in nächster Zeit
den Platz einschreitig
machen wird. Dahinter ist ein bisschen Pulumi.
Und alles andere
spielt eigentlich kaum eine
größere Rolle.
Kann ein Terraform mit Ansible zusammenarbeiten?
Ja, kannst du.
Ja, kannst du machen.
Okay, also die Host-Datei,
die kannst du auch mit irgendeinem Inline-Bash-
Script in Terraform
erstellen.
Also ich würde nicht, ich würde nicht
Ansible zusätzlich noch
an Start bringen, nur um eine Host-Datei
zu erstellen.
Auf VMs.
Aber du kannst selbstverständlich Terraform
und Ansible zusammen ausführen.
Gibt's ja mehrere Möglichkeiten,
wie du, wie du das machen kannst.
Also es gibt ja bei Ansible, dass du das
pullen kannst aus dem Git-Repo.
Oder aber auch von einem zentralen Ding.
Quasi pushen kannst auf die Kisten zum
ausführen. Du könntest quasi zum Beispiel
am Ende von einer,
von einem Terraform-Run,
der dir VMs anlegt,
könntest du in dem Cloud-Init-File
anstoßen, dass Ansible ausgeführt wird.
Das geht. Das ist sogar sehr sinnvoll zusammen
zu verbinden, wenn man komplexere
Konfigurationsaufgaben für VMs hat.
Also wenn du wirklich öfters
VMs anlegst mit Terraform,
die dann auch irgendwie
ein bisschen komplizierter eingerichtet werden müssen,
als einfach nur Standard-Image drauf,
dann ist das sogar wirklich sinnvoll,
Terraform und Ansible zu kombinieren.
Dann kann Ansible die Kiste konfigurieren,
Terraform hat sie vorher angelegt.
Kann ich mit Terraform standardisierte
AD-Gruppen anlegen? Hab ich noch nie gemacht,
weil ich mit WinRows nichts zu tun habe,
aber ich bin mir ziemlich sicher, dass das geht.
Original scharf,
Dankeschön für den Sub.
So, so sieht's aus.
Und ich zeige euch heute, wie man das ordentlich macht
in Terraform für Google Cloud.
Wir werden heute Folgendes machen.
Moment, ich muss mal kurz was im Chat
noch beantworten.
Wie kommuniziert Terraform mit Ansible?
Gar nicht.
Ja, okay, jetzt weiß ich,
was du machen willst. Du willst das Ansible
Inventory erstellen
mit Terraform.
Ja, das würde ich gar nicht
so sehr machen, also
es gibt keine direkte, also
vielleicht mittlerweile irgendwelche Plugins
oder so, keine Ahnung.
Also es gibt jetzt standardmäßig keine direkte
Kommunikation zwischen Terraform und Ansible.
Es gibt verschiedene Sachen, wie du das
machen kannst. Du willst ja quasi
dein Ansible Inventory,
nicht das Haus, weil du willst dein
Ansible Inventory mit Terraform erstellen.
Ja gut, so direkt
gibt es da glaube ich nichts eingebautes.
Was du machen kannst in Terraform,
das gucken wir uns vielleicht auch an.
Nachher, du kannst Outputs definieren.
Das sind quasi Werte,
die Terraform, nennen wir es mal,
exportiert.
Also zum Beispiel, du legst
eine VM an mit Terraform
und dann kannst du als Output
die IP und den Namen
von dieser VM, die sie
bekommen haben nach dem Anlegen, mit Terraform
wieder exportieren und das
könnte dann in irgendeine Art und Weise
Ansible bei dir benutzen, um sich
eine Inventory zu bauen, wenn man das so machen will.
Schöner wäre die Variante allerdings,
wenn du das
unabhängig voneinander machst,
weil du willst ja auch nicht unbedingt
immer Terraform am Start haben, wenn du
Sachen konfigurieren willst.
Da sind wir jetzt aber schon
im Bereich ja quasi
Discovery, Service Discovery, Inventory
Discovery und
Resource Management, dass du quasi irgendwo
einen Key Value Store hast
oder irgendwo was, wo das registriert wird
initial, die Kisten, die du aufgebaut hast.
Ja, oder aber
die einfachere Variante
ist, also es gibt zwei Varianten, die noch deutlich
simpler sind, du benutzt einfach
das jeweilige Cloud
Kommandozeilen Tool von dem Cloud
Anbieter, was du verwendest.
Du kannst ja zum Beispiel relativ easy
hier mit GCP
GCP, Compute
Ne, was ist es?
Ach, weiß ich gar nicht.
Kannst du dir
deine VMs anzeigen lassen?
Dresses List?
Ach so, ich hab noch das falsche Projekt
ausgewählt. Muss man gleich erinnern.
Also kannst du ja auch mit dem jeweiligen Cloud
Tool des Anbieters, was du ausgewählt hast
auflisten, was dann dort Terraform
für VMs erzeugt hat.
Ist besser, da musst du die zwei Sachen nicht
miteinander, nicht miteinander vertraten.
Ja, oder
die
allerbeste Lösung, wenn möglich
bei dir, ist
du lässt einfach nach einem Terraform
Run, automatisch
die neu erstellte VM
Ansible ausführen.
Du kannst dir in Terraform Inline Script
angeben, so eine Zeile, gibst du einfach
Ansible Run oder Ansible
Playbook, dann kannst du
ein Git Repo angeben. Das Ansible
automatisch Applied wird nach jedem Starten
einer neuen VM. Das ist auch möglich.
Musst du halt überlegen, was bei dir am besten
passt.
So.
Ich muss
mal das Default-Projekt
festlegen, kurz bevor wir hier
anfangen. So, kommen wir jetzt
aber mal zu Terraform und Google Cloud.
Ich habe ein Projekt angelegt schon,
Kekkelstream 1.
Was
habe ich hier
reinkopiert?
Ach, die Projektnummer, ich habe
das falsche kopiert, null.
Ja, aber das sind meistens
Provisioners, die sollte man wirklich
nur als letzte Instanz wählen.
Ich weiß nicht, auf was ich das
bezieht jetzt gerade.
Du magst Recht haben, aber ich weiß nicht,
was du meinst.
So.
Ich habe damit noch nichts
gemacht. Ich weiß, dass das irgendwie so
ein bisschen
Container zentriertes
OS ist, wo alles abgeschottet in
Containern läuft. Glaube zumindest, dass es
das war. Oder bringe ich das jetzt
durcheinander? Ich weiß nur, dass das irgendwas ist,
mit eigenem Package-Manager dafür alles
containerisiert. Habe ich noch nicht ausprobiert.
Klingt security-technisch nach einer sehr
nice Sache und auch
um Dinger rückstandslos zu entfernen.
Ich habe es noch nicht ausprobiert. Keine Ahnung.
Kann ich nichts weiter zu sagen.
Ach, ist das nicht das mit
den, was war denn das mit den Containern?
Ja, stimmt.
Das könntest du machen lassen. Wobei
die schönere, ach, jetzt weiß ich, was er
meint. Die schönere Variante
ist ja, in den Cloud-Init-File zu nehmen.
Was das macht. Ich würde auch
nicht Terraform unbedingt warten lassen,
sondern in den Cloud-Init
reinhauen und wo beim Start anhand am Ende
Ansible ausgeführt wird.
Da kann ja Terraform, also ich würde gar nicht
Terraform wissen lassen, dass
Ansible noch läuft.
Also sprich, wozu
wozu sollte das Terraform
wissen? Terraform ist fertig, sobald
die VM über die API angelegt ist und
running ist. Danach kann
Ansible machen, was es will. Also ich würde gar nicht so
sehr Kommunikation zwischen Ansible
und Terraform probieren,
denn es sind ja zwei unterschiedliche Tools, die auch
nicht zwangsläufig von einer abhängig sind.
So, also, fangen wir
mal an. Ich habe ein Projekt angelegt in
der Google Cloud mit dem
sprechenden Namen CackleStream1.
Wenn ihr ein
Projekt in der Google Cloud anlegt,
zeige ich euch gerade mal
einen, ja weiß ich, ob ich es Trick nennen soll, aber
ich nenne es mal Trick.
Normalerweise ist es so, wenn ihr
hier irgendwas anlegt, so blub,
ihr könnt das Ding Projektname blub
und dann erstellt
Google für euch eine Projekt-ID,
die weltweit einzigartig
ist.
Allerdings muss ich sagen,
ist das Handling
von diesen IDs
vielleicht
ein bisschen kryptisch, wenn man die
an manchen Stellen eintragen muss.
Wenn man die Möglichkeit hat, hier oben
was einzutragen, was Unic ist,
dann
seht ihr, dann generiert das
keine Projekt-ID. Also, man muss sich
überlegen, was einem wichtiger
ist, ein cooler
kurzer
Projektname und dafür aber
eine nicht zu merkende ID
oder vielleicht ein etwas
längerer Projektname,
aber der weltweit einzigartig ist
und ihr könnt den überall verwenden, für
ID und Projektname. Also, ich finde es
zum Beispiel ganz cool, wenn man so was
macht, ich keine Ahnung, man kann ja zum
Beispiel so was machen, Wupplos Stream
1 und sowas in der Richtung.
Da ist es relativ sicher davon auszugehen,
dass das weltweit Unic ist. Ich weiß,
es gibt jetzt bestimmt irgendwelche Leute, die mir auf
den Sack gehen wollen und Wupplos Stream
1, 2, 3, 4 und so schon bei sich registrieren,
einfach nur, dass es angelegt ist.
Ich
kenne euch, aber
normalerweise finde ich das die schönere
Variante, weil
dann ist es irgendwo sprechend
und ohne kryptische ID
hinten dran.
Natürlich könnte die Dinger dann nicht so
was nennen wie Default oder so.
Das ist logischerweise
schon belegt.
Es gibt mehrere Leute
auf der Welt, die schon den
Projektnamen Default verwendet haben.
Also, unser Ziel heute ist folgendes.
Ich mache nochmal von Anfang an.
Das, was wir gestern gemacht haben,
ist ja nahezu nichts.
Ich zeige es einfach nochmal.
Also, was wir heute
machen, ist folgendes. Wir richten
Google Cloud in Terraform ein.
Wir legen Netzwerke an.
Wir legen einen
Router an. Wir legen
NAT-Regeln an,
dass wir ins Internet kommen.
Wir legen Firewall-Regeln an,
dass wir per SSH drauf zugreifen können.
Was machen wir noch?
Wir legen
eine Jump-VM an,
dass wir uns quasi
von daheim in unsere private Google Cloud
connecten können.
Wir legen einen
kubaniertes Account an.
Wir legen ein kubaniertes Cluster an
und ein kubaniertes Note Pool an.
Allerdings nicht
public.
Das ist alles in einem privaten Netz.
Also, wir werden das Netz hier verwenden.
Das ist alles in einem privaten Netz.
Das heißt, man kommt
vom Internet aus nicht dran.
Ist, glaube ich, sogar ein bisschen billiger,
weil man keine Public IPs braucht.
Kostet ja alles Geld.
Gehen wir ganz stark davon aus, dass es nicht umsonst ist.
Ich weiß es gar aus dem Kopf nicht, wie sehr Google Cloud ist.
Aber höchstwahrscheinlich sind Public IPs
nicht umsonst.
Das heißt, das ist alles
nicht aus dem Internet erreichbar.
Das heißt, man muss über so einen Zwischenschritt,
dass man zu seinem Cluster
hin kommt, was nicht bedeutet,
dass der Cluster
nicht von außen erreichbar gemacht werden kann
für Services, die
ins Internet angeboten werden sollen.
Aber der Cluster selbst, also
die Management Interface von dem Cluster
und von den VMS, die sind nicht
aus dem Internet erreichbar. Services, die auf dem
Cluster laufen, irgendwelche
Webseiten oder Web Services, die
können aus dem Internet per Ingress erreichbar
sein. Aber ich denke mal, das werden wir heute nicht
machen, soweit werden wir nicht kommen.
Wie gesagt, ich sag's nochmal,
ich hab mir das nicht alles selbst ausgedacht.
Ich hab gestern ein richtig
gutes Tutorial-Video geguckt.
Und zwar von ihm hier.
Sonst hätte ich das auch nicht so schnell gecheckt,
wie man die Sachen gut in Terraform macht.
Ich hab ein paar Sachen angepasst und
ein paar Dinger fehlen aus meiner Sicht
auch, die man machen sollte.
Gut, auf geht's.
Windows? Was Windows? Wo Windows?
Ach du meinst hier Windows?
Ja, ich hab
ein Setup aus Linux-Kiste hier und
Windows-Kiste hier.
Ich hab für Videos
schneiden, also Videos auf YouTube hochladen,
Streaming, Sachen spielen,
hab ich Windows und für alles andere hab ich meine
Linux-Kiste am Start. Oder ist das
ein gutes KDE-Stream? Nö.
Das ist Windows. Das ist Windows und das ist
Linux. I3.
Arch, by the way,
sollte ich vielleicht öfters mal erwähnen, weil die coolen
sind, wenn sie Arch verwenden.
Ich glaube,
noch cooler wäre man nur mit Gentoo.
Oder Gentoo.
Wie das manche
aussprechen.
Gut, also
fangen wir nochmal ganz von Anfang an
an. Ich werde mir mal ein paar
Sachen, Leute, ich werde mir...
Wir müssen noch eine Sache
von gestern fixen, sonst muss ich ein neues
Projekt anlegen und dann fangen wir nochmal von vorne an.
Hier, an der Stelle
Ich mach das jetzt einfach mal,
dann erklär ich,
dann erklär ich.
Dann erklär ich, was es macht,
wenn wir so weit sind.
Terraform, apply.
Mehr haben wir
noch nicht.
Und dann machen wir das nochmal rückgängig,
dann lösche ich das nochmal und dann fangen wir
von vorne an. So, ja, jetzt.
Wunderbar. Okay,
gut. Terraform
destroy.
So, man hat jetzt vielleicht, ohne dass ich
jetzt das im Detail erklärt habe, was passiert ist.
Man sieht jetzt zum Beispiel schon mal was
richtig cooles.
Terraform kann
Ressourcen anlegen und kann, also mit Terraform
apply Sachen anlegen. Ich hoffe, ich habe
mein Projekt jetzt nicht kaputt gemacht,
weil ich das vorhin das letzte Mal vergessen habe.
Okay,
scheint zu funktionieren.
Terraform merkt sich, welche Ressourcen das
angelegt hat und kann die Dinge danach
auch wieder löschen. Das ist sehr
praktisch. Also wenn ich, ich kann quasi was aufbauen
und rückstandslos wieder entfernen.
Also eigentlich ganz coole Geschichte.
Wenn man zum Beispiel in einem Projekt was aufbaut,
wo man nicht danach, wenn man fertig
ist, das ganze Projekt in der Cloud wieder löscht.
Aber ich zeige euch, ich zeige
es euch, ich zeige es euch im Detail.
Ich hoffe, das klappt jetzt. Network,
Network delete.
Er sollte alles löschen.
So, und ich kopiere mir jetzt noch zwei Scripts,
die ich mir gebastelt habe rüber.
Das macht es nämlich ein bisschen einfacher.
Das erste ist ein
Init-Script.
Legen wir hier mal rein. Ah, nein, es hat
funktioniert. So.
Gehe keine Terraform.
So, ich lösche mal, ich lösche
mal kurz meinen ganzen Terraform-State so,
als hätten wir noch nichts gemacht.
Terraform-Logs und alles. So,
dann machen wir noch ein extra Verzeichnis,
nennen wir mal Terraform. Und da
werde ich jetzt alles, was es hier gibt,
reinschieben.
Weil wir müssen es ein bisschen besser strukturieren.
Denn hier kommt ja später noch ein
Kubernetes-Ordner
hin und alles Mögliche. Wir machen jetzt
mal hier ein Script hin. Das nennen wir
Init und ich kopiere mal was rein.
Und dann führen wir das mal aus.
Das macht es nämlich ein bisschen einfacher.
So, also.
Ich kann euch nicht sagen, ob das der weisheitsletzte
Weg ist, es so zu machen. Aber
wenn ich ein neues Google Cloud-Projekt
anfange,
bereite ich das ein bisschen darauf vor,
dass ich mit Terraform meine ganzen Ressourcen
anlegen kann. So, und zwar
ich suche mir einmal die aktuelle Projekt-ID
raus, dann lege ich ein
Storage-Bucket an, zeige ich euch
gleich, wofür das notwendig ist. Und
ich lösche die Default
Firewall-Regel und das Default-Netzwerk.
An das Trinken erinnern. Sehr gut.
Exzellent.
Schau dir mehr Terra
nix an.
Terra-Nigma kenn ich.
Ach so, für... Ja, es gibt
ja ein paar Terraform-Wrapper.
Ich bin immer noch für Raw Terraform.
So, dann führen wir das jetzt mal aus.
Ach so, das alte Storage-
Bucket können wir auch noch löschen.
Cloud-Storage.
Ist das so fett? Alter.
Äh.
Das da.
Ja. So, löschen wir
mal das Storage-Bucket.
Und dann fangen wir
an.
Delete.
Zack.
Okay,
füllen wir das Init-Script aus. Wie gesagt,
ich mach nachher noch ein GitHub-Repo,
wo das alles drin ist.
Auflöschen, please.
Yes.
Yes.
So.
Das dauert jetzt kurz und da können wir uns schon
ein bisschen um die Terraform-Sachen kümmern.
Dieses Init-Script
habe ich mir einfach nur gemacht, damit es schneller
geht, aus einem Default-Projekt
zu bauen,
dass ich Terraform ausführen kann.
Also, das erste, was man für Terraform konfigurieren
muss, ist folgendes.
Man muss angeben, welche Provider
man benutzen will. Wir benutzen Google.
Das heißt, wir benutzen den Terraform-Provider
für Google. Wer hätte das gedacht?
Dann muss man noch einstellen,
welches Projekt
man befüllen will.
Und ihr seht ja hier, Cackle Stream 1
ist mein Projekt.
Und dann noch, in welcher Region wir das Ganze
machen wollen. Wir wollen das in der Region
Europe West
3 machen.
Europe West 3 ist Frankfurter
Main. Wenn man nicht weiß,
was es ist, Europe West
3, man kann hier nachgucken
bei Google. Zum Beispiel hier
Europe West 3.
Da sieht man Europe West 3 ABC.
Das sind die Zonen, by the way.
Europe West 3 ist
Frankfurt, Germany. Aktuell
zumindest. Könnte ja mal irgendwann vielleicht auch woanders
sein.
Und das hier ist die Region.
Und das hier unten drunter, dieses Minus ABC,
das sind Zonen.
Und manchmal muss man
bei Google Cloud
entscheiden. Möchte man Sachen
in einer Region oder in einer Zone
aufbauen. Wenn man das in einer Region
aufbaut, dann werden manchmal, also
zum Beispiel, wenn man
Storage oder sowas anlegt, dann wird das
automatisch repliziert. Oder die
Kubernetes
Note
Na, wie heißt
Control Plane wird dann
repliziert automatisch in
alle Zonen einer Region. Das kostet
mehr. Ist nice. Will man aber oftmals
gar nicht haben. Das heißt, man sollte sich immer überlegen, möchte man
was regional oder wirklich nur
was in einer Zone aufbauen.
Und in einer Zone es aufzubauen ist meistens billiger,
weil regional bedeutet in der Regel
in allen drei Zonen einer Region.
Ja, und es gibt noch bei
Storage auch die Möglichkeit, das tatsächlich
Location-weit zu
machen, also dann quasi EU,
US und sonst was in der Richtung.
So, also das ist eine ganz simple
Config, was ich eigentlich
wofür ich Terraform in dem Fall
hier eigentlich verwenden will. Ich will das für
ach so, ich soll euch vielleicht mal das
Terraform-Config-File zeigen.
Ich will das Ganze für die Projekt-ID
Kackelstream 1 verwenden.
Region ist Europe West 3, das ist Frankfurt
und die Zone, wenn ich Zonen verwenden
möchte, dann möchte ich quasi so
die Unterregion
minus A verwenden.
Und die Unterregion in dem Fall ist halt
die Zone, so nennt sich das bei Google Cloud.
Und wenn man
hier so ein File anlegt, Terraform.tf
wars,
dann zieht Terraform hier den Inhalt
dieser Variablen aus diesem
Variabel-File. Das ist ganz nützlich.
Es kann ja durchaus sein, dass ich die gleiche
Terraform-Config, ja jetzt mach weiter,
es kann ja durchaus sein, dass ich
die gleiche Terraform-Config
für mehrere
Projekte
benutzen möchte. Also es kann ja
sein, dass ich zum Beispiel sowas habe wie
Kackelstream Dev, Kackelstream
Live oder sowas, und
dass ich das darauf dann applyn will.
Also ist das ganz sinnvoll, so
die grundlegenden Sachen, so wie Region,
Zone und vor allem Projekt-ID
Variabel
zu machen.
Man könnte sich das übrigens auch komplett
sparen. Man könnte zum Beispiel hier
auch sagen Default
und dann könnte ich hier mein Projekt eintragen.
Was ich aber ein bisschen blöd finde,
weil es könnte ja sein, dass ich was anderes
verwenden will und aus Versehen es dann in diesem Projekt
ausführe, obwohl ich das vielleicht gar nicht will.
Also so finde ich
es auf jeden Fall schöner. Bis jetzt
passiert noch nichts großartig. Wir haben jetzt nur den Google
Provider konfiguriert
und paar Basic-Variabeln
angelegt, die wir in Zukunft brauchen.
Der nächste Schritt ist das hier.
Ja, da
kann man sich jetzt überschreiten, macht man das
in der Extra-Datei oder macht man das
hier beim Provider drinnen.
Was das macht, ist
die Voraussetzung dafür,
dass wir überhaupt was anderes machen können,
nämlich das aktiviert in
meinem Projekt erstmal bestimmte
Services, weil wenn ich jetzt hier
reingehe,
ich habe es gestern schon aktiviert,
ich kann es euch nicht zeigen. Normalerweise,
wenn ich jetzt hier reingehe in die Kubernetes-Engine
oder in die Compute-Engine,
Compute-Engine ist bei Google Cloud
quasi alles was mit virtuellen Maschinen zu tun hat,
dann würde jetzt normalerweise hier was aufploppen.
Achtung, dieser Service ist nicht
aktiviert, sie müssen diesen Service
erst aktivieren. So, nachdem
ich weiß, dass ich
eben den Compute-Engine verwenden
will, kann ich hier sagen
Compute-Engine, Service,
aktivieren.
Und das hier ist wichtig an
der Stelle.
Ich weiß nicht, ob es ein
Bug ist oder
ob es
bei Design ist.
Man kann
über Terraform Services,
die man aktiviert hat für das Projekt,
nicht wieder deaktivieren.
Ich bin mir auch gar nicht sicher,
ob man Services wieder deaktivieren kann
irgendwie übers Web-Interface.
Ich glaube, ich habe es noch nie gemacht, geht wahrscheinlich auch
irgendwo. Also wenn ich jetzt beispielsweise
für dieses Projekt Compute-Engine
komplett wieder deaktivieren will,
dann wüsste ich gar nicht, wo ich es mache.
Und wenn man das hier
ausstellt
und ich mein Terraform Projekt wieder entferne,
dann lässt es das Ganze
einfach enabled.
Das ist quasi nur Anschalt Only,
Ausschalten gibt es nicht mehr.
Das ist bei Services wichtig, weil also
bei diesen AP Endpunkten wichtig,
weil ansonsten schmeißt Terraform irgendwelche Fehler.
Also wenn ihr komische Fehler bekommt
bei Terraform Destroy, belegt euch vielleicht,
ob ihr bei Service Definition Disable
und Destroy ausschalten müsst.
Das
dient aber alles noch dazu, das vorzubereiten
für das, was wir machen wollen, also es ist noch nichts
noch gar nichts Großartiges passiert.
Der nächste Block
ist wichtig, wenn ihr Terraform nicht nur alleine
verwenden wollt. Überlegt euch mal,
ihr verwendet Terraform im Team
und die Konfics hier
benutzen mehrere Leute gleichzeitig.
Ich habe ja gesagt, Terraform
speichert sich, was es
gemacht hat.
Standardmäßig speichert Terraform
das Lokal bei euch im Verzeichnis
Was?
Ah, ich kann noob. Mein Init
Skript funktioniert ja noch gar nicht so.
Okay.
Warum eigentlich nicht?
Ach so, weil ich das falsche Projekt eingetragen habe, oder?
Ja, ich noob aber def eingetragen,
Alter. Muss eins rein, nicht
f0.
Naja, egal.
Wenn man jetzt mit mehreren Leuten verwenden
möchte, dann
ist der State
den Terraform standardmäßig hier
bei euch lokal im Datei verzeichnet,
also im Filesystem speichert,
wird dann zum Problem,
weil es könnte jetzt ja sein, dass
ich das gerade ausführe, bei mir lokal
und jemand anderes bei sich lokal ausführt,
dann haben wir zwei Terraform-States,
die voneinander nichts wissen.
Das muss über kurz oder lang
zu Problemen führen,
weil Terraform merkt sich, was
es gemacht hat und versucht es immer
rückgängig zu machen, wenn jemand außerhalb
von Terraform was dran gemacht hat.
Und wenn jemand Terraform auf einem anderen Rechner laufen lässt
und die beiden voneinander nichts
wissen, dann denken beide,
der andere hätte es ihm kaputt gemacht.
Das heißt, man muss sich überlegen, wo man den
Terraform-State
speichert
und den Terraform-State kann man
zum Beispiel in einem Storage-Bucket speichern
in der Cloud.
Auf dieses Bucket greifen dann alle Leute
gleichermaßen zu
und es gibt keinerlei
Synchronisationsprobleme und
einer macht dem anderen den State kaputt.
Das heißt, da können mehrere Leute Terraform ausführen
und
es ist
alles gut, es funktioniert.
Ja, macht das.
Heute nicht arbeiten, Max,
oder ist montags frei?
Ich habe montags frei, immer.
Schon seit ich Teilzeit arbeite,
habe ich montags frei.
So, ich muss mal kurz hier
in meine Buckets gehen, ich habe glaube
irgendwas erstellt, was ich nicht erstellen wollte,
weil ich blöd rumeditiert
habe.
Ist so,
alles da, gut.
Eigentlich alles
alles kurz.
Warum ist mein Innscript
gefailt?
Na gut, keine Ahnung,
ich fülle es.
Ich fülle es so nochmal aus.
Geht doch alles.
Alles gut.
Also,
wie wird bei GCP der State Cloud
gemanagt oder passiert das auch?
Ja, das passiert im Bucket.
Wenn du das über Cloud
Storage machst, hast du überhaupt
keine Probleme mehr, wenn mehrere Leute...
Okay, überhaupt keine Probleme
ist bei Terraform immer so ein Ding.
Es kann immer irgendwie außer Sync geraten, weil
irgendwelche komischen Sachen passieren, aber
du solltest keine Probleme mehr haben, wenn du den
kompletten Terraform-State
in dem Cloud, in dem Bucket
speicherst. Du musst das übrigens auch nicht
hier zwangsläufig
so machen, wie ich das gemacht habe.
Die haben...
Die unterstützen mehrere Backends.
Also, sie unterstützen alle
großen Cloud-Anbieter
jeweils die...
...das Spezielle von denen,
aber auch S3.
Bin ich jetzt bescheuert? Wo ist das denn?
Each Backend?
Available Backends.
Also, die können natürlich...
Die können Google Cloud, die können
natürlich auch Azure.
Und die können auch S3.
Du kannst es auch in Postgre speichern,
als SQL im Kubernetes-Cluster,
über irgendeinen Web-Server, was auch immer.
Oder auch in Konsul.
Oder auch ganz, ganz toll in der
Alibaba-Cloud.
Nee, du kannst bei AWS
auch in einem S3-Bucket
speichern, wenn du willst. Du musst das nicht
über DynamoDB machen.
Kannst du auch? Also, würde mich
sehr... Ach, Moment!
Moment!
Sekunde, ich hab nichts gesagt.
Mist, ich hab Mist erzählt.
This Backend also
supports State Locking and Consistency
Checking.
Okay.
Also...
Vielleicht hab ich dir auch Mist erzählt.
Vielleicht hab ich dir
auch Mist erzählt.
This Backend supports State Locking.
Nee, doch, geht. Das geht alles über das
Storage-Bucket.
In dem Fall. Ja.
Es funktioniert. Also, das kann man alles
so machen.
Wenn man mehrere Umgebungen hat,
dass man z.B. Dev-Int
oder was weiß ich,
Dev-Life oder was auch immer
hat, kann man hier noch Unterordner machen,
wenn man den State unterschiedlich
speichern will.
Ja.
Muss man sich ein bisschen angucken,
was der jeweilige Cloud-Anbieter unterstützt.
Und jetzt können mehrere Leute
parallel Terraform ausführen, ohne das,
was kaputt geht.
Soweit so gut.
Es gibt eine Sache, die man noch
machen sollte, die wir jetzt aber nicht
gemacht haben. Man kann
hier, bei diesem
Storage in der Google Cloud einstellen,
dass das eine History sich speichert.
Sollte man nämlich mal, für den Fall der
Fälle, dass irgendjemand Mist baut
und was kaputt macht, könnte man so
den State wieder, hat man quasi so einen
automatischen, es nennt man sieben Tage
Backup vom Terraform-State,
falls irgendjemand Mist eingetippt hat.
Ich möchte es einfach schnell wieder rückgängig machen.
Machen wir es an der Stelle nicht. It's overkill brauchen wir nicht.
So, und wenn wir das alles eingerichtet haben,
können wir anfangen, unser Google Cloud
Projekt
anzulegen. Das war alles nur
Zeremonie jetzt,
zur Vorbereitung.
Und wir fangen jetzt an.
Also, wie gesagt, wir legen
Netzwerke an, Routen an, Firewallregeln
an, JumpVM
in die Cloud an,
ein paar Accounts an, Kubernetes Cluster und
Kubernetes Cluster Note Pool. Wir fangen
allerdings ganz simpel an
und führen es mal aus und dann versuche ich mal
so ein bisschen grob zu erklären, was Terraform
eigentlich macht und
warum es nützlich ist
und was das Coole ist, wenn man
seine Infrastruktur mit Terraform verwaltet.
So, damit wir
unsere ganze Cloud Infrastruktur anlegen, ist das
allererste, was wir brauchen, ein Netzwerk.
Und ich bin mal so frei und benenne das
um, und zwar ein Network.
Und wir fassen das
ein bisschen zusammen.
Wir fassen nämlich Subnetz und
Network zusammen, habe ich mir überlegt, ist glaube ich
einfacher zu verstehen, was letztendlich
was letztendlich passiert.
Also,
hier sagt man
mittels Terraform an
die Cloud, dass ein Netzwerk erstellt werden soll.
Und zwar wollen wir ein Netzwerk erstellen
mit der IP Range
oder mit der
SIDR,
wie ich vor immer das genau ausspricht, Range, das ist
Classless Inter-Domain Routing.
Kein Mensch
spricht das jemals so aus.
Wie auch immer, man könnte es einfach
IP Range nennen,
aber in dem Fall ist es IP SIDR Range.
Es gibt eigentlich
heutzutage so gut wie, also warum sollte man
diese Sachen benutzen, ich weiß es nicht.
Zumindest
das ist die Range, die wir
für unsere IPs benutzen.
Das heißt 10.0.0.0
slash 24.
Wir können mal gucken, ich meine es ist
in dem Fall wirklich extrem simpel.
Leute, wer hat aufgepasst
in der Berufsschule?
Was ist die erste IP und was ist die letzte
IP, die wir benutzen können in der Range?
Was ist die
erste IP und was ist die letzte IP?
Nutzbare IP.
Nutzbare IP,
wohlgemerkt. Also
10.0.0.0 slash
24. Die erste IP ist was?
HiIQ Chat. Jetzt
enttäuscht mich nicht. Es ist sehr simpel,
das kann jeder Azubi in der dritten Woche.
1, richtig. Und was ist die
letzte nutzbare IP in diesem
Netz?
254.
Exzellent. Also das ist quasi
das hier,
bis
254.
Korrekt. Wir können übrigens
auch nochmal überprüfen, dass wir recht haben hier.
IPCalc, sehr nice tool.
Hosts minimum 1,
Hosts maximum 254.
Und Broadcast
Adresse ist 255.
Das heißt nutzbar in diesem Netz sind
254 IPs.
Und die Netzadresse ist
das da.
Und die Netzadresse ist das hier. Und die Netmaske
in dem Fall ausgeschrieben, das da.
Also das ist glaube ich
ziemlich simpelste IP-Netz,
was man überhaupt haben kann.
Oftmals verwenden
man daheim ja eher das hier
oder sowas hier. Aber
wir machen jetzt einfach mal das hier. Hat den
Vorteil, werdet ihr später auch noch sehen,
ein Netz zu verwenden, was ihr nicht
zu Hause benutzt, weil
wenn ihr euch dann später so
nennen wir es mal VPN-mäßig
in eure Cloud verbinden wollt,
ist es praktisch, wenn es dieses Netz
bei euch lokal nicht gibt.
Weil ansonsten habt ihr das Problem,
dann könnt ihr entweder nur auf das Netz lokal bei euch zugreifen
oder auf das Netz in der Cloud zugreifen,
weil das ist das gleiche Netz. Oder ihr müsst
sehr viel komische Dinge machen.
Also es ist sinnvoll, ein Netz zu benutzen,
was man lokal nicht
hat. Zumal man hier sehr viele
Möglichkeiten hat, dass es glaube ich standardmäßig
in Slash 8,
was dafür vorgesehen ist, als private
IP-Range. Wobei ist es das
überhaupt?
Ich glaube, es ist standardmäßig in Slash 8.
Also man kann
quasi das hier, das hier und das hier frei
benutzen.
Ja, geht wieder besser. Gestern Abend
war ziemlich
Pepo-Pupu-Fire-Toilet
angesagt.
Ja,
beste Umgebung dafür.
Wie kommst du drauf,
dass es vielleicht nicht möglich sein sollte?
Das ist die beste
Entwicklungsplattform für .NET
mittlerweile.
Besser als Windows.
Die beste Plattform um .NET-Programme
.NET Core bzw.
es ist ja eigentlich nur noch .NET heißt es jetzt.
.NET 6, 7, 8
Programme entwickeln will
ist das beste, was man machen kann auf Liebungsbasis.
Ja, es sei denn, man möchte eine
Windows-Only-GUI, das ist korrekt,
aber warum sollte man eine Windows-GUI
unter Liebungserstellen wollen?
Es gibt Möglichkeiten, mittlerweile relativ
gut Cross-Plattform-GUIs mit C-Sharp zu
erstellen. Leider ist Cross-Plattform-
GUI immer noch ziemlich Paints-Champ.
Auch der Grund, warum sich
Electron so durchgesetzt hat die letzten Jahre.
Avalonia ist
Pretty Pog.
Hier geht es ja wieder besser.
Probiert kann man es nicht nennen.
Ich habe mal ein paar Beispiels
Sachen durch
geklont von der Webseite
und habe das mal, also nicht großartig
was gemacht.
Alles klar.
Wo bin ich jetzt stehen geblieben?
Genau, also wir legen jetzt
Netzwerke in der Google Cloud an.
Ist Elektron nicht nur eine Web-Anwendung?
Doch, doch, ist es.
Eine Windows-Only-GUI
unter Linux
entwickeln ist schwierig.
Da kannst du es ja nicht ausprobieren.
Also wenn du Windows-Only-Software
entwickelst, dann würde ich die logischer
Weise auch unter Windows entwickeln.
Wenn du Avalonia
Cross-Plattform-GUI benutzen möchtest,
die würde dann unter Linux und
Windows laufen.
Aber testen mussten sie nach wie vor unter
Windows.
Also wenn du Windows-Only-Software
entwickelst, dann macht das unter
Windows. Was anderes macht auch keinen Sinn.
Wie gesagt, es sei denn du benutzt ein
Cross-Plattform-GUI-Framework
und
hoffst, dass es unter Windows dann auch so aussieht
wie das, was du unter Linux testest.
Unter Windows-Programmieren macht
halt überhaupt keinen Spaß.
Aber wir müssen bei Google Cloud
Config bleiben, sonst wird das heute nichts.
Also, wir legen jetzt Netzwerke
in der Google Cloud an.
Also wir haben ein relativ großzügiges
Netz, 254 Hosts
haben wir hierfür.
Und es gibt ein paar Sachen,
die man hier angeben muss, die
ganz praktisch sind.
Oder besser gesagt, ohne die es ein bisschen
schwierig wird. Und zwar das erste ist hier die
Pens-On.
Wir möchten Netzwerke erst anlegen,
nachdem alle Apis
aktiviert wurden
im Google Cloud Projekt, was wir brauchen.
Das müssen wir aber nur einmalig machen,
dass wir sicherstellen können, bevor wir anfangen
irgendwas zu machen, dass
die nötigen Apis im Projekt
eingeschaltet sind. Das kann 20-30 Sekunden
dauern. Und dementsprechend ist das
einmal sinnvoll als Abhängigkeit zu
definieren.
So. Routing-Mode ist erstmal
wurscht. Wie das Ding heißt, ist auch erstmal
wurscht. Ich benutze es, ich nenne es mal
übrigens um. Ich nenne es mal Default,
weil ich habe das Default-Netzwerk gelöscht.
Deswegen bin ich viel angenehmer. Default.
Routing-Mode,
ihr könnt übrigens, wenn ihr die Terraform
Visual Studio Code
Erweiterung installiert habt, könnt ihr euch die
Hilfetexte aus der jeweiligen Cloud
Api Docs anzeigen lassen.
Sprich, was hier möglich ist.
Und hier steht auch so ein kleiner Hilfetext dabei.
Und dass es als mögliche Values
Regional und Global gibt. In dem Fall
Routing-Mode für unser Netzwerk
Regional ist vollkommen okay.
Auto-Create-Subnetz würde ich auch
anschalten. Ich will die Sachen selbst anlegen.
Und das hier
finde ich tatsächlich relativ
nützlich. Und zwar
Delete Default Routes on Create
setze ich auf False.
Das ist ganz nützlich dafür, wenn man
zum Beispiel mit seinen VMs ins
Internet raus will, um Paket-Updates zu machen.
Braucht zwar glaube ich noch eine
Nattregel. Bin ich mir jetzt gar nicht ganz sicher.
Aber
wenn man das komplett abgeschottet isoliert machen will,
dann würde man das auf True setzen.
Wenn man aber zum Beispiel mit seiner VM noch ordentlich
ins Internet will und eine Default
Route haben möchte, dann würde ich
das hier ausschalten. So.
Und hier unten legen wir das
eigentliche Netzwerk an. Also
das hier ist im Prinzip nur so ein
Container.
In dem man dann verschiedene Subnets
anlegen kann. Man sieht es ja auch hier,
dass man hier erst die IP-Range angibt.
Nicht wie erwartet im Netzwerk
selbst, sondern erst bei Subnet.
So. Und das ist im Prinzip
alles, was ich euch schon erzählt habe. Wir
legen ein Subnet an
mit der IP-Range. Quasi
die hier bis die hier.
In der Region, die wir
vorher als Variable definiert haben. Also quasi
hier in der Region
in der Region Europe West 3.
Und sagen, dieses Subnet
gehört zu diesem Network.
Und
das ganze soll private sein.
Ach nee, Moment. Das ist nicht das Private.
Das ist, dass die VMs Google
API Services
zugreifen können. Ja, das ist sinnvoll. Das schalten
wir ein. Und das war's.
Das war's für die Netzwerk-Config. Mehr müssen wir an der
Stelle nicht machen. Was jetzt passieren
wird, ist folgendes.
Wir werden jetzt mal hier in Netzwerke
gehen. Übrigens bei Google Cloud nennen sich
Netzwerke VPC.
Ich glaube Virtual Private Cloud.
Und deshalb man das nicht einfach nur Networks genannt hat.
Entzieht sich meiner Kenntnis.
Trotzdem
muss man sagen, Google Cloud
hat, finde ich, noch ganz gute
Benamung. Nicht so wie AWS, wo man
echt nicht weiß, wie die Sachen heißen.
Bei Google Cloud, finde ich, heißen die Dinger
meistens relativ gut benannt.
Cloud Storage, SQL,
Compute, Kubernetes. Kann man sich
alles was drunter vorstellen. Bei Amazon
heißt das dann
Fargate und
solche Geschichten. Wobei es auch EKS und
was auch immer. Ich finde bei Amazon sind die Dinger
eklig benannt. Zumindest das, was ich bisher von Amazon
gesehen habe.
Bei Google sind die Dinger ganz okay.
Route 53
ist doch eindeutig. Ich meine zumindest
Port 53 und DNS bringt man halbwegs
zusammen. Aber die Benamung
bei AWS, finde ich, finde ich wir.
Ihr seht, wir haben keinerlei Netzwerke angelegt.
Das heißt, wenn wir unsere Terraform
ausführen, wir haben übrigens noch was vergessen,
aber ich lege erstmal das Netzwerk an,
dann zeige ich euch, was noch fehlt.
Sollten wir
danach ein Netzwerk haben und zwar mit
dieser IP-Range. Also probieren wir das mal aus.
Terraform, apply.
So, wenn man Terraform
apply macht,
was?
Ah, ja, ich sollte
es auch vielleicht default nennen und nicht
die Sache von gestern. Wenn man Terraform
apply macht,
dann zeigt am Terraform immer erst an, bevor
es anfängt, was es machen würde.
Das heißt, Terraform hat sich jetzt
connected zur Google Cloud.
Übrigens, wer sich fragt,
wie connectet sich Terraform überhaupt
zur Google Cloud?
Das geht über das Google Cloud
Command Line Tool,
wenn man das eingerichtet hat. Das heißt,
bevor man das macht, muss man einmal
Gcloud Inet machen und
Gcloud
Auth
ja,
Gcloud Auth Application, die vollen Login,
Projektname, bla, bla, bla. Also muss es
einmal einrichten und sich einmal anmelden.
Und dann kann Terraform
das Ganze über das Google Cloud Utility
machen. Das heißt, man muss in Terraform und
in seinem Git Repo, wo man seine Terraform
Files hat, keine
Tokens und Passwörter hinterlegen, sofern
derjenige
ein
eingerichtetes Google Cloud
Command Line Tool bei sich
auf dem Rechner hat. So, also wenn man
sagt Terraform Apply, dann sagt einem Terraform
erstmal, was es machen würde.
Bei IBM heißt die Dinge, glaube ich, auch VPC,
aber IBM
IBM verkauft, verkaufen einem
auch tolle Großrechner, die dann bei einem im Rechenzentrum
stehen und einem aber gar nicht gehören.
IBM macht viele
Sachen, wobei die allerschlimmsten sind immer noch
Oracle, was das angeht, was
Lizenzierung und wirre, wirre Verträge
angeht. Ich glaube, an Oracle kommt da nichts ran.
Wobei ich hab gehört, SAP soll auch nicht
ohne sein. Bin ich froh, dass ich nie
große Berührungspunkte bisher mit
SAP hatte und ich hab's ehrlich gesagt auch
nicht vor.
So, jetzt, wenn man Terraform Apply macht,
dann erzählt einem Terraform erstmal,
was es machen würde. Da kann man jetzt gucken,
ob das, ob das passt, was man da,
was man da vor, also ob
das so sein soll.
Terraform ist jetzt
zu meiner, zu meiner Google Cloud in das Projekt
gegangen und hat nachgeguckt, was ist denn schon da
und was soll das anlegen und jetzt schickt es mir
ein Report
und sagt, okay,
ich würde folgende Sachen
machen. Ich leg ein Netzwerk
an und ich leg
ein Subnet an mit diesen
Eigenschaften. So, da kann ich jetzt durchgucken
und alles ist
gut. Ja, und es enabelt unser Projekt,
unsere Apis.
Okay, sieht gut aus. Yes.
Wer keinen Bock hat,
da drauf zu warten
und sich sicher ist, dass er immer fehlerfrei
seine Terraform-Config erstellt,
der kann sagen,
auch wenn ich es nicht unbedingt empfehlen würde,
der kann sagen,
Terraform Apply minus Auto Approve.
Und dann sagt es
automatisch Yes.
Ich weiß nicht, wie ihr das handhabt, Leute,
seid ihr eher im, ich guck mir
alles an und sag Yes oder seid ihr eher
im Auto Approve Lager für die Leute, die schon
in Terraform benutzen oder Terraform auf das benutzen.
Ich muss sagen, ich bin faul
as fuck. Das heißt, ich benutze ganz gerne
Auto Approve.
So, Terraform ist durch.
Wo läuft denn Terraform in der Praxis?
Ah, da kann ich gleich was zu sagen.
So, Terraform ist durch und wir sehen,
dass es ein
Netzwerk gibt mit dem Namen Default
mit einem Subnet.
Und das ist das
Subnet, was wir angegeben haben. Also, das hat
immer funktioniert. So, und jetzt
das Feature schlechthin,
warum man
Terraform benutzt.
Oder zwei. Zwei Features
schlechthin, warum man Terraform benutzt.
Das erste habt ihr gesehen, man hat jetzt
alles, was man hier hat, definiert in der Textdatei.
Das heißt, wenn ich ein nächstes Projekt
anlege
und das genauso konfigurieren will,
würde ich die Variablen austauschen,
würde sagen Terraform Init, Terraform
Apply und es wäre genau
in dem Zustand, wie es sein soll.
So.
Ich sage jetzt nochmal was hierzu.
Also, wo läuft denn
Terraform dann in der Praxis auch nach VM?
Terraform kann ohne Probleme bei dir auf
dem Laptop laufen. Oder aber
in der VM. Oder in der Cloud VM.
Oder theoretisch glaube ich sogar in der Cloud Shell.
Du kannst
allerdings auch einen
Workflow
in GitLab oder in GitHub machen,
der für dich Terraform
Applied. Da gibt es ein ganz cooles
Projekt, GitHub
Terraform Atlantis.
Wenn man im Team arbeitet, ist es tatsächlich
ganz nice. Und zwar
ist das eine Pull Request
Integration für
Terraform.
Kannst du das auch in einer Pipeline ausführen? Genau.
Oder GitHub Actions, was ja im Prinzip
ist ja eine Pipeline.
Wenn man das im Team hat,
im Team benutzt und das ein bisschen strukturiert
haben will, dann kann man
Atlantis benutzen.
Und Atlantis
hängt sich quasi in diesen Pull Request
Workflow rein. Ich weiß nicht, ob
sie ein gutes...
Ja, hier sieht man es eigentlich ganz gut.
Open Pull Request. So. Und dann
schreibt die Atlantis quasi rein.
Was würde jetzt passieren mit einem Terraform
Plan? Terraform Plan ist das,
was das anzeigt, was es machen würde.
So. Und dann
können auch andere Leute diesen Output
von dem Plan reviewen
und approven. Und wenn das gemerged
wird, so
dann...
Wenn das gemerged wird, dann
wird das Ganze...
Ach ne, man
Applied es erst und dann
Merch man das. Okay, auch gut.
Dann ist das Ganze ein bisschen nachvollziehbarer.
Wo ist
der Unterschied zu
Docker mit
Docker Compose?
Docker
ist die...
Dein
Command Line Tool, um Container
zu starten, zu stoppen,
downzulonen und anzulegen.
Und Docker Compose ist was,
um mehrere Docker Images
miteinander zu verbinden.
Also du kannst
mit Docker Compose
alles das machen, was du
auch von Hand mit Docker machen kannst.
Nur halt in einer automatisierten
Art und Weise.
Und unterm Strich sind alle diese Tools,
egal ob es Docker ist,
Potman, Kubernetes
oder was auch immer,
benutzen die alle die gleiche Funktionalität.
Das kann man nicht oft genug sagen.
Also die Möglichkeit,
Container auszuführen.
Container gibt es ja eigentlich gar nicht.
Habe ich ja schon oft genug gesagt.
Das sind Kernel Funktionen von Linux.
Und
das besteht hauptsächlich aus zwei
Portman. Aber
Pogman wäre auch... Gibt es mal ein Gitter?
Okay.
Es gibt schon auf Github Projekte, die sich
Pogman nennen.
Also
noch mal zu dieser
Container Geschichte.
Letztendlich sind das zwei
Funktionalitäten zum größten Teil, die
Linux anbietet. Und die
ausdampflos alle Container
Runtimes benutzen.
Weil es gibt keine Container außer
das, was der Linux Kernel anbietet.
Das sind zwei Sachen. Das sind
C Groups
und Namespaces.
Das eine ist so ein bisschen
Sandboxing. Und das andere
ist,
dass man quasi
so tun kann, als wären
Sachen anders. Zum Beispiel andere IP,
andere Hausnahme, als es wirklich ist.
Unter der Haube gibt es keine Container.
Es gibt Sandboxte Prozesse
mit C Groups.
Und Prozessen, den du
vorgaukelst, der Hausnahme wäre anders,
mit Namespaces.
Aber unter der Haube gibt es eigentlich keine
Container in dem Sinn.
Sondern das ist Linux Kernel
Funktionalität, die
um ganz normale
Prozesse drumherum gelegt wird.
Also ob ihr jetzt zum Beispiel
Echo 1, 2, 3
so ausführt, oder
Docker, Run,
Ubuntu,
Echo, Echo 1, 2, wie geht das nochmal?
Ubuntu. Ich weiß nicht, ob das so funktioniert.
Wie führt man
ein Kommando drin aus?
Muss ich Bash? Muss ich Bash
minus C machen?
Ich habe keinen Plan
aus dem Kopf. Muss ich,
ich muss Exec machen, oder?
Das kann ich mir aus dem Kopf
nicht merken.
Exec, hier, das, das, das muss ich machen.
Aber ich will Runnen und direkt Exec
machen.
Das muss ich machen.
Aber ich will Runnen und direkt Exec machen.
Warte mal, wie ging das? Minus It?
Das will ich machen.
Okay.
Also ob ich jetzt so was hier
mache,
kann man jetzt hier
minus C, Echo 1,
2, 3 machen? Ja, das funktioniert.
Okay, also ob ich jetzt so was hier
ausführe, oder ob
ich hier so was hier ausführe,
da ist weniger Unterschied,
im Detail natürlich schon, ist weniger Unterschied
dazwischen, als man vielleicht so denkt.
Das hier wird auf meinem Hostsystem
ausgeführt, genauso wie
das. Es wird beides auf meinem Hostsystem
ausgeführt. Der Unterschied ist, hier
benutzt es Binaries von Ubuntu, um es auszuführen,
aber unter der Haube
sind das beides Prozesse
auf meinem Host.
Das ist nicht wie bei VMs.
Also,
da ist weniger Unterschied
dazwischen, als man denkt.
Das hier ist kein Ubuntu im eigentlichen Sinne.
Das ist nach wie vor der ganz normale Kernel, der hier
auch läuft.
Wenn ich jetzt mir nicht
hier anzeigen lasse, was ich finde,
da bin ich mal gespannt, was eigentlich in der VM
passiert, wenn ich das ausführe.
Im Container passiert, wenn ich das ausführe.
Da steht
Mist da, der nicht
wirklich stimmt. Doch, hier seht ihr es doch,
guckt mal. Also euch wird vielleicht was
auffallen bei den Sachen. Hier lasse ich mir die Kernel
Version anzeigen auf meinem Host.
Hier lasse ich mir die Kernel Version im Container anzeigen.
Guckt mal da. Es ist
der gleiche Kernel.
Also, ob ich was im Container
ausführe, oder ob
ich was auf meinem Host ausführe, das ist gar
kein so großer Unterschied. Das eine
ist ein Prozess, der direkt
losläuft.
Auf meinem Host. Und das andere
ist ein Prozess, der auch direkt losläuft,
nur, dass noch ein bisschen Sandboxing
Zeug drumherum kommt.
Es sind beides Prozesse
auf meinem Host.
Im gleichen Kernel.
Das ist der große Unterschied von Containern
zu VMs. Containern gibt es in
dem Sinn nicht.
Container ist ein Sammelsurium aus verschiedenen
Sachen, die der Linux-Kernel
bereitstellt und die
manche Container-Runtimes wie Docker
oder Kubernetes bereitstellen.
Einen Container an sich
gibt es in Linux gar nicht. Auch wenn das so
genannt wird. Das ist ein Sammelsurium aus
verschiedenen Funktionen.
Und unter der Haube sind es ganz normale
Prozesse, die im gleichen
Kernel laufen, wie alles andere,
was ich sonst so ausführe.
Bei einer VM wäre das was
anderes. Wenn ich zum Beispiel sage
hier, wir machen
mal eine Debian
Testkiste.
Yes, leg die mal
neu an.
Das ist richtig. Kubernetes
ist keine Container-Runtime. Kubernetes ist
ein Orchestrierungstool.
Ich weiß
gar nicht, was Kubernetes mittlerweile standardmäßig
für eine Runtime nimmt.
Container-D oder
Runtime? Ich weiß gar nicht.
Ist das bei LXC gleich?
Richtig. Das ist bei LXC genau das gleiche.
Seba
Su-Chan95
Danke für den Sub.
Das ist bei LXC genau das
gleiche. Diese ganzen Tools,
ob Potman, ob Docker, ob
ja, was, was auch immer.
Aus irgendwelchen
Gründen
funktioniert meine Debian.
Oh, ich habe einen Kernel-Update gemacht
ohne neu zu starten. Kann das sein?
Starten meine VMs nicht. Auch, auch sehr cool.
Ja, vor gut.
Geht
ja gut los, weil ich jetzt
zeigen wollte.
Vielleicht startet
es jetzt.
Okay, scheint jetzt
zu funktionieren.
Foxbox, danke für den Sub.
Ja,
das ist bei LXC genau das gleiche.
Das ist bei LXC genau das gleiche.
Also egal, ob es Docker, LXC oder
Potman ist, das sind alles nur
Frontends für die
Linux-Kernel-Funktionalität,
die man
zusammenfasst als Container.
Also Namespaces,
Cgroups. Natürlich gehört
zu so einer Container-Runtime
noch mehr. Die muss das Image runterladen
vom Container. Die muss die Kernel-Apis
ansteuern. Aber unter der Haube ist das genau das gleiche.
Also es macht eigentlich
keinen Unterschied, ob ich LXC, Docker
oder Potman verwende.
Am Ende ist das ein Prozess,
der über die Kernelschnittstellen für
Containerisierung gestartet
wird.
Natürlich ist das Handling. So, und jetzt ist jetzt
der Unterschied. Also guckt euch mal an.
Docker run
unaim-a, sagt er mir,
es ist Arch,
was sehr merkwürdig ist für den Ubuntu,
warum das ein Arch-Kernel ist.
unaim-a auf meinem Host
und unaim-a im Container. Ihr seht, das ist
der gleiche Kernel. unaim-a
unaim-a
in der VM
ist Linux Debian Test VM 5.10.
Also das ist der
große Unterschied zwischen VMs und Container.
Auch wenn das immer ein bisschen über den Haufen geschmissen wird.
Eine VM ist wirklich ein
komplett virtualisiertes System,
inklusive eigenem Kernel.
Braucht dementsprechend auch mehr Ressourcen.
Was
Speicherplatz angeht, was vor allem
RAM Usage angeht.
Deswegen sind Container deutlich schneller gestartet
und leichtgewichtiger als VMs. Weil Container
gar nicht existieren.
Weil Container ganz normale
Prozesse in meinem Host-Kernel sind wie alles
andere auch. Nur ein bisschen besser
gesandboxed.
Also es ist ein Unterschied, ob man was in der
VM ausführt oder im Container.
Weil es ist grundlegend einfach was ganz
anderes. Das heißt, ich kann auch in der VM
solche Sachen machen, die ich im Container nicht machen kann.
Ich kann im VM zum Beispiel
Kernel-Module laden.
In einem Container kann ich keine Kernel-Module laden
oder unloaden.
Oder mit
speziellen Rechten mag das vielleicht gehen.
Aber zumindest teilen sich
alle Container, die ich starte, die gleichen
Host-Kernel-Module.
Es ist ein Unterschied.
Der
Sebaro war schon lange
nicht mehr da.
Ist eine VM nicht auch ein Prozess?
Nee, VM ist zwar
ein Prozess, der gestartet wird, um die VM
zu starten. Aber das
ist wirklich ein komplett virtualisiertes
System.
Du siehst ja beispielsweise
auch nicht.
Okay, guck mal. Wir machen jetzt
noch mal Docker Run.
Sleep 5
ane-a.
Und jetzt habe ich PSAugs
Grabsleep. Ich hoffe, das funktioniert.
Guck mal da. Was sehe ich denn hier?
Was sehe ich hier als ganz normalen Prozess
in meiner Prozess-Tabelle?
Was sehe ich hier? Bash-C-Sleep.
Was ein weiterer Beweis dafür
ist, es ist ein stinknormaler
Prozess, den ich im
Container starte.
Also ihr seht es ja.
Ups, das habe ich Mist gemacht.
So, hier.
Da steht es.
Laufende Prozesse.
Bin-Bash-C-Sleep-20.
Das ist das, was ich eigentlich im Container
ausgeführt habe. Also weiteres Beleg dafür,
dass das stimmt, was ich gesagt habe.
Wenn ich was im Container starte, es gibt keinen Container.
Das ist ein ganz normaler Prozess,
der hier in meinem System
läuft. Nur ein bisschen besser Sandbox.
Und jetzt ist er weg.
Also das ist wie bei Portal,
wo es den Cake nicht gibt. Container
gibt es eigentlich gar nicht.
Das ist ein Sammelsurium aus Sachen, was man
so unter Container versteht.
Es gibt keinen Container in dem Sinn.
Genau. Und bei einer VM
ist das anders. Wenn ich eine VM
starte...
Oh, ich
starte nochmal eine VM.
Eine Abstraktionsebene.
Ja, es ist
eher so ein bisschen Sandbox.
Das Ganze, genau. So, wenn ich
eine VM starte
und darin was ausführe, dann wird euch
auffallen, da sehe ich
nichts. Okay, die VM läuft auch
noch nicht.
VM läuft gleich.
Gut, dass wir den VM-Champ gebaut haben, oder?
Kann ich so Sachen besser
zeigen.
Auf Debian.
Booten. Please.
Pray. So. Und jetzt mache ich hier
mal das Gleiche. Was habe ich
in der VM ausgeführt? Was habe ich im Container ausgeführt?
Sleep 20
una-a. So.
Und jetzt werdet ihr sehen, ich sehe nichts.
Was ich sehe,
ist so Sachen wie zum Beispiel
QEMU,
dass das läuft, oder KVM, ne.
Ja, sowas sehe ich.
Also ich sehe, dass ein Prozess läuft,
der eine VM
gestartet hat, wenn ich weiß,
was dieser Prozess macht. Aber ich sehe nicht,
was in dieser VM läuft,
in der Prozessliste von meinem Host.
Weil es eben in einer komplett
virtualisierten Kiste läuft, wo mein Host-Könnel,
mein Host-System gar nichts mit zu tun hat.
Das einzige, was ich hier starte, ist die Software,
die die Virtualisierung macht.
Aber was dann in der virtualisierten
VM läuft, hat mit meinem Host-System
nichts zu tun. Dementsprechend
sieht man auch hier, ich habe einen eigenen Kernel,
das ich ein paar Debian erwarten würde.
Aber wenn ich jetzt exakt das gleiche mache,
guck mal Leute, ich habe hier oben gestartet,
ein Debian 11
habe ich gestartet, so.
Jetzt machen wir mal Docker Run
Debian,
Debian 11,
weiß ich gar nicht,
ob man 11 überhaupt machen kann.
Und ihr werdet feststellen, dass,
naja, Download Image, warte mal, ich mach's mal weg.
Und ihr werdet feststellen, dass der Unterschied
schon ziemlich groß ist. Hier ist es nämlich nach wie vor
Host-Kernel 6.2.11 Arch.
Und hier oben ist es das gleiche Debian-System,
es ist beides ein Debian 11.
Hier oben in der VM habe ich einen Kernel
15 Debian und hier unten habe ich,
obwohl ich ergeblich einen Debian,
in Anführungsstrichen gestartet habe, nach wie vor
6.2.11 Arch.
Und man sieht, Container ist komplett
geschwindelt, es gibt keine Container,
es gibt nur ein paar
Kernel-Funktialitäten und ein bisschen
Kleber,
die das zusammenführt
als Container-Runtime und man nennt
es dann halt Container.
So, aber genug zu dem Thema.
Das kann man nicht oft genug sagen.
Es ist egal, welche Container-Runtime du benutzt,
das ist im Prinzip immer das gleiche.
Man kann auch Container-Images in einer
VM ausführen, dann sind es trotzdem
keine Container mehr danach.
Das ist VMs.
So, aber, wo bin ich denn stehen geblieben?
Ich wollte eigentlich die zwei
Features zeigen, warum man Terraform
benutzt. Also das erste habt ihr ja schon
gesehen, man könnte jetzt reproduzierbar
das in unterschiedlichen Projekten anlegen.
Egal, ob das jetzt in meinem
Kekkelstream 1 Projekt ist
oder in Kekkelstream 99
oder in Plup 1, 2, 3, egal,
ich könnte reproduzierbar den ganzen
gleichen Krempel anlegen, immer wieder mit Terraform
immer auf die gleiche Art und Weise.
So, eine weitere
coole Geschichte ist,
Terraform merkt
sich, was es gemacht hat.
Terraform hat einen State.
Terraform hat sich gemerkt,
wenn ich jetzt hier nochmal apply mache,
Terraform hat sich gemerkt,
dass es Netzwerke
für mich angelegt hat.
Das heißt, wenn ich jetzt nochmal Terraform apply
macht, schlägt es auch nicht vor, was zu machen,
sondern weil sie sagt, hier meine Infrastruktur
und das, was ich konfiguriert habe,
ist gleich. Das heißt, Terraform
weiß, dass es die Netzwerke angelegt hat.
Das bedeutet auch,
wenn ich jetzt alles wieder löschen
will, was ich mit Terraform aufgebaut habe,
dann könnte ich sagen, Terraform destroy,
mache ich jetzt nicht, weil ich brauche sie ja gleich
wieder, dann würde Terraform nachgucken
und sagen, jawoll,
ich werde einfach das hier,
was ich angelegt habe, nämlich
das Network und das Subnetz und
hier so ein paar Apis eingeschaltet, werde ich einfach
wieder löschen.
Und um euch zu zeigen,
dass das auch tatsächlich der Fall ist,
oder ja, wenn ich jetzt
hier von Hand noch ein zusätzliches Netzwerk
anlege, hier blablablup, ist wurscht,
wie das heißt, hier Subnet,
122168.0,
slash 412, einen Moment, das ist der
Name, lulw,
nee,
region, asia,
ist vollkommen egal, ip range.
So, wenn ich das hier jetzt anlege
und sage
Terraform destroy,
dann
sagt Terraform,
Achtung,
dann sagt Terraform nichts
von diesem
neu angelegten Netzwerk, warum nicht?
Weil Terraform es nicht selbst erstellt hat.
Terraform interessiert sich nur für Sachen,
die Terraform selbst angelegt hat.
Ja, man kann Sachen importieren in Terraform,
aber in der Regel ist es so,
Terraform hat es nicht selbst angelegt,
also interessiert sich Terraform auch nicht dafür.
Terraform merkt sich,
was es erstellt hat und kann das auch wieder löschen
oder überprüfen,
ob das
richtig ist. So, jetzt gucken wir uns mal
was weiteres an. Ich lösche das
jetzt einfach wieder.
Ja,
je nachdem, was der macht, das ist nämlich
das nächste, was ich euch zeigen will. So,
ich lösche den Krempel gerade mal wieder.
Delete, yes.
Also, das nicht
mit Terraform erstellt hat. So, mal angenommen,
ich gehe jetzt in das Netzwerk rein,
was ich mit Terraform angelegt
habe. So, ich gehe jetzt in das Subnet hier rein.
Ich weiß gar nicht, ob man noch was gescheit ändern kann.
Und ich sage jetzt, ne, ne,
IP-Range soll eigentlich
slash 25 sein anstatt
slash 24. Und ich
speichere das jetzt.
Natürlich, man kann immer noch viel zusätzlich machen.
Also, fuck.
Äh.
21,
aber man darf es nicht kleiner machen.
Man darf es nur größer machen.
Dann frah, oh, warte,
jetzt bin ich gespannt. Jetzt bin ich gespannt,
was Terraform macht. Wahrscheinlich
wird Terraform das Netzwerk neu erzeugen,
weil es es nicht ändern kann. Oder
es buggt rum. Gucken wir mal.
Bin immer gespannt, wie Terraform
das jetzt handelt. Weil Terraform
müsste es ja eigentlich kleiner machen.
So.
Also, irgendjemand ist jetzt
ins Webinterface gegangen und hat daran rumgespielt.
Jetzt mache ich mal Terraform
Apply. Und jetzt
guckt Terraform nach. Wie ist der State?
Wie er sein soll? Und wie ist der State,
wie er wirklich ist?
Und jetzt sagt Terraform, Moment mal.
Guck mal da.
Irgendwie
ist es slash 23, aber es
sollte doch slash 24 sein.
Terraform ist aber auch schlau genug,
dass es weiß, es kann es nicht
einfach ändern, sondern es muss
replaced werden.
Dann sage ich yes. Und dann
wird Terraform jetzt das Subnet
löschen und das Subnet neu anlegen
mit der richtigen Subnet Mask oder mit
der richtigen Netmask hinten dran.
Das sind die
zwei richtig nice Features
an Terraform. Dass Terraform
weiß, was es gemacht
hat und Terraform Sachen
korrigieren kann oder Sachen rückstandslos
rückstandslos, manchmal bleiben
ein paar Sachen übrig, rückstandslos
wieder entfernen kann.
Neben dem reproduzierbar aufbauen.
Und das ist wirklich extrem nice.
Gerade wenn ihr im Projekt seid,
also wenn mehrere Leute hier in diesem
Projekt rumwursten und ihr
wollt danach nicht das komplette Projekt löschen,
wenn ihr durch seid, sondern nur eure Ressourcen
wieder entfernen, dann ist Terraform wirklich
sehr praktisch, weil ansonsten müsstet ihr
von Hand hingehen und alles im Webinterface
wieder wegklicken, was ihr
angelegt habt.
So und jetzt hat Terraform
das wieder geändert.
Deswegen sage ich
rückstandslos.
So und jetzt wird Terraform wieder sagen
jo, passt.
Manchmal muss man mit Terraform
Sachen exkluden.
Und ich sage das jetzt auch nur, weil manch, weil das jemand
im Chat gesagt hat.
Das kann man hier machen. Man kann hier glaube ich
ignore. Man kann
sagen, hey Terraform, scheiß
mal drauf, weil einer was von Hand geändert hat.
Aber das sollte man nur an Stellen machen,
wo es auch wirklich notwendig ist.
Ansonsten
versucht Terraform das immer in den Stand zu bringen,
wie es hier im Code
definiert ist. Was auch der Grund ist, warum sich
das ganze Infrastructure
als Code nennt. Weil man seine
Infrastruktur hier in
Code, in
Textdateien beschreibt.
Gut, jetzt machen wir mal was Sinnvolles.
Wir haben im Prinzip nur ein Netzwerk angelegt bisher.
Wir brauchen noch ein paar andere Sachen.
Und zwar
wir möchten ja
VMs erzeugen können,
ein Kubernetes Cluster erzeugen können,
in diesem Kubernetes Cluster
also verschiedene mehrere Nodes für
den Kubernetes Cluster erzeugen können,
mehrere Pots laufen lassen
können und Deployments
laufen lassen können im Kubernetes Cluster.
Das heißt wir brauchen noch mehr IP
Ranges. Und das ist ein bisschen eklig
gemacht
in der Terraform
config für
Google Cloud. Wir machen
Ignore Changes, ja, wir machen
auch Ignore Changes bei einer VM gleich,
nämlich für SSH Keys.
Und dafür gibt es folgende Sachen
neben einer primären
IP Range kann
man für ein Subnet auch, es ergibt
Netzwerk technisch, Leute, ich sag's euch
an der Stelle, Netzwerk technisch
ergibt das, was ich jetzt hier konfiguriere, keinen
Sinn, aber man muss es bei der Google Cloud
so machen, sonst funktioniert es nicht.
So, man kann jetzt
nämlich noch sagen,
mehrere Secondary IP
Ranges anlegen,
wollen einen Block haben.
Secondary IP Range. Und da kann
man jetzt noch zusätzliche IP Ranges
definieren, die nicht zwangsläufig
hier drinnen liegen
müssen unbedingt,
sondern andere.
Ja, da kann man zum Beispiel
jetzt sowas sagen wie
Range Name. So, also wir brauchen
einmal eine Range für
Kubernetes Pots.
Also letztendlich die Container,
die ich in meinem Kubernetes Cluster laufen lasse,
die sollen ja interne IPs kriegen
und das macht man über so eine
Secondary Range.
So, da muss man wieder eine weitere Range
angeben und da können wir jetzt irgendwie sowas machen,
wie hier 10.
1.0.0
slash 4.2.
Dann können wir halt nur
mal gucken, ob das geht oder ob es irgendwelche
Auflagen gibt, dass man das nicht
machen darf.
Wir können auch
slash 16 machen, das ist vielleicht gar nicht so
blöd. Dann haben wir mehr Platz,
dann haben wir mehr Pots.
Dann können wir quasi das hier für Pots
benutzen.
So, das können wir machen
und dann brauchen wir nochmal was.
Das können wir jetzt keine Ahnung,
10.2.0.0 slash 16 oder wir können
auch was kleineres nehmen, ist eigentlich vollkommen
egal.
Das nächste ist für
Wobei brauchen wir das überhaupt?
Ich glaube, das reicht.
Für die Notes selber.
Ich bin mir nicht
sicher, ob wir das brauchen.
Müssen wir gleich mal gucken.
Warte mal, lass mich mal kurz spicken.
Ja doch, das brauchen wir.
Das brauchen wir.
Für die
für den Kubernetes Cluster selbst.
Für die
oder besser gesagt für die Notes
brauchen wir da brauchen wir noch eine Range.
Nehmen wir jetzt
core neighbors services
und das ist 10.2.0.0
slash 16.
Warum? Ergibt nicht wirklich
Sinn von der Konfig, aber die Google Cloud
API möchte, dass wir das so machen.
Ansonsten, ansonsten funktioniert es nicht.
So, machen wir Terraform Apply, gucken ob wir
alles richtig gemacht haben.
Waschtabs
Die Wäschetabs Challenge
Gut, weiter geht's.
Terraform legt
den ganzen
darf das mehrfach sein?
Ja, das darf mehrfach sein.
Wie gesagt, ich muss auch sagen, Terraform
ist zwar mit Abstand das verbreitetste Tool dafür,
ich persönlich mag die
Syntax von HCL nicht wirklich.
Man muss sich halt
ein bisschen mit auskennen,
weil das so zu den
Standard Skills mittlerweile gehört.
Ich persönlich mag die Terraform
Syntax nicht. Deswegen finde ich das
gut, dass sie sowas wie CDKTF
haben, dass man das mit normalen Programmiersprachen
machen kann. Mein persönlicher Favorit
ist nach wie vor Pulumi, aber
Terraform ist mit Abstand das
verbreitetste, schlicht und ergreifend.
Diesmal wirklich. Wieso, das ist das zweite Mal,
wo wir das gemacht haben.
Letztes Mal hatte ich keinen Bock mehr zu streamen und gestern hatte ich Dünnschiss.
Konnte ich nicht streamen.
So, jetzt haben wir das angelegt.
Ist die Playlist vorbei?
Die war zu epik, die Playlist.
Kann die Syntax
komplex werden?
Ja, auch, aber die Syntax ist nicht schön.
Zum Beispiel, wenn du Schleifen machen musst
und sowas, das ist einfach abartig.
Es könnte ja durchaus sein, dass ich eine Schleife
machen will, hierfür jetzt,
für mehrere Netzwerke.
Und Schleifen hier.
Terraform Loops.
Es ist nicht
wirklich aktuell.
2016.
Offizielles Ding.
Dann machst du das so.
Dann schreibst du count
und dann macht er das mehrfach und dann kannst du hier
count index Variable einsetzen.
Das ist wirklich abartig.
Schleifen, Schleifen, Terraform.
Das wird, da ist
genau, man kann
man kann dann auch irgendwie so Wildcard
Pattern und so ein komisches Gedöns
machen. Das ist wirklich abartig.
Also insofern
ist so Sachen wie Pulumi schon ganz nice,
weil man einfach eine vollwertige echte
Programmiersprache hat. Alles, alles ist gut.
So, also machen wir weiter.
Als nächstes ist äußerst
unspektakulär. Das
nennen wir Zwei, Zwei
Router Punkt TF.
Und das kopiere ich jetzt, weil wir da
nicht ergreifend gar nichts machen müssen.
So, wärm.
Das ist einfach.
Äh, ups.
Vielleicht auch im richtigen Ort, ne?
So, ich glaube dazu muss ich jetzt nicht
über nichts Großartiges erzählen.
Das nächste wird schon ein bisschen interessanter.
Wir müssen
NUT-Regeln anlegen. Also sprich, dass
unsere VMs zum Beispiel ins
Internet connecten können.
Ah.
Was ist mit dieser Playlist
los, Mensch?
Was bin ich hier? Du siehst Terra
vorm Konfig.
In dem Fall. So, als nächstes
brauchen wir
NUT-Regeln,
damit unsere Kisten ins
Internet kommen. So, das geht folgendermaßen.
Resource, Google
Compute
Router, NUT.
Jetzt werdet ihr auch gleich sehen, warum wir einen
Router anlegen mussten, weil ich kann
keine NUT-Regeln anlegen, ohne dass ich vorhin
einen Router angelegt hab.
So, NUT-Regeln, dass unsere Kisten
ins Internet kommen. Also, Name,
NUT.
Haben wir nur eins.
So, Router, gleich.
Ähm, den hier, im Endeffekt.
Äh, Router.
Moment, warte mal.
Ich hoffe, das Ding kann Autocomplete.
Google Default Router.
Genau, ist kein Autocomplete.
Äh, Selflink.
Wenn man
Selflink benutzt, das ist auch so eine Google
Terraform-Eigenheit, dann
bedeutet das, dass diese Ressource
gelinkt wird mit dieser Ressource.
Und die hängen voneinander ab.
Das heißt dann Weiß Terraform.
Das hier darf es erst
nach dem hier machen.
So, wenn ich das richtig verstanden hab.
...
...
...
...
...
...
...
...
...
...
...
Ja, so, ähm,
jetzt müssen wir noch zwei andere Sachen anlegen.
Und zwar müssen wir sehen, angehen welche
Regen. Regen haben wir eine Variable
für. Unsere
Regen ist...
Unsere Regen ist Europe West
3.
Also Regen gleich Europe West 3.
Jetzt müssen wir
ein bisschen Terraform-Matching machen
für Google. Und zwar
Copy Paste ich mir das mal. Wir müssen folgendes
angeben. Source Sub Network
IP Ranges to Nut.
Da steht auch die Erklärung zu, was das
Ganze ist.
High IQ Shit
also erlaubt ist
All Sub Networks
All IP Ranges
All Sub Nets, All Primary IPs
und List of Sub Networks.
Das heißt ich kann jetzt hier selbst angeben
welche
Nut IPs das Ding
verwenden soll.
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
